

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="Deep Hough Voting for 3D Object Detection in Point Clouds  读论文三步曲：泛读，精读，总结。  泛读  泛读：快速浏览，把握概要。重点读标题、摘要、结论、所有小标题。  Title部分 Deep Hough Voting for 3D Object Detection in Point Clouds  任务: 3D Object Detec">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Hough Voting for 3D Object Detection in Point Clouds阅读笔记">
<meta property="og:url" content="http://example.com/2022/01/13/Deep-Hough-Voting-for-3D-Object-Detection-in-Point-Clouds%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Deep Hough Voting for 3D Object Detection in Point Clouds  读论文三步曲：泛读，精读，总结。  泛读  泛读：快速浏览，把握概要。重点读标题、摘要、结论、所有小标题。  Title部分 Deep Hough Voting for 3D Object Detection in Point Clouds  任务: 3D Object Detec">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201121501675.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131028819.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131028926.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131029273.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131031595.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131033019.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131036132.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131039905.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131041972.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131042169.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131349652.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131409342.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131411338.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131413119.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131416235.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131418704.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131100180.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131100772.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201131055607.png">
<meta property="article:published_time" content="2022-01-13T08:35:34.000Z">
<meta property="article:modified_time" content="2022-01-13T08:36:48.456Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/chence17/picgo/master/202201121501675.png">
  
  
  <title>Deep Hough Voting for 3D Object Detection in Point Clouds阅读笔记 - Hexo</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 6.0.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Deep Hough Voting for 3D Object Detection in Point Clouds阅读笔记">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-01-13 16:35" pubdate>
        January 13, 2022 pm
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      44k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      364 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Deep Hough Voting for 3D Object Detection in Point Clouds阅读笔记</h1>
            
            <div class="markdown-body">
              <h1 id="deep-hough-voting-for-3d-object-detection-in-point-clouds">Deep Hough Voting for 3D Object Detection in Point Clouds</h1>
<blockquote>
<p>读论文三步曲：泛读，精读，总结。</p>
</blockquote>
<h2 id="泛读">泛读</h2>
<blockquote>
<p>泛读：快速浏览，把握概要。重点读标题、摘要、结论、所有小标题。</p>
</blockquote>
<h3 id="title部分">Title部分</h3>
<p>Deep <u>Hough Voting</u> for <em>3D Object Detection</em> in <em>Point Clouds</em></p>
<ul>
<li>任务: 3D Object Detection</li>
<li>方法: Hough Voting</li>
<li>特点: Point Clouds</li>
</ul>
<h3 id="abstract部分">Abstract部分</h3>
<p>In this work, we return to ﬁrst principles to <u>construct a 3D detection pipeline for point cloud data and as generic as possible</u>.</p>
<p>作者目的是直接利用点云数据进行三维检测而不是将点云数据转换为其他中间类型.</p>
<p>However, <u>due to the sparse nature of the data</u> – samples from 2D manifolds in 3D space – we face a major challenge when directly predicting bounding box parameters from scene points: <u>a 3D object centroid can be far from any surface point thus hard to regress accurately in one step</u>.</p>
<p>由于点云数据的稀疏特性, 直接一步准确的回归边界框参数是困难的, 因为三维物体的中心往往距离表面点很远, 而点云扫描到的主要是表面点.</p>
<p>To address the challenge, we propose <u>VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting</u>.</p>
<p>因此, 作者设计了VoteNet, 一个端到端的结合深度点云网络和霍夫投票策略的直接使用点云输入的三维物体检测算法.</p>
<p>Our model <u>achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D</u> with a simple design, compact model size and high efﬁciency.</p>
<p>作者提出的架构在ScanNet和SUN RGB-D两个数据集上取得了SOTA并且设计简单、模型紧凑和运行高效.</p>
<p>Remarkably, VoteNet outperforms previous methods by using purely geometric information <u>without relying on color images</u>.</p>
<p>VoteNet仅对点进行操作, 没有使用任何额外的彩色图片.</p>
<h3 id="conclusion部分">Conclusion部分</h3>
<p>The network learns to <u>vote to object centroids directly from point clouds</u> and learns to <u>aggregate votes through their features and local geometry to generate high-quality object proposals</u>.</p>
<p>作者提出的网络学习直接从点云投票到物体中心点并且通过投票结果的特征和局部几何结构来融合投票结果从而生成高质量的物体候选框.</p>
<p>In future work we intend to <u>explore how to incorporate RGB images into our detection framework</u> and to <u>utilize our detector in downstream application such as 3D instance segmentation</u>.</p>
<p>未来, 作者考虑探索如何引入彩色信息到检测架构并且在一些下游应用(例如三维实例分割)中利用检测器.</p>
<p>We believe that the synergy of Hough voting and deep learning can be generalized to more applications such as <u>6D pose estimation</u>, <u>template based detection</u> etc. and expect to see more future research along this line.</p>
<p>作者认为霍夫投票和深度学习的方法可以拓展到其他应用(例如六维位姿估计和基于模板的检测)并且希望有更多的基于此类方法的研究.</p>
<h3 id="小标题分析">小标题分析</h3>
<ul>
<li>Introduction <em>[简介]</em></li>
<li>Related Work <em>[相关工作]</em></li>
<li>Deep Hough Voting <em>[<u>深度霍夫投票</u>]</em></li>
<li>VoteNet Architecture <em>[VoteNet网络架构]</em>
<ul>
<li>Learning to Vote in Point Clouds <em>[<u>从点云中学习投票</u>]</em></li>
<li>Object Proposal and Classiﬁcation from Votes <em>[<u>从投票中提取物体和类别</u>]</em></li>
<li>Implementation Details <em>[实现细节]</em></li>
</ul></li>
<li>Experiments <em>[实验]</em>
<ul>
<li>Comparing with State-of-the-art Methods <em>[与其他SOTA的比较]</em></li>
<li>Analysis Experiments <em>[分析实验]</em></li>
<li>Qualitative Results and Discussion <em>[定性分析的结果和讨论]</em></li>
</ul></li>
<li>Conclusion <em>[结论]</em></li>
<li>Supplement <em>[附加材料]</em>
<ul>
<li>Details on Architectures and Loss Functions <em>[<u>架构和损失函数的细节</u>]</em></li>
<li>More Analysis Experiments <em>[更多的分析实验]</em></li>
<li>ScanNet Per-class Evaluation <em>[ScanNet的每个类别的测试]</em></li>
<li>Visualization of Votes <em>[投票可视化]</em></li>
</ul></li>
</ul>
<h3 id="泛读总结">泛读总结</h3>
<blockquote>
<p>泛读目标及效果自测：1.论文要解决什么问题? 2.论文采用了什么方法? 3.论文达到什么效果?</p>
</blockquote>
<p>论文要解决什么问题? - 直接使用原始点云数据的三维物体检测.</p>
<p>论文采用了什么方法? - 为了解决点云中物体中心与物体表面点相距甚远的问题, 作者结合深度学习和霍夫投票策略进行物体检测.</p>
<p>论文达到什么效果? - VoteNet在ScanNet和SUN RGB-D两个数据集上达到了SOTA, VoteNet设计简单、模型紧凑和运行高效.</p>
<h2 id="精读">精读</h2>
<blockquote>
<p>精读：通过泛读找到需要精读重点部分和快速略过的部分，选出精华，仔细阅读。</p>
</blockquote>
<h3 id="introduction部分">Introduction部分</h3>
<p>More speciﬁcally, in this work, we aim to <u>estimate oriented 3D bounding boxes as well as semantic classes of objects from point clouds</u>.</p>
<p>作者的目标是从点云中估计有向三维边界框及物体的语义类别.</p>
<p>Compared to images, 3D point clouds provide <u>accurate geometry and robustness to illumination changes</u>.</p>
<p>相比于图像, 三维点云能够在光照变化的情况下具有准确的几何特征和鲁棒性.</p>
<p>On the other hand, <u>point clouds are irregular</u>.</p>
<p>同时点云是不规则的.</p>
<p>Thus <u>typical CNNs are not well suited to process them directly</u>.</p>
<p>因此, 经典的CNN无法有效的直接处理点云.</p>
<p>To avoid processing irregular point clouds, current 3D detection methods <u>heavily rely on 2D-based detectors</u> in various aspects.</p>
<p>为了避免处理不规则的点云, 大部分三维检测算法严重的依赖于二维检测器.</p>
<p>For example, some extend 2D detection frameworks such as the Faster/Mask R-CNN to 3D.</p>
<p>一些算法拓展二维检测器到三维.</p>
<p>They <u>voxelize the irregular point clouds to regular 3D grids and apply 3D CNN detectors</u>, which <u>fails to leverage sparsity in the data and suffer from high computation cost</u> due to expensive 3D convolutions.</p>
<p>这些算法先将不规则的点云体素化为规则的三维网格并使用基于三维卷积网络的检测器. 但是, 这些算法往往无法有效利用数据的稀疏性质并且需要很大的计算成本进行运算.</p>
<p>Alternatively, some projects point to <u>regular 2D bird’s eye view images</u> and then apply 2D detectors to localize objects.</p>
<p>另一些算法则使用常规的三维场景的二维鸟瞰图视角的图片并将二维检测器使用在这些图片上来检测物体.</p>
<p>This, however, sacriﬁces geometric details which may be critical in cluttered indoor environments.</p>
<p>这种做法牺牲了几何细节, 这些几何细节对于室内场景的检测至关重要.</p>
<p>More recently, others proposed a cascaded two-step pipeline by ﬁrstly detecting objects in front-view images and then localizing objects in frustum point clouds extruded from the 2D boxes, which however is <u>strictly dependent on the 2D detector and will miss an object entirely if it is not detected in 2D</u>.</p>
<p>最近有一种级联的两步骤方法, 先在前视图图像中进行二维检测检测物体, 之后通过二维图像检测结果定位点云中的三维物体, 这种方法依赖于二维检测器, 如果一个物体没有在二维检测器中检测到, 那么将会整个丢失.</p>
<p>In this work we <u>introduce a point cloud focused 3D detection framework that directly processes raw data and does not depend on any 2D detectors neither in architecture nor in object proposal</u>.</p>
<p>作者引入了一个基于点云的三维检测器, 直接处理原始的三维点云数据, 在整个系统和物体候选框的生成过程中, 不使用任何二维检测器.</p>
<p>Our detection network, VoteNet, is based on recent advances in 3D deep learning models for point clouds, and is inspired by the generalized <u>Hough voting process</u> for object detection.</p>
<p>作者提出的方法基于最近具有进展的基于点云的三维深度学习方法和基于霍夫投票过程的物体检测方法.</p>
<p>We <u>leverage PointNet++</u>, a hierarchical deep network for point cloud learning, <u>to mitigates the need to convert point clouds to regular structures</u>.</p>
<p>作者直接使用Point++结构来提取特征从而不用将点云转换为规则的数据结构.</p>
<p>By directly processing point clouds not only do we <u>avoid information loss by a quantization process</u>, but we also <u>take advantage of the sparsity in point clouds by only computing on sensed points</u>.</p>
<p>直接处理点云不仅避免了量化过程中的信息损失, 也利用了点云数据稀疏的特性.</p>
<p>While PointNet++ has shown success in object classiﬁcation and semantic segmentation, few research study how to detect 3D objects in point clouds with such architectures.</p>
<p>虽然PointNet++在物体分类和语义分割领域取得了成功, 但是如何将其应用到检测领域的研究却很少.</p>
<p>A naïve solution would be to follow common practice in 2D detectors and <u>perform dense object proposal to propose 3D bounding boxes directly from the sensed points</u> (with their learned features).</p>
<p>一个自然的思路是借用二维检测的方式使用有效的点直接进行稠密物体候选框的生成.</p>
<p>However, <u>the inherent sparsity of point clouds makes this approach unfavorable</u>.</p>
<p>然而, 点云数据的稀疏特性使这种方式不太有效.</p>
<p>In images there often exists a pixel near the object center, but it is often <u>not the case in point clouds</u>.</p>
<p>在图片中, 总存在一个接近物体中心的像素, 但是在点云中却不常见.</p>
<p>As depth sensors only capture surfaces of objects, 3D object centers are likely to be in empty space, <u>far away from any point</u>.</p>
<p>由于三维传感器往往只捕捉物体的表面, 三维物体的中心却常常原理表面, 在点云中, 三维物体的中心经常是空的.</p>
<p>As a result,point based networks <u>have difﬁculty aggregating scene context in the vicinity of object centers</u>.</p>
<p>因此, 基于点云的算法很难直接聚合场景上下文信息.</p>
<p><u>Simply increasing the receptive ﬁeld does not solve the problem</u> because as the network captures larger context, it also causes more inclusion of nearby objects and clutter.</p>
<p>简单的增大感受野不能解决这个问题, 因为增大感受野虽然能够获得更多的上下文信息, 其也会包含很多邻近物体和聚类.</p>
<p>To this end, we propose to <u>endow point cloud deep networks with a voting mechanism similar to the classical Hough voting</u>.</p>
<p>因此, 作者设计了一个基于霍夫投票机制的直接使用点云数据的深度网络来实现检测算法.</p>
<p>By voting we essentially <u>generate new points that lie close to objects centers</u>, which can be grouped and aggregated to generate box proposals.</p>
<p>通过投票, 作者能够获得靠近物体中心的新点, 这些点能够分组融合以生成候选框.</p>
<p>In contrast to traditional Hough voting with multiple separate modules that are difﬁcult to optimize jointly, <u>VoteNet is end-to-end optimizable</u>.</p>
<p>与具有独立模块的传统的霍夫投票算法不同, VoteNet是一个端到端的结构.</p>
<p>We evaluate our approach on two challenging 3D objectdetection datasets: SUN RGB-D and ScanNet.</p>
<p>作者在SUN RGB-D和ScanNet数据集上测试了算法.</p>
<p>On both datasets VoteNet, <u>using geometry only</u>, signiﬁcantly outperforms prior arts that use both RGB and geometry or even multi-view RGB images.</p>
<p>VoteNet的表现超过其他的算法, VoteNet只使用了几何信息, 其他算法即使用了几何信息也使用了颜色信息.</p>
<p>Our study shows that the voting scheme supports more effective context aggregation, and veriﬁes that VoteNet <u>offers the largest improvements when object centers are far from the object surface</u>.</p>
<p>作者的研究表明VoteNet能有效的聚合上下文信息并对中心点远离物体表面的物体检测的提升最大.</p>
<p>In summary, the contributions of our work are:</p>
<ul>
<li>A reformulation of Hough voting in the context of deep learning through an end-to-end differentiable architecture, which we dub VoteNet.</li>
<li>State-of-the-art 3D object detection performance on SUN RGB-D and ScanNet.</li>
<li>An in-depth analysis of the importance of voting for 3D object detection in point clouds.</li>
</ul>
<p>作者的贡献如下:</p>
<ul>
<li>霍夫投票机制在三维点云检测中基于深度学习算法的重构, 提出了VoteNet架构.</li>
<li>在SUN RGB-D和ScanNet上取得了SOTA.</li>
<li>详细分析了在基于点云的三维物体检测中使用霍夫投票的重要性.</li>
</ul>
<h3 id="related-work部分">Related Work部分</h3>
<p>Originally introduced in the late 1950s, <u>the Hough transform translates the problem of detecting simple patterns in point samples to detecting peaks in a parametric space</u>.</p>
<p>霍夫变换最初引入于20世纪50年代末, 它将检测样本中简单模式的问题转化为检测参数空间中的峰值问题.</p>
<p>The Generalized Hough Transform further <u>extends this technique to image patches as indicators for the existence of a complex object</u>.</p>
<p>广义霍夫变换进一步将该技术扩展到图像面片, 作为复杂对象的指示器.</p>
<h3 id="deep-hough-voting部分">Deep Hough Voting部分</h3>
<p>A traditional Hough voting 2D detector comprises <u>an ofﬂine and an online step</u>.</p>
<p>传统霍夫二维检测器包括一个在线步骤和一个离线步骤.</p>
<p>First, given a collection of images with annotated object bounding boxes, <u>a codebook is constructed with stored mappings between image patches (or their features) and their offsets to the corresponding object centers</u>.</p>
<p>首先, 给定一组带有注释对象边界框的图像, 通过存储图像面片(或其特征)及其到相应对象中心的偏移之间的映射来构造codebook.</p>
<p>At inference time, <u>interest points are selected from the image to extract patches around them</u>.</p>
<p>在推断时, 从图像中选择兴趣点以提取其周围的面片.</p>
<p>These patches are then <u>compared against patches in the codebook to retrieve offsets and compute votes</u>.</p>
<p>然后将这些面片与codebook中的面片进行比较, 以检索偏移量并计算投票.</p>
<p>As object patches will tend to <u>vote in agreement</u>, <u>clusters will form near object centers</u>.</p>
<p>由于面片倾向于一致投票, 因此在物体中心附近将形成簇.</p>
<p>Finally, the <u>object boundaries are retrieved by tracing cluster votes back to their generating patches</u>.</p>
<p>最后, 通过将成簇的投票回溯到其生成的面片来检索物体边界.</p>
<p>We identify two ways in which this technique is <u>well suited to our problem of interest</u>.</p>
<p>作者确定了这项技术非常适合基于点云的检测问题的两种方式.</p>
<p>First, <u>voting-based detection is more compatible with sparse sets than region-proposal networks (RPN) is</u>.</p>
<p>首先, 基于投票的检测比区域提议网络(RPN)更适合稀疏的数据集.</p>
<p>For the latter, the <u>RPN has to generate a proposal near an object center which is likely to be in an empty space, causing extra computation</u>.</p>
<p>对于后者, RPN必须在物体中心附近生成候选框, 物体中心可能位于空区域, 从而导致额外的计算.</p>
<p>Second, <u>it is based on a bottom-up principle where small bits of partial information are accumulated to form a conﬁdent detection</u>.</p>
<p>其次, 霍夫变换基于自底向上的原则, 即积累局部信息进行检测.</p>
<p>Even though neural networks can potentially aggregate context from a large receptive ﬁeld, <u>it may be still beneﬁcial to aggregate in the vote space</u>.</p>
<p>尽管神经网络可以潜在地从一个大的感受野聚合上下文信息, 但在投票空间的聚合仍然是有益的.</p>
<p>However, as traditional Hough voting comprises multiple separated modules, <u>integrating it into state-of-the-art point cloud networks is an open research topic</u>.</p>
<p>然而, 由于传统的霍夫投票方法包含多个分离的模块, 因此将其集成到最先进的点云网络中是一个开放的研究课题.</p>
<p>To this end, we <u>propose the following adaptations to the different pipeline ingredients</u>.</p>
<p>为此, 作者对不同的组件进行以下调整.</p>
<p><strong>Interest points</strong> are <u>described and selected by deep neural networks</u> instead of depending on hand-crafted features.</p>
<p><strong>兴趣点</strong>由深度神经网络描述和选择, 而不是依靠手工制作的特征.</p>
<p><strong>Vote generation</strong> is <u>learned by a network</u> instead of using a codebook.</p>
<p><strong>投票生成</strong>由网络学习, 而不是使用codebook.</p>
<p><u>Leveraging larger receptive ﬁelds</u>, voting can be made less ambiguous and thus more effective.</p>
<p>利用更大的接受范围, 投票可以减少歧义, 从而更有效.</p>
<p>In addition, <u>a vote location can be augmented with a feature vector allowing for better aggregation</u>.</p>
<p>此外, 投票位置可以通过允许更好聚合的特征向量来增加.</p>
<p><strong>Vote aggregation</strong> is <u>realized through point cloud processing layers with trainable parameters</u>.</p>
<p><strong>投票聚合</strong>通过具有可训练参数的点云处理层实现.</p>
<p>Utilizing the vote features, <u>the network can potentially ﬁlter out low quality votes and generate improved proposals</u>.</p>
<p>利用投票功能, 网络可以潜在地过滤低质量的投票, 并生成改进的提案.</p>
<p><strong>Object proposals</strong> in the form of: location, dimensions, orientation and even semantic classes can be directly generated from the aggregated features, mitigating the need to trace back votes’ origins.</p>
<p><strong>物体候选框</strong>的形式: 位置、维度、方向, 甚至语义类别, 可直接从聚合特征生成, 从而减少追溯投票来源的需要.</p>
<h3 id="votenet-architecture部分">VoteNet Architecture部分</h3>
<p><u>Illustration of the VoteNet architecture</u> for 3D object detection in point clouds.</p>
<p>下图是VoteNet的架构图.</p>
<figure>
<img src="https://raw.githubusercontent.com/chence17/picgo/master/202201121501675.png" srcset="/img/loading.gif" lazyload alt="image-20220112150055821" /><figcaption>image-20220112150055821</figcaption>
</figure>
<p>Given an input point cloud of <span class="math inline">\(N\)</span> points with XYZ coordinates, a backbone network (implemented with PointNet++ layers) subsamples and learns deep features on the points and <u>outputs a subset of <span class="math inline">\(M\)</span> points but extended by <span class="math inline">\(C\)</span>-dim features</u>.</p>
<p>输入为具有XYZ坐标的<span class="math inline">\(N\)</span>个点的点云, 主干网络(使用PointNet++层实现)对点的特征进行二次采样和学习, 输出<span class="math inline">\(M\)</span>个点组成的输入点云的子集(坐标和扩展了的<span class="math inline">\(C\)</span>维特征).</p>
<p>This subset of points are considered as <u>seed points</u>.</p>
<p>这<span class="math inline">\(M\)</span>个点被视为种子点.</p>
<p><u>Each seed independently generates a vote</u> through a voting module.</p>
<p>种子点通过投票模块独立生成投票点.</p>
<p>Then <u>the votes are grouped into clusters</u> and <u>processed by the proposal module to generate the ﬁnal proposals</u>.</p>
<p>然后将投票点分组, 由候选框生成模块进行处理, 生成候选框.</p>
<p>The <u>classiﬁed and NMSed proposals become the ﬁnal 3D bounding boxes output</u>.</p>
<p>分类和经过NMS后的候选框将成为最终的三维物体边界框输出.</p>
<p>The entire network can be split into two parts: <u>one that processes existing points to generate votes</u>; and <u>the other part that operates on virtual points – the votes – to propose and classify objects</u>.</p>
<p>整个网络可分为两部分: 一部分处理现有点以生成投票; 而另一个作用于虚拟点的部分(投票点)则是对物体进行候选框的生成和分类.</p>
<h4 id="learning-to-vote-in-point-clouds部分">Learning to Vote in Point Clouds部分</h4>
<p>From an input point cloud of size <span class="math inline">\(N\times 3\)</span>, with a 3D coordinate for each of the <span class="math inline">\(N\)</span> points, we aim to <u>generate <span class="math inline">\(M\)</span> votes, where each vote has both a 3D coordinate and a high dimensional feature vector</u>.</p>
<p>输入为<span class="math inline">\(N\times3\)</span>的点云, 每个点都有一个三维坐标, 生成<span class="math inline">\(M\)</span>个投票点, 其中每个投票点都有一个三维坐标和一个高维特征向量.</p>
<p>There are two major steps: <u>point cloud feature learning through a backbone network and learned Hough voting from seed points</u>.</p>
<p>有两个主要步骤: 通过骨干网络学习点云特征和从种子点学习霍夫投票.</p>
<h5 id="point-cloud-feature-learning">Point cloud feature learning</h5>
<p>Generating an accurate vote requires <u>geometric reasoning and contexts</u>.</p>
<p>生成准确的投票需要几何推理和上下文信息.</p>
<p>Instead of relying on hand-crafted features, we <u>leverage recently proposed deep networks on point clouds for point feature learning</u>.</p>
<p>作者不依赖手工制作的特征, 而是利用点云深度网络进行点特征学习.</p>
<p>While our method is not restricted to any point cloud network, we <u>adopt PointNet++ as our backbone</u> due to its simplicity and demonstrated success on tasks ranging from normal estimation, semantic segmentation to 3D object localization.</p>
<p>虽然作者的方法不局限于任何点云网络, 但由于其简单性, 作者采用PointNet++作为主干.</p>
<p>The backbone network has several set-abstraction layers and feature propagation (upsampling) layers with skip connections, which outputs <u>a subset of the input points with XYZ and an enriched <span class="math inline">\(C\)</span>-dimensional feature vector</u>.</p>
<p>主干网络具有多个集合抽象层和跳连的特征传播(上采样)层, 其输出具有XYZ坐标和<span class="math inline">\(C\)</span>维特征向量的输入点云的子集.</p>
<p>The results are <u><span class="math inline">\(M\)</span> seed points of dimension <span class="math inline">\((3+C)\)</span></u>.</p>
<p>输出维度为<span class="math inline">\((3+C)\)</span>的<span class="math inline">\(M\)</span>个种子点.</p>
<p>Each seed point <u>generates one vote</u>.</p>
<p>每个种子点产生一票.</p>
<h5 id="hough-voting-with-deep-networks">Hough voting with deep networks</h5>
<p>Compared to traditional Hough voting where the votes (offsets from local key-points) are determined by look ups in a pre-computed codebook, we <u>generate votes with a deep network based voting module, which is both more efﬁcient (without kNN look ups) and more accurate as it is trained jointly with the rest of the pipeline</u>.</p>
<p>在传统的霍夫投票中, 投票(局部关键点的偏移量)是通过在预先计算的codebook中搜索来确定的, 作者使用基于深度神经网络的投票模块生成投票, 该模块更有效(无kNN查找)且更准确, 因为它与其余部分联合训练.</p>
<p>Given a set of seed points <span class="math inline">\(\left\{s_{i}\right\}_{i=1}^{M}\)</span> where <span class="math inline">\(s_{i}=\left[x_{i} ; f_{i}\right]\)</span> with <span class="math inline">\(x_{i} \in \mathbb{R}^{3}\)</span> and <span class="math inline">\(f_{i} \in \mathbb{R}^{C}\)</span>, <u>a shared <em>voting module</em> generates votes from each seed independently</u>.</p>
<p>对于种子点集<span class="math inline">\(\left\{s_{i}\right\}_{i=1}^{M}\)</span>, 其中<span class="math inline">\(s_{i}=\left[x_{i} ; f_{i}\right]\)</span>, <span class="math inline">\(x_{i} \in \mathbb{R}^{3}\)</span>, <span class="math inline">\(f_{i} \in \mathbb{R}^{C}\)</span>, 权重共享的投票模块从每个种子点生成投票点.</p>
<p>Speciﬁcally, <u>the voting module is realized with a multi-layer perceptron (MLP) network</u> with fully connected layers, ReLU and batch normalization.</p>
<p>具体而言, 投票模块是通过多层感知器(MLP)网络实现的, 此模块具有全连接层、ReLU和批量标准化.</p>
<p>The MLP <u>takes seed feature <span class="math inline">\(f_i\)</span> and outputs the Euclidean space offset <span class="math inline">\(\Delta x_{i} \in \mathbb{R}^{3}\)</span> and a feature offset <span class="math inline">\(\Delta f_{i} \in \mathbb{R}^{C}\)</span></u> such that the vote <span class="math inline">\(v_{i}=\left[y_{i} ; g_{i}\right]\)</span> generated from the seed <span class="math inline">\(s_i\)</span> has <span class="math inline">\(y_{i}=x_{i}+\Delta x_{i}\)</span> and <span class="math inline">\(g_{i}=f_{i}+\Delta f_{i}\)</span>.</p>
<p>多层感知器接受种子点特征<span class="math inline">\(f_i\)</span>作为输入并输出位置偏移<span class="math inline">\(\Delta x_{i} \in \mathbb{R}^{3}\)</span>和特征残差<span class="math inline">\(\Delta f_{i} \in \mathbb{R}^{C}\)</span>, 于是可以从种子点<span class="math inline">\(s_i\)</span>生成投票点<span class="math inline">\(v_{i}=\left[y_{i} ; g_{i}\right]\)</span>, 其中<span class="math inline">\(y_{i}=x_{i}+\Delta x_{i}\)</span>, <span class="math inline">\(g_{i}=f_{i}+\Delta f_{i}\)</span>.</p>
<p>The predicted 3D offset <u><span class="math inline">\(\Delta x_i\)</span> is explicitly supervised by a regression loss</u>:</p>
<p>预测的三维偏移量<span class="math inline">\(\Delta x_i\)</span>是<u>由回归损失函数监督</u>的: <span class="math display">\[
L_{\text {vote-reg }}=\frac{1}{M_{\text {pos }}} \sum_{i}\left\|\Delta x_{i}-\Delta x_{i}^{*}\right\| \mathbb{1}\left[s_{i} \text { on object}\right]
\]</span> where <u><span class="math inline">\(\mathbb{1}[s_{i} \text { on object}]\)</span> indicates whether a seed point <span class="math inline">\(s_i\)</span> is on an object surface and <span class="math inline">\(M_\text{pos}\)</span> is the count of total number of seeds on object surface</u>.</p>
<p>其中<span class="math inline">\(\mathbb{1}[s_{i} \text { on object}]\)</span>表示种子点<span class="math inline">\(s_i\)</span>是否在物体的表面, <span class="math inline">\(M_\text{pos}\)</span>则是在物体表面种子点的总数.</p>
<p><span class="math inline">\(\Delta x_{i}^{*}\)</span> is the <u>ground truth displacement from the seed position <span class="math inline">\(x_i\)</span> to the bounding box center of the object it belongs to</u>.</p>
<p><span class="math inline">\(\Delta x_{i}^{*}\)</span>代表真实偏移量, 是对应的三维物体中心点与种子点位置<span class="math inline">\(x_i\)</span>之间的距离.</p>
<p>Votes are <u>the same as seeds in tensor representation but are no longer grounded on object surfaces</u>.</p>
<p>投票点与种子点使用相同的向量表示, 但投票点不再分布在物体表面.</p>
<p>A more essential difference though is their position – <u>votes generated from seeds on the same object are now closer to each other than the seeds are, which makes it easier to combine cues from different parts of the object</u>.</p>
<p>但更本质的区别在于它们的位置, 从同一对象上的种子点生成的投票点比种子点相互之间更接近, 这使得组合来自对象不同部分的特征更容易.</p>
<h4 id="object-proposal-and-classiﬁcation-from-votes部分">Object Proposal and Classiﬁcation from Votes部分</h4>
<h5 id="vote-clustering-through-sampling-and-grouping">Vote clustering through sampling and grouping</h5>
<p>While there can be many ways to cluster the votes, we opt for a simple strategy of <u>uniform sampling and grouping according to spatial proximity</u>.</p>
<p>虽然有许多方法可以对投票进行聚类, 但作者选择了一种简单的策略, 即根据空间接近程度进行平均采样和分组.</p>
<p>Speciﬁcally, from a set of votes <span class="math inline">\(\left\{v_{i}=\left[y_{i} ; g_{i}\right] \in \mathbb{R}^{3+C}\right\}_{i=1}^{M}\)</span> <u>using farthest point sampling based on <span class="math inline">\({y_i}\)</span> in 3D Euclidean space</u>, to get <span class="math inline">\({v_{i_k}}\)</span> with <span class="math inline">\(k =1,\cdots,K\)</span>.</p>
<p>对一个投票点集合<span class="math inline">\(\left\{v_{i}=\left[y_{i} ; g_{i}\right] \in \mathbb{R}^{3+C}\right\}_{i=1}^{M}\)</span>, 基于其坐标参数<span class="math inline">\({y_i}\)</span>进行最远点采样(FPS), 以此获取<span class="math inline">\({v_{i_k}}\)</span>, 其中<span class="math inline">\(k =1,\cdots,K\)</span>.</p>
<p>Then we <u>form <span class="math inline">\(K\)</span> clusters by ﬁnding neighboring votes to each of the <span class="math inline">\(v_{i_k}\)</span>’s 3D location</u>: <span class="math inline">\(\mathcal{C}_{k}=\left\{v_{i}^{(k)} \mid\left\|v_{i}-v_{i_{k}}\right\| \leq r\right\}\)</span> for <span class="math inline">\(k =1,\cdots,K\)</span>.</p>
<p>之后, 根据采样到的<span class="math inline">\(K\)</span>个点, 作者根据空间距离进行聚类产生<span class="math inline">\(K\)</span>类, 得到<span class="math inline">\(\mathcal{C}_{k}=\left\{v_{i}^{(k)} \mid\left\|v_{i}-v_{i_{k}}\right\| \leq r\right\}\)</span>, 其中<span class="math inline">\(k =1,\cdots,K\)</span>.</p>
<h5 id="proposal-and-classiﬁcation-from-vote-clusters">Proposal and classiﬁcation from vote clusters</h5>
<p>As a vote cluster is in essence a set of high-dim points, we can <u>leverage a generic point set learning network to aggregate the votes in order to generate object proposals</u>.</p>
<p>由于投票簇本质上是一组高维点, 作者利用点云神经网络来聚合投票点, 以生成物体候选框.</p>
<p>Compared to the back-tracing step of traditional Hough voting for identifying the object boundary, this procedure allows to <u>propose amodal boundaries even from partial observations, as well as predicting other parameters like orientation, class, etc</u>.</p>
<p>与传统的霍夫投票识别对象边界的回溯步骤相比, 此过程允许根据部分观测值预测边界, 同时预测其他参数, 如方向、类别等.</p>
<p>In our implementation, we <u>use a shared PointNet for vote aggregation and proposal in clusters</u>.</p>
<p>作者使用一个共享的PointNet来进行簇中的投票点聚合和候选框的生成.</p>
<p>Given a vote cluster <span class="math inline">\(\mathcal{C}=\left\{w_{i}\right\}\)</span> with <span class="math inline">\(i=1, \ldots, n\)</span> and its cluster center <span class="math inline">\(w_{j}\)</span>, where <span class="math inline">\(w_{i}=\left[z_{i} ; h_{i}\right]\)</span> with <u><span class="math inline">\(z_{i} \in \mathbb{R}^{3}\)</span> as the vote location and <span class="math inline">\(h_{i} \in \mathbb{R}^{C}\)</span> as the vote feature</u>.</p>
<p>投票点簇<span class="math inline">\(\mathcal{C}=\left\{w_{i}\right\}\)</span>, 其中<span class="math inline">\(i=1, \ldots, n\)</span>, 以及簇的中心<span class="math inline">\(w_{j}\)</span>, 且<span class="math inline">\(w_{i}=\left[z_{i} ; h_{i}\right]\)</span>, 其中<span class="math inline">\(z_{i} \in \mathbb{R}^{3}\)</span>是投票点的位置, <span class="math inline">\(h_{i} \in \mathbb{R}^{C}\)</span>是投票点特征.</p>
<p>To enable usage of local vote geometry, we <u>transform vote locations to a local normalized coordinate system by <span class="math inline">\(z_{i}^{\prime}=\left(z_{i}-z_{i}\right) / r\)</span></u>.</p>
<p>为了利用局部投票点的集合特征, 作者将投票点进行坐标归一化到局部坐标系, 归一化方式为<span class="math inline">\(z_{i}^{\prime}=\left(z_{i}-z_{i}\right) / r\)</span>.</p>
<p>Then an object proposal for this cluster <span class="math inline">\(p(C)\)</span> is generated by <u>passing the set input through a PointNet-like module</u>:</p>
<p>然后, 通过将投票点集合传递给类似PointNet的模块, 生成集群<span class="math inline">\(p(C)\)</span>的候选框: <span class="math display">\[
p(\mathcal{C})=\operatorname{MLP}_{2}\left\{\max _{i=1, \ldots, n}\left\{\operatorname{MLP}_{1}\left(\left[z_{i}^{\prime} ; h_{i}\right]\right)\right\}\right\}
\]</span> where votes from each cluster are <u>independently processed by a <span class="math inline">\(\text{MLP}_1\)</span> before being max-pooled (channel-wise) to a single feature vector and passed to <span class="math inline">\(\text{MLP}_2\)</span> where information from different votes are further combined</u>.</p>
<p>其中, 来自每个簇的投票点由<span class="math inline">\(\text{MLP}_1\)</span>独立处理, 之后通过最大池化(通道)到单个特征向量并传递到<span class="math inline">\(\text{MLP}_2\)</span>中, 其中来自不同投票点的信息进一步合并.</p>
<p>We represent the proposal <span class="math inline">\(p\)</span> as a multidimensional vector with <u>an objectness score, bounding box parameters (center, heading and scale) and semantic classiﬁcation scores</u>.</p>
<p>作者将候选框<span class="math inline">\(p\)</span>表示为一个多维向量, 带有物体性分数、边界框参数(中心、朝向角和尺寸)和语义分类分数.</p>
<h5 id="loss-function">Loss function</h5>
<p>The loss functions in the proposal and classiﬁcation stage consist of <u>objectness, bounding box estimation, and semantic classiﬁcation losses</u>.</p>
<p>候选框生成和分类阶段的损失函数包括对象性损失函数、边界框估计损失函数和语义分类损失函数.</p>
<p>We supervise the objectness scores for votes that are located <u>either close to a ground truth object center (within 0.3 meters) or far from any center (by more than 0.6 meters)</u>.</p>
<p>作者监督投票点的物体性分数, 这些投票点要么靠近物体边界框中心(0.3米以内), 要么远离任何中心(超过0.6米).</p>
<p>We consider proposals generated from those votes as <u>positive and negative proposals</u>, respectively.</p>
<p>作者认为从这些投票点中产生的候选框分别是积极的和消极的.</p>
<p>Objectness predictions for other proposals are <u>not penalized</u>.</p>
<p>其他候选框的无效预测不受惩罚.</p>
<p>Objectness is <u>supervised via a cross entropy loss normalized by the number of non-ignored proposals in the batch</u>.</p>
<p>物体性通过一个交叉熵损失函数进行监督, 该损失使用批中未被忽略的候选框的个数进行标准化.</p>
<p>For positive proposals we further <u>supervise the bounding box estimation and class prediction according to the closest ground truth bounding box</u>.</p>
<p>对于正向的候选框, 作者用与其最接近的真实值, 监督边界框估计和类别预测.</p>
<p>Speciﬁcally, we follow which <u>decouples the box loss to center regression, heading angle estimation and box size estimation</u>.</p>
<p>具体而言, 作者是将边界框损失解耦为中心点损失、朝向角损失和边界框尺寸损失.</p>
<p>For semantic classiﬁcation we <u>use the standard cross entropy loss</u>.</p>
<p>对于语义分类, 作者使用交叉熵损失函数.</p>
<p>In all regression in the detection loss we <u>use the Huber (smooth-<span class="math inline">\(L_1\)</span>) loss</u>.</p>
<p>在所有检测损失函数中, 作者使用Huber(<span class="math inline">\(L_1\)</span>平滑)损失.</p>
<h4 id="implementation-details部分">Implementation Details部分</h4>
<h5 id="input-and-data-augmentation">Input and data augmentation</h5>
<p>Input to our detection network is <u>a point cloud of <span class="math inline">\(N\)</span> points randomly sub-sampled from either a popped-up depth image (<span class="math inline">\(N = 20k\)</span>) or a 3D scan (mesh vertices, <span class="math inline">\(N = 40k\)</span>)</u>.</p>
<p>VoteNet的输入是从深度图像(<span class="math inline">\(N=20k\)</span>)或三维扫描(网格顶点, <span class="math inline">\(N=40k\)</span>)中随机采样<span class="math inline">\(N\)</span>点的点云.</p>
<p>In addition to XYZ coordinates, we also include <u>a height feature</u> for each point indicating its distance to the ﬂoor.</p>
<p>除XYZ坐标外, 输入还包括每个点的高度特征, 以指示其到地面的距离.</p>
<p>The ﬂoor height is <u>estimated as the <span class="math inline">\(1\%\)</span> percentile of all points’ heights</u>.</p>
<p>地面高度估计为所有点的高度的<span class="math inline">\(1\%\)</span>.</p>
<p>To augment the training data, we <u>randomly sub-sample the points from the scene points</u> on-the-ﬂy.</p>
<p>为了增加训练数据, 作者从场景点云上随机进行采样.</p>
<p>We also randomly ﬂip the point cloud in both horizontal direction, <u>randomly rotate the scene points by Uniform<span class="math inline">\([-5\degree, 5\degree]\)</span> around the upright-axis, and randomly scale the points by Uniform<span class="math inline">\([0.9, 1.1]\)</span></u>.</p>
<p>作者在旋转和尺度上随机调整点云, 围绕垂直轴以均匀分布<span class="math inline">\([-5\degree，5\degree]\)</span>随机旋转点云, 以均匀分布<span class="math inline">\([0.9，1.1]\)</span>随机缩放点云.</p>
<h5 id="network-architecture-details">Network architecture details</h5>
<p>The <u>backbone feature learning network is based on PointNet++</u>, which has four set abstraction (SA) layers and two feature propagation/upsamplng (FP) layers, where the SA layers have increasing receptive radius of <span class="math inline">\(0.2\)</span>, <span class="math inline">\(0.4\)</span>, <span class="math inline">\(0.8\)</span> and <span class="math inline">\(1.2\)</span> in meters while they sub-sample the input to <span class="math inline">\(2048\)</span>, <span class="math inline">\(1024\)</span>, <span class="math inline">\(512\)</span> and <span class="math inline">\(256\)</span> points respectively.</p>
<p>主干特征学习网络基于PointNet++, 它有四个集合抽象(SA)层和两个特征传播/上采样(FP)层，其中集合抽象层的感知半径为<span class="math inline">\(0.2\)</span>, <span class="math inline">\(0.4\)</span>, <span class="math inline">\(0.8\)</span>和<span class="math inline">\(1.2\)</span>, 以米为单位, 同时它们将输入分采样为<span class="math inline">\(2048\)</span>, <span class="math inline">\(1024\)</span>, <span class="math inline">\(512\)</span>和<span class="math inline">\(256\)</span>.</p>
<p>The two FP layers up-sample the 4th SA layer’s output back to <span class="math inline">\(1024\)</span> points <u>with 256-dim features and 3D coordinates</u>.</p>
<p>两个上采样层向上采样第四个集合抽象层的输出, 输出<span class="math inline">\(1024\)</span>个点, 具有<span class="math inline">\(256\)</span>维的特征和三维坐标.</p>
<p>The voting layer is realized through a multi-layer perceptron with FC output sizes of <span class="math inline">\(256\)</span>, <span class="math inline">\(256\)</span>, <span class="math inline">\(259\)</span>, where <u>the last FC layer outputs XYZ offset and feature residuals</u>.</p>
<p>投票层通过多层感知器实现, 全连接层输出大小为<span class="math inline">\(256\)</span>, <span class="math inline">\(256\)</span>, <span class="math inline">\(259\)</span>, 其中最后一个全连接层输出XYZ偏移量和特征残差.</p>
<p>The proposal module is implemented as <u>a set abstraction layer with a post processing <span class="math inline">\(\text{MLP}_2\)</span> to generate proposals after the max-pooling</u>.</p>
<p>候选框生成模块是一个带有后处理的<span class="math inline">\(\text{MLP}_2\)</span>的集合抽象层, 用于在最大池之后生成候选框.</p>
<p>The SA uses radius <span class="math inline">\(0.3\)</span> and <span class="math inline">\(\text{MLP}_1\)</span> with output sizes of <span class="math inline">\(128\)</span>, <span class="math inline">\(128\)</span>, <span class="math inline">\(128\)</span>.</p>
<p>SA使用的半径为<span class="math inline">\(0.3\)</span>, <span class="math inline">\(\text{MLP}_1\)</span>输出大小分别为<span class="math inline">\(128\)</span>, <span class="math inline">\(128\)</span>, <span class="math inline">\(128\)</span>.</p>
<p>The max-pooled features are further processed by <span class="math inline">\(\text{MLP}_2\)</span> with output sizes of <span class="math inline">\(128\)</span>, <span class="math inline">\(128\)</span>, <span class="math inline">\(5+2NH+4NS+NC\)</span> where the output consists of <u><span class="math inline">\(2\)</span> objectness scores, <span class="math inline">\(3\)</span> center regression values, <span class="math inline">\(2NH\)</span> numbers for heading regression (<span class="math inline">\(NH\)</span> heading bins) and <span class="math inline">\(4NS\)</span> numbers for box size regression (<span class="math inline">\(NS\)</span> box anchors) and <span class="math inline">\(NC\)</span> numbers for semantic classiﬁcation</u>.</p>
<p>最大池特征由<span class="math inline">\(\text{MLP}_2\)</span>进一步处理, 输出大小为<span class="math inline">\(128\)</span>, <span class="math inline">\(128\)</span>, <span class="math inline">\(5+2NH+4NS+NC\)</span>, 其中输出包括<span class="math inline">\(2\)</span>个对象性分数, <span class="math inline">\(3\)</span>个中心回归值, 朝向回归为<span class="math inline">\(2NH\)</span>个数值(<span class="math inline">\(NH\)</span>个朝向锚点), 检测框大小回归为<span class="math inline">\(4NS\)</span>个数值(<span class="math inline">\(NS\)</span>个锚框), 语义分类结果为<span class="math inline">\(NC\)</span>个数值.</p>
<h5 id="training-the-network">Training the network</h5>
<p>We train the entire network end-to-end and from scratch with an <u>Adam optimizer</u>, <u>batch size <span class="math inline">\(8\)</span> and an initial learning rate of <span class="math inline">\(0.001\)</span></u>.</p>
<p>作者使用Adam优化器对整个网络进行从头开始的端到端培训, 批量大小为<span class="math inline">\(8\)</span>, 初始学习率为<span class="math inline">\(0.001\)</span>.</p>
<p>The learning rate is <u>decreased by <span class="math inline">\(10\times\)</span> after <span class="math inline">\(80\)</span> epochs and then decreased by another <span class="math inline">\(10\times\)</span> after <span class="math inline">\(120\)</span> epochs</u>.</p>
<p>学习率在<span class="math inline">\(80\)</span>代后除<span class="math inline">\(10\)</span>, 在<span class="math inline">\(120\)</span>代后再除<span class="math inline">\(10\)</span>.</p>
<p>Training the model to convergence on one Volta Quadro GP100 GPU takes <u>around <span class="math inline">\(10\)</span> hours on SUN RGB-D and less than <span class="math inline">\(4\)</span> hours on ScanNetV2</u>.</p>
<p>在单张Volta Quadro GP100 GPU上训练模型, SUN RGB-D大约需要<span class="math inline">\(10\)</span>小时, ScanNetV2不到<span class="math inline">\(4\)</span>小时.</p>
<h5 id="inference">Inference</h5>
<p>Our VoteNet is able to <u>take point clouds of the entire scenes and generate proposals in one forward pass</u>.</p>
<p>VoteNet能够获取整个场景的点云并在一次前向传播中生成物体候选框.</p>
<p>The proposals are post-processed by a 3D NMS module <u>with an IoU threshold of <span class="math inline">\(0.25\)</span></u>.</p>
<p>三维候选框经过三维NMS模块进行后处理, IoU阈值为<span class="math inline">\(0.25\)</span>.</p>
<h3 id="experiments部分">Experiments部分</h3>
<h4 id="comparing-with-state-of-the-art-methods部分">Comparing with State-of-the-art Methods部分</h4>
<p>SUN RGB-D is a <u>single-view RGB-D dataset for 3D scene understanding</u>.</p>
<p>SUN RGB-D是用于三维场景理解的单视图RGB-D数据集.</p>
<p>It consists of ~5K RGB-D training images annotated with amodal oriented 3D bounding boxes for <u>37 object categories</u>.</p>
<p>它由约5K个RGB-D训练图像组成, 这些图像有向三维边界框标注37个对象类别.</p>
<p>To feed the data to our network, we ﬁrstly <u>convert the depth images to point clouds using the provided camera parameters</u>.</p>
<p>为了将数据提供给网络, 作者首先使用提供的相机参数将深度图像转换为点云.</p>
<p>We follow a standard evaluation protocol and <u>report performance on the 10 most common categories</u>.</p>
<p>作者报告10个最常见类别的性能.</p>
<p>ScanNetV2 is a richly annotated dataset of <u>3D reconstructed meshes of indoor scenes</u>.</p>
<p>ScanNetV2是一个具有丰富注释的室内场景三维重建网格数据集.</p>
<p>It contains 1.2K training examples collected from hundreds of different rooms, and <u>annotated with semantic and instance segmentation for 18 object categories</u>.</p>
<p>它包含从数百个不同房间收集的1.2K训练示例, 并对18个对象类别进行语义和实例分割注释.</p>
<p>Compared to partial scans in SUN RGB-D, <u>scenes in ScanNetV2 are more complete and cover larger areas with more objects on average</u>.</p>
<p>与SUN RGB-D中的部分扫描相比, ScanNetV2中的场景更完整, 平均覆盖面积更大, 对象更多.</p>
<p>We <u>sample vertices from the reconstructed meshes as our input point clouds</u>.</p>
<p>作者从重建的网格中采样顶点作为输入点云.</p>
<p>Since ScanNetV2 does not provide amodal or oriented bounding box annotation, we aim to <u>predict axis-aligned bounding boxes instead</u>.</p>
<p>由于ScanNetV2不提供有向边界框注释, 因此作者的目标是预测轴对齐的边界框.</p>
<p><u>3D object detection results on SUN RGB-D val set</u>. Evaluation metric is average precision with 3D IoU threshold 0.25.</p>
<p>在SUN RGB-D验证集上的三维对象检测结果. 评估指标是3D IoU阈值为0.25的mAP.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131028819.png" srcset="/img/loading.gif" lazyload /></p>
<p><u>3D object detection results on ScanNetV2 val set</u>.</p>
<p>ScanNetV2验证集上的三维对象检测结果.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131028926.png" srcset="/img/loading.gif" lazyload /></p>
<p>VoteNet <u>outperforms all previous methods</u> by at least 3.7 and 18.4 mAP increase in SUN RGB-D and ScanNet respectively.</p>
<p>VoteNet在SUN RGB-D和ScanNet中的mAP分别至少增加了3.7和18.4, 优于所有以前的方法.</p>
<p>Notably, we achieve such improvements when we <u>use geometric input (point clouds) only</u> while they used both geometry and RGB images.</p>
<p>值得注意的是, 作者只使用几何输入(点云).</p>
<p>Importantly, <u>the same set of network hyperparameters was used in both datasets</u>.</p>
<p>重要的是, 两个数据集中使用了相同的网络超参数进行训练.</p>
<h4 id="analysis-experiments部分">Analysis Experiments部分</h4>
<h5 id="to-vote-or-not-to-vote">To Vote or Not To Vote?</h5>
<p>A straightforward baseline to VoteNet is a network that <u>directly proposes boxes from sampled scene points</u>.</p>
<p>VoteNet的一个简单的基线是一个直接从采样的场景点提出边界框的网络.</p>
<p>Such a baseline – which we refer to as <em>BoxNet</em> – is <u>essential to distill the improvement due to voting</u>.</p>
<p>这样一个基线, 作者称之为<em>BoxNet</em>, 这对于对比投票带来的改进至关重要.</p>
<p>The BoxNet has the same backbone as the VoteNet but instead of voting, <u>it directly generates boxes from the seed points</u>.</p>
<p>BoxNet与VoteNet具有相同的主干, 但它没有投票模块, 而是直接从种子点生成边界框.</p>
<p>The following table shows <u>voting boosts the performance by a signiﬁcant margin of ~5 mAP on SUN RGB-D and &gt;13 mAP on ScanNet</u>.</p>
<p>在SUN RGB-D上, 投票显著提高了约5点的mAP的性能, 在ScanNet上, 投票显著提高了约13点的mAP的性能.</p>
<p><u>Comparing VoteNet with a no-vote baseline</u>. Metric is 3D object detection mAP.</p>
<p>将VoteNet与BoxNet进行比较. 评价指标是三维目标检测mAP.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131029273.png" srcset="/img/loading.gif" lazyload /></p>
<p>In what ways, then, <u>does voting help</u>?</p>
<p>那么，投票在哪些方面有帮助呢?</p>
<p>We argue that since in sparse 3D point clouds, existing scene points are often far from object centroids, <u>a direct proposal may have lower conﬁdence and inaccurate amodal boxes</u>.</p>
<p>作者认为, 由于在稀疏的3D点云中, 现有的场景点通常远离对象质心, 因此直接建议可能具有较低的可信度和不准确三维边界框.</p>
<p>Voting, instead, brings closer together these lower conﬁdence points and <u>allows to reinforce their hypothesis though aggregation</u>.</p>
<p>相反, 投票使这些较低的置信点更紧密地结合在一起并允许通过聚集来强化其假设.</p>
<p>Voting helps <u>increase detection contexts</u>.</p>
<p>投票有助于增加检测上下文.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131031595.png" srcset="/img/loading.gif" lazyload /></p>
<p>Seed points that generate good boxes (BoxNet), or good votes (VoteNet) which in turn generate good boxes, are overlaid (in blue) <u>on top of a representative ScanNet scene</u>.</p>
<p>生成好框的种子点(BoxNet)或生成好框的好投票(VoteNet)在ScanNet场景上的覆盖情况(蓝色).</p>
<p>As the voting step effectively increases context, VoteNet <u>demonstrates a much denser cover of the scene</u>, therefore increasing the likelihood of accurate detection.</p>
<p>由于投票步骤有效地增加了上下文信息, VoteNet展示了更密集的场景覆盖, 因此增加了准确检测的可能性.</p>
<p>We proceed with a second analysis in following figure showing on the same plot (in separate scales), for each SUN RGB-D category: <u>(in blue dots) gains in mAP between VoteNet and BoxNet, and (in red squares) closest distances between object points (on their surfaces) and their amodal box centers, averaged per category and normalized by the mean class size (a large distance means the object center is usually far from its surface)</u>.</p>
<p>作者在下图中继续进行第二次分析, 在同一个图上(以单独的比例)显示每个SUN RGB-D类别:(蓝色点)VoteNet和BoxNet之间的mAP增益, 以及(红色正方形)物体点(在其表面上)与其有向边界框中心之间的最近距离, 按类别平均并按平均类大小标准化(较大的距离意味着对象中心通常远离其表面).</p>
<p><u>Sorting the categories according to the former</u>, we see a strong correlation.</p>
<p>根据前者对类别进行排序, 作者发现有很强的相关性.</p>
<p>Namely, <u>when object points tend to be further from the amodal box center, voting helps much more</u>.</p>
<p>也就是说, 当对象点趋向于离边界框中心更远时, 投票的帮助更大.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131033019.png" srcset="/img/loading.gif" lazyload /></p>
<h5 id="effect-of-vote-aggregation">Effect of Vote Aggregation</h5>
<p>In following figure (right), we show that <u>vote aggregation with a learned Pointnet and max pooling achieves far better results than manually aggregating the vote features in the local regions due to the existence of clutter votes</u> (i.e. votes from non-object seeds).</p>
<p>在下图(右图)中, 由于存在噪音票(即来自非物体种子点的投票), 使用PointNet和最大池的投票聚合比手动聚合局部区域中的投票特征获得更好的结果.</p>
<p>In following figure (left), we show <u>how vote aggregation radius affects detection</u> (tested with PointNet using max pooling).</p>
<p>在下图(左图)中, 展示了投票聚合半径如何影响检测结果(使用包含最大池的PointNet进行测试).</p>
<p><u>Vote aggregation analysis</u>.</p>
<p>投票聚合分析.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131036132.png" srcset="/img/loading.gif" lazyload /></p>
<p>Left: <u>mAP@0.25 on SUN RGB-D for varying aggregation radii when aggregating via Pointnet (max)</u>.</p>
<p>左: mAP@0.25, 在SUN RGB-D上, 在通过PointNet(最大值池化)聚合时改变聚合半径.</p>
<p>Right: <u>Comparisons of different aggregation methods (radius = 0.3 for all methods)</u>.</p>
<p>右: 不同聚合方法的比较(所有方法的聚合半径为0.3m).</p>
<p><u>Using a learned vote aggregation is far more effective than manually pooling the features</u> in a local neighborhood.</p>
<p>使用学习的投票聚合比手动聚合特征要有效得多.</p>
<h5 id="model-size-and-speed">Model Size and Speed</h5>
<p>Our proposed model is very efﬁcient since <u>it leverages sparsity in point clouds and avoids search in empty space</u>.</p>
<p>作者提出的模型非常有效, 因为它利用了点云中的稀疏性, 避免了在空白空间中搜索.</p>
<p>Compared to previous best methods, our model is more than <span class="math inline">\(4\times\)</span> smaller than FPointNet (the prior art on SUN RGB-D) in size and more than <span class="math inline">\(20\times\)</span> times faster than 3D-SIS (the prior art on ScanNetV2) in speed.</p>
<p>与以前的最佳方法相比, 作者的模型在尺寸上比FPointNet(SUN RGB-D上的现有技术)小4倍多, 在速度上比3D-SIS(ScanNetV2上的现有技术)快20倍多.</p>
<p><u>Model size and processing time (per frame or scan)</u>.</p>
<p>模型大小和处理时间(每帧或扫描).</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131039905.png" srcset="/img/loading.gif" lazyload /></p>
<h4 id="qualitative-results-and-discussion部分">Qualitative Results and Discussion部分</h4>
<p><u>Qualitative results of 3D object detection in ScanNetV2</u>. Left: our VoteNet, Right: ground-truth.</p>
<p>ScanNetV2中三维目标检测的定性结果. 左: 作者的检测结果, 右: 真实值.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131041972.png" srcset="/img/loading.gif" lazyload /></p>
<p><u>Qualitative results on SUN RGB-D</u>. Both left and right panels show (from left to right): an image of the scene (not used by our network), 3D object detection by VoteNet, and ground-truth annotations.</p>
<p>SUN RGB-D上的定性结果. 左右均显示(从左到右): 场景图像(VoteNet未使用)、VoteNet的三维对象检测结果和真实值.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131042169.png" srcset="/img/loading.gif" lazyload /></p>
<p>There are still <u>limitations in our method</u> though.</p>
<p>不过，作者的方法仍然存在局限性.</p>
<p>Common failure cases include <u>misses on very thin objects</u> like doors, windows and pictures denoted in black bounding boxes in the top scene.</p>
<p>常见的失败情况包括对非常薄的对象(如门、窗和顶部场景中用黑色边框表示的图片)检测失败.</p>
<p>As we do <u>not make use of RGB information</u>, detecting these categories is almost impossible.</p>
<p>由于我们不使用RGB信息, 检测这些类别几乎是不可能的.</p>
<p>A less successful amodal prediction is shown in the bottom right scene where an extremely partial observation of a very large table is given.</p>
<p>在SUN RGB-D定性结果的右下角的场景中显示了一个不太成功的有向边界框预测, 其中给出了一个非常大的桌子的极小部分观测数据.</p>
<h3 id="supplement部分">Supplement部分</h3>
<h4 id="details-on-architectures-and-loss-functions部分">Details on Architectures and Loss Functions部分</h4>
<h5 id="votenet-architecture-details">VoteNet architecture details</h5>
<p>The backbone network, based on the PointNet++ architecture, has four set abstraction layers and two feature up-sampling layers.</p>
<p>主干网络基于PointNet++的体系结构, 有四个集合抽象层和两个特征上采样层.</p>
<p>The <u>detailed layer parameters are shown as follow</u>.</p>
<p>详细的图层参数如下所示.</p>
<p><u>Backbone network architecture</u>: layer specifications.</p>
<p>主干网络架构.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131349652.png" srcset="/img/loading.gif" lazyload /></p>
<p>Each set abstraction (SA) layer has a receptive ﬁeld speciﬁed by a ball-region radius <span class="math inline">\(r\)</span>, a MLP network for point feature transform <span class="math inline">\(M L P\left[c_{1}, \ldots, c_{k}\right]\)</span> where <u><span class="math inline">\(c_i\)</span> is output channel number of the <span class="math inline">\(i\)</span>-th layer in the MLP</u>.</p>
<p>每个集合抽象(SA)层都有一个由半径<span class="math inline">\(r\)</span>指定的感受野, 一个用于点特征变换的MLP网络<span class="math inline">\(M L P\left[c_{1}, \ldots, c_{k}\right]\)</span>, 其中<span class="math inline">\(c_i\)</span>是MLP中第<span class="math inline">\(i\)</span>层的输出通道数.</p>
<p>The SA layer also subsamples the input point cloud with farthest point sampling to <span class="math inline">\(n\)</span> points.</p>
<p>SA层还对输入点云进行二次采样, 使用最远点采样为<span class="math inline">\(n\)</span>个点.</p>
<p>Each SA layer is speciﬁed by <span class="math inline">\(\left(n, r,\left[c_{1}, \ldots, c_{k}\right]\right)\)</span>.</p>
<p>每个SA层由<span class="math inline">\(\left(n, r,\left[c_{1}, \ldots, c_{k}\right]\right)\)</span>指定.</p>
<p>We also normalize the XYZ scale of points in each local region by the region radius.</p>
<p>通过区域半径标准化每个局部区域中点的XYZ尺度.</p>
<p>Each set feature propagation (FP) layer upsamples the point features by <u>interpolating the features on input points to output points</u> (each output point’s feature is weighted average of its three nearest input points’ features).</p>
<p>每个集合特征传播(FP)层通过将输入点上的特征插值到输出点来上采样点特征(每个输出点的特征是其最近三个输入点特征的加权平均值).</p>
<p>It also combines the skip-linked features through a MLP (interpolated features and skip-linked features are concatenated before fed into the MLP).</p>
<p>它还通过MLP组合跳接的特征(插入的特征和跳接的特征在进入到MLP之前连接在一起).</p>
<p>Each FP layer is speciﬁed by <span class="math inline">\(\left[c_{1}, \ldots, c_{k}\right]\)</span> where <span class="math inline">\(c_i\)</span> is the output of the <span class="math inline">\(i\)</span>-th layer in the MLP.</p>
<p>每个FP层由<span class="math inline">\(\left[c_{1}, \ldots, c_{k}\right]\)</span>指定, 其中<span class="math inline">\(c_i\)</span>是MLP中第<span class="math inline">\(i\)</span>层的输出.</p>
<p>The voting module as mentioned in the main paper is a MLP that <u>transforms seeds’ features to votes including a XYZ offset and a feature offset</u>.</p>
<p>投票模块是一个MLP, 它将种子点的特征转换为投票点, 包括XYZ偏移和特征偏移.</p>
<p>The seed points are outputs of the fp2 layer.</p>
<p>种子点是fp2层的输出.</p>
<p>The voting module MLP has output sizes of 256, 256, 259 for its fully connected layers.</p>
<p>投票模块MLP的全连接层的输出大小为256、256、259.</p>
<p>The last fully connected layer does not have ReLU or BatchNorm.</p>
<p>最后一个完全连接的层没有ReLU或BatchNorm.</p>
<p>The proposal module as mentioned in the main paper is a SA layer followed by another MLP after the max-pooling in each local region.</p>
<p>候选框生成模块是一个SA层, 在每个局部区域的最大池之后是另一个MLP.</p>
<p>The layer’s output has <span class="math inline">\(5 + 2N H + 4N S + N C\)</span> channels where <u><span class="math inline">\(N H\)</span> is the number of heading bins (we predict a classiﬁcation score for each heading bin and a regression offset for each bin – relative to the bin center and normalized by the bin size)</u>, <u><span class="math inline">\(N S\)</span> is the number of size templates (we predict a classiﬁcation score for each size template and 3 scale regression offsets for height, width and length)</u> and <u><span class="math inline">\(N C\)</span> is the number of semantic classes</u>.</p>
<p>该层的输出具有<span class="math inline">\(5+2N H+4N s+N C\)</span>个通道，其中<span class="math inline">\(N H\)</span>是朝向角类别数(预测每个朝向角的分类分数和回归偏移量), <span class="math inline">\(N S\)</span>是尺寸类别数(预测每个尺寸模板的分类分数以及高度、宽度和长度的3个尺度回归偏移量), 而<span class="math inline">\(N C\)</span>是语义类别的数量.</p>
<p>In SUN RGB-D: <span class="math inline">\(N H = 12\)</span>, <span class="math inline">\(N S = N C = 10\)</span>, in ScanNet: <span class="math inline">\(N H = 12\)</span>, <span class="math inline">\(N S = N C = 18\)</span>.</p>
<p>在SUN RGB-D中: <span class="math inline">\(N H=12\)</span>, <span class="math inline">\(N S=N C=10\)</span>, 在ScanNet中: <span class="math inline">\(N H=12\)</span>, <span class="math inline">\(N S=N C=18\)</span>.</p>
<p>In the ﬁrst 5 channels, the ﬁrst two are for objectness classiﬁcation and the rest three are for center regression (relative to the vote cluster center).</p>
<p>在前5个通道中, 前两个用于对象分类, 其余三个用于中心回归(相对于投票聚类中心).</p>
<h5 id="votenet-loss-function-details">VoteNet loss function details</h5>
<p>The network is trained end-to-end with a multi-task loss including <u>a voting loss, an objectness loss, a 3D bounding box estimation loss and a semantic classiﬁcation loss</u>.</p>
<p>网络通过多损失进行端到端训练, 包括投票损失、物体分数损失、三维边界框估计损失和语义分类损失.</p>
<p>We weight the losses such that they are in similar scales with <span class="math inline">\(\lambda_1 = 0.5\)</span>, <span class="math inline">\(\lambda_2 = 1\)</span> and <span class="math inline">\(\lambda_3 = 0.1\)</span>.</p>
<p>作者对损失进行加权, 使其处于类似的比例, 分别为<span class="math inline">\(\lambda_1=0.5\)</span>、<span class="math inline">\(\lambda_2=1\)</span>和<span class="math inline">\(\lambda_3=0.1\)</span>. <span class="math display">\[
L_{\text {VoteNet }}=L_{\text {vote-reg }}+\lambda_{1} L_{\text {obj-cls }}+\lambda_{2} L_{\text {box }}+\lambda_{3} L_{\text {sem-cls }}
\]</span></p>
<p>Among the losses, the vote regression loss is as deﬁned in the main paper (with L1 distance).</p>
<p>在损失中, 投票回归损失如正文中的定义(L1距离).</p>
<p>For ScanNet we <u>compute the ground truth votes as offset from the mesh vertices of an instances to the centers of the axis-aligned tight bounding boxes of the instances</u>.</p>
<p>对于ScanNet, 计算从实例的mesh顶点到实例的轴对齐紧边界框中心的偏移作为真实投票点的偏移.</p>
<p>Note that since the bounding box is not amodal, they can vary in sizes due to scan completeness (e.g. a chair may have a ﬂoating bounding box if its leg is not recovered from the reconstruction).</p>
<p>请注意，由于扫描的完整程度, 其大小可能会有所不同(例如, 如果椅子的腿未从重建中恢复则椅子可能有一个悬浮的边界框).</p>
<p>For SUN RGB-D since <u>the dataset does not provide instance segmentation annotations but only amodal bounding boxes</u>, we cannot compute a ground truth vote directly (as we don’t know which points are on objects).</p>
<p>对于SUN RGB-D, 由于数据集不提供实例分割注释, 而只提供有向边界框, 因此无法直接计算真实投票点(因为不知道对象上有哪些点).</p>
<p>Instead, we <u>consider any point inside an annotated bounding box as an object point (required to vote) and compute its offset to the box center as the ground truth</u>.</p>
<p>相反, 作者将注释框中的任何点视为投票点并将其偏移到框中心作为真实值.</p>
<p>In cases that a point is in multiple ground truth boxes, we keep a set of up to three ground truth votes, and <u>consider the minimum distance between the predicted vote and any ground truth vote in the set during vote regression on this point</u>.</p>
<p>在一个点位于多个真实边界框的情况下, 作者保持最多三个真实投票点并考虑在预测投票点和聚合过程中的任何具有与真实投票点之间的最小距离的预测投票点回归在这一真实投票点上.</p>
<p>The <u>objectness loss is just a cross-entropy loss for two classes</u> and the <u>semantic classiﬁcation loss is also a cross-entropy loss of <span class="math inline">\(N C\)</span> classes</u>.</p>
<p>物体分数损失是两个类的交叉熵损失, 语义分类损失也是<span class="math inline">\(N C\)</span>类的交叉熵损失.</p>
<p>The box loss follows (but without the corner loss regularization for simplicity) and is <u>composed of center regression, heading estimation and size estimation sub-losses</u>.</p>
<p>边界框损失(但为了简单起见, 没有角度损失正则化)由边界框中心回归、朝向角估计和尺寸估计等三个子损失组成.</p>
<p>In all regression in the box loss we use the <u>robust L1-smooth loss</u>.</p>
<p>在所有边界框回归损失中, 作者使用稳健的L1平滑损失.</p>
<p>Both the box and semantic losses are <u>only computed on positive vote clusters</u> and <u>normalized by the number of positive clusters</u>.</p>
<p>边界框和语义损失仅在正向投票点簇上计算并通过正向投票点簇的数量进行归一化. <span class="math display">\[
L_{\text {box }} =L_{\text {center-reg }}+0.1 L_{\text {angle-cls }}+L_{\text {angle-reg }} +0.1 L_{\text {size-cls }}+L_{\text {size-reg }}
\]</span> One difference though is that, instead of a naïve regression loss, we <u>use a <em>Chamfer loss</em> for <span class="math inline">\(L_{\text{center-reg}}\)</span> (between regressed centers and ground truth box centers)</u>.</p>
<p>不过, 一个不同之处是作者对<span class="math inline">\(L_{\text{center reg}}\)</span>(在回归中心点和真实中心点之间)使用Chamfer损失<span class="math inline">\(L_{\text{center-reg}}\)</span>而不是简单的回归损失.</p>
<p>It requires that each positive proposal is close to a ground truth object and each ground truth object center has a proposal near it.</p>
<p>这要求每个正向提案靠近一个真实物体, 每个真实物体中心在其附近有一个候选框.</p>
<p>The latter part also inﬂuences the voting in the sense that it encourages non-object seed points near the object to also vote for the center of the object, which <u>helps further increase contexts in detection</u>.</p>
<p>后一部分也影响投票, 因为它鼓励靠近物体的噪音种子点也投票给物体的中心, 这有助于进一步增加检测的上下文信息.</p>
<h5 id="boxnet-architecture-details">BoxNet architecture details</h5>
<p>Our baseline network without voting, BoxNet, shares most parts with the VoteNet.</p>
<p>BoxNet与VoteNet共享大部分内容.</p>
<p>They <u>share the same backbone architecture</u>.</p>
<p>它们共享相同的主干架构.</p>
<p>But instead of voting from seeds, the BoxNet <u>directly proposes bounding boxes and classiﬁes object classes from seed points’ features</u>.</p>
<p>但是, BoxNet没有从种子中投票, 而是直接预测边界框并根据种子点的特征对物体进行分类.</p>
<p>To make the BoxNet and VoteNet have similar capacity we also <u>include a SA layer for the proposal in BoxNet</u>.</p>
<p>为了使BoxNet和VoteNet具有类似的容量, 作者在BoxNet中的候选框生成模块中添加了SA层.</p>
<p>However this SA layer takes seed clusters instead of vote clusters i.e. it samples seed points and then combines neighboring seeds with MLP and max-pooling.</p>
<p>但是, 该SA层采用种子点簇而不是投票点簇, 即它对种子点进行采样, 然后将相邻种子点与MLP和最大值池相结合.</p>
<p>This SA layer has exactly the same layer parameters with that in the VoteNet, followed by the same <span class="math inline">\(\text{MLP}_2\)</span>.</p>
<p>此SA层与VoteNet中的层参数完全相同, 后跟相同的<span class="math inline">\(\text{MLP}_2\)</span>.</p>
<h5 id="boxnet-loss-function-details">BoxNet loss function details</h5>
<p>BoxNet has the same loss function as VoteNet, <u>except it is not supervised by vote regression</u>.</p>
<p>BoxNet与VoteNet具有相同的损失函数, 只是它不受投票回归损失的监督.</p>
<p>There is also <u>a slight difference in how objectness labels</u> (used to supervise objectness classiﬁcation) are computed.</p>
<p>物体分数标签(用于监督物体分数分类)的计算方式也略有不同.</p>
<p>As seed points (on object surfaces) are often far from object centroids, <u>it no longer works well to use the distances between seed points and object centroids to compute the objectness labels</u>.</p>
<p>由于种子点(在物体表面)通常远离物体中心, 因此使用种子点和物体中心之间的距离来计算物体分数不再有效.</p>
<p>In BoxNet, we <u>assign positive objectness labels to seed points that are on objects</u> (those belonging to the semantic categories we consider) and <u>negative labels to all the other seed points on clutter</u> (e.g. walls, ﬂoors).</p>
<p>在BoxNet中, 作者将正向物体分数标签指定给物体上的种子点(属于作者考虑的语义类别的种子点), 将负向标签指定给其他噪音种子点(例如墙、地板). <span class="math display">\[
L_{\mathrm{BoxNet}}=\lambda_{1} L_{\mathrm{obj}-\mathrm{cls}}+\lambda_{2} L_{\mathrm{box}}+\lambda_{3} L_{\mathrm{sem}-\mathrm{cls}}
\]</span></p>
<h4 id="more-analysis-experiments部分">More Analysis Experiments部分</h4>
<h5 id="average-precision-and-recall-plots">Average precision and recall plots</h5>
<p><u>Number of proposals per scene v.s. Average Precision (AP) and Average Recall (AR) on SUN RGB-D.</u></p>
<p>SUN RGB-D数据集上每个场景的候选框数量 v.s. 平均精度(AP)和平均召回率(AR).</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131409342.png" srcset="/img/loading.gif" lazyload /></p>
<p>The AP and AR are <u>averaged across the 10 classes</u>.</p>
<p>AP和AR在10个类别中取平均值.</p>
<p>The recall is <u>maximum recall</u> given a fixed number of detection per scene.</p>
<p>召回率是给定每个场景固定数量的检测结果的最大召回率.</p>
<p>We report two ways of using the proposals: <u>joint and per-class</u>.</p>
<p>两种使用候选框的方式: 联合和每类.</p>
<p>The "joint proposal" means that we <u>assign each proposal to a single class</u> (the class with the highest classification score).</p>
<p>"联合候选"是指将每个候选框分配给一个类别(分类分数最高的类别).</p>
<p>The "per-class proposal" means that we <u>assign each proposal to all the 10 classes</u> (the objectness score is multiplied by the semantic classification probability).</p>
<p>"每类候选"指将每个候选框分配给所有10个类别(对象性分数乘以语义分类概率).</p>
<p>For the joint proposal we <u>propose <span class="math inline">\(K\)</span> objects’ bounding boxes for all the 10 categories</u>, where we consider each proposal as the semantic class it has the largest conﬁdence in, <u>and use their objectness scores to rank them</u>.</p>
<p>对于联合候选, 对于10个类别的<span class="math inline">\(K\)</span>个物体边界框, 每个候选框视为具有最大置信度的语义类别并使用物体分数对其进行排序(意思是先分类, 然后根据物体分数进行排序和NMS).</p>
<p>For the per-class proposal we <u>duplicate the <span class="math inline">\(K\)</span> proposal 10 times thus have <span class="math inline">\(K\)</span> proposals per class where we use the multiplication of semantic probability for that class and the objectness probability to rank them</u>.</p>
<p>对于每类建议, 作者将<span class="math inline">\(K\)</span>个物体框在每个类里进行语义类别预测, 每类有<span class="math inline">\(K\)</span>个候选框, 使用物体框在该类的语义概率和物体分数的乘积作为依据对它们进行排序(意思是按照每个框类别概率与物体分数的乘积作为依据进行排序和NMS).</p>
<p>The latter way of using proposals gives us <u>a slight improvement on AP and a big boost on AR</u>.</p>
<p>后一种使用建议的方式对AP有轻微的改进, 对AR有很大的提升.</p>
<h5 id="context-of-voting">Context of voting</h5>
<p>One difference of a deep Hough voting scheme with the traditional Hough voting is that we can <u>take advantage of deep features, which can provide more context knowledge for voting</u>.</p>
<p>深度霍夫投票方案与传统霍夫投票方案的一个不同之处在于可以利用深度学习的特征为投票提供更多的上下文信息.</p>
<p>In following table we show <u>how features from different levels of the PointNet++ affects detection performance</u> (from SA2 to FP3, the network has increasing contexts for voting).</p>
<p>在下表中, 作者展示了PointNet++不同级别的特性如何影响检测性能(从SA2到FP3, 网络中用于投票的上下文信息不断增加).</p>
<p><u>Effects of seed context for 3D detection</u>. Evaluation metric is mAP@0.25 on SUN RGB-D.</p>
<p>种子点上下文对三维检测的影响. 评估指标为mAP@0.25, 基于数据集SUN RGB-D.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131411338.png" srcset="/img/loading.gif" lazyload /></p>
<p>FP3 layer is extended from the FP2 with a MLP of output sizes 256 and 256 with 2048 output points (the same set of XYZ as that output by SA1).</p>
<p>FP3层从FP2扩展而来, MLP的输出大小为256和256, 有2048个输出点(与SA1输出的XYZ相同).</p>
<h5 id="multiple-votes-per-seed">Multiple votes per seed</h5>
<p>In default we just <u>generate one vote per seed</u> since we ﬁnd that with large enough context there is little need to generate more than one vote to resolve ambiguous cases.</p>
<p>默认情况下, 只为每个种子生成一票, 因为在足够大的上下文中, 几乎不需要生成多个投票来解决歧义的情况.</p>
<p>However, it is still possible to <u>generate more than one vote with our network architecture</u>.</p>
<p>然而, VoteNet网络架构仍然可以产生多个投票.</p>
<p>Yet to break the symmetry in multiple vote generation, one has to <u>introduce some bias to different votes to prevent then from pointing to the same place</u>.</p>
<p>然而, 为了打破多重投票的对称性, 必须对不同的投票引入一些偏置, 以防止它们指向同一个地方.</p>
<p>In experiments, we ﬁnd that <u>one vote per seed achieves the best results</u>, as shown in following table.</p>
<p>在实验中, 作者发现每个种子点投一票可以获得最佳结果, 如下表所示.</p>
<p><u>Effects of number of votes per seed</u>. Evaluation metric is mAP@0.25 on SUN RGB-D.</p>
<p>每个种子点的投票数的影响. 评估指标为mAP@0.25, 使用SUN RGB-D数据集.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131413119.png" srcset="/img/loading.gif" lazyload /></p>
<p>We ablate by using a vote factor of 3, where <u>the voting module generates 3 votes per seed with a MLP layer spec: [256, 256, 259 ∗ 3])</u>.</p>
<p>作者设计了投3张票的消融实验, 其中投票模块使用MLP层[256、256、259 * 3]为每个种子点生成3张投票.</p>
<p>In computing the vote regression loss on a seed point, we <u>consider the minimum distance between any predicted votes to the ground truth vote</u> (in case of SUN RGB-D where we may have a set of ground truth votes for a seed, we compute the minimum distance among any pair of predicted vote and ground truth vote).</p>
<p>在计算种子点的投票回归损失时, 作者考虑任何预测投票点与真实投票点之间的最小距离.</p>
<p>To break symmetry, we <u>generate 3 random numbers</u> and inject them to the second last features from the MLP layer.</p>
<p>为了打破对称性, 作者生成3个随机数并将它们注入MLP层的倒数第二个特征.</p>
<p>We show results both with and without this procedure which shows <u>no observable difference</u>.</p>
<p>作者展示了使用和不使用随机数的结果, 没有明显差异.</p>
<h5 id="on-proposal-sampling">On proposal sampling</h5>
<p>In the proposal step, <u>to generate <span class="math inline">\(K\)</span> proposals from the votes, we need to select <span class="math inline">\(K\)</span> vote clusters</u>.</p>
<p>在候选框生成这一步, 要从投票点中生成<span class="math inline">\(K\)</span>个候选框, 需要选择<span class="math inline">\(K\)</span>个投票点簇.</p>
<p>How to select those clusters is a design choice we study here (<u>each cluster is simply a group of votes near a center vote</u>).</p>
<p>对于如何选择这些簇, 作者进行了如下实验(每个簇是靠近中心投票点的一组投票点).</p>
<p><u>Effects of proposal sampling</u>. Evaluation metric is mAP@0.25 on SUN RGB-D.</p>
<p>不同候选框抽样方法的影响. 评估指标为mAP@0.25, 数据集为SUN RGB-D.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131416235.png" srcset="/img/loading.gif" lazyload /></p>
<p><u>256 proposals</u> are used for all evaluations.</p>
<p>所有测试都是基于生成的256个候选框.</p>
<p>Our method is <u>not sensitive to how we choose centers for vote groups/clusters</u>.</p>
<p>作者提出的方法对如何选择投票组/簇中心不敏感.</p>
<p>From 1024 vote clusters, <u>vote FPS samples <span class="math inline">\(K\)</span> clusters based on votes’ XYZ</u>.</p>
<p>在1024个投票点簇中, 基于投票点的最远点采样根据投票点的XYZ获取<span class="math inline">\(K\)</span>簇.</p>
<p>Seed FPS ﬁrstly samples on seed XYZ and then <u>ﬁnds the votes corresponding to the sampled seeds</u>.</p>
<p>基于种子点的最远点采样首先在种子点XYZ上采样, 然后找到与采样种子点对应的投票点.</p>
<p>Random sampling simply <u>selects a random set of <span class="math inline">\(K\)</span> votes and take their neighborhoods for proposal generation</u>.</p>
<p>随机抽样只需选择一组随机的<span class="math inline">\(K\)</span>个投票点并将他们的邻近投票点用于候选框生成.</p>
<p>Note that <u>the results from above table are from the same model</u> trained with vote FPS to select proposals.</p>
<p>上表中的结果来自同一个模型, 该模型使用基于投票点的最远点采样进行训练, 以生成候选框.</p>
<p>We can see that while seed FPS gets the best number in mAP, the difference caused by different sampling strategies is small, <u>showing the robustness of our method</u>.</p>
<p>虽然基于种子点的最远点采样在mAP中最优, 但是不同的采样策略造成的差异很小, 这表明了作者使用的方法的鲁棒性.</p>
<h5 id="effects-of-the-height-feature">Effects of the height feature</h5>
<p>In point clouds from indoor scans, <u>point height is a useful feature in recognition</u>.</p>
<p>在室内扫描的点云中, 点高度是一个有用的识别特征.</p>
<p>As mentioned in the main paper, we can use <span class="math inline">\(1\%\)</span> of the <span class="math inline">\(Z\)</span> values (<span class="math inline">\(Z\)</span>-axis is up-right) of all points from a scan as an approximate as the ﬂoor height <span class="math inline">\(z_\text{ﬂoor}\)</span>, and then <u>compute the a point <span class="math inline">\((x, y, z)\)</span>’s height as <span class="math inline">\(z − z_\text{ﬂoor}\)</span></u>.</p>
<p>作者使用扫描中所有点的<span class="math inline">\(Z\)</span>值(<span class="math inline">\(Z\)</span>-轴向上向右)中的<span class="math inline">\(1\%\)</span>作为地面高度<span class="math inline">\(z_\text{floor}\)</span>的近似值, 然后将点<span class="math inline">\((x, y, z)\)</span>的高度使用<span class="math inline">\(z− z_\text{floor}\)</span>计算.</p>
<p>Effects of the height feature. Evaluation metric is mAP@0.25 on both datasets.</p>
<p>高度特征的影响. 评估指标为mAP@0.25.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131418704.png" srcset="/img/loading.gif" lazyload /></p>
<p>We see that <u>adding the height feature consistently improves performance</u> in both SUN RGB-D and ScanNet.</p>
<p>添加高度特征可以提高SUN RGB-D和ScanNet数据集上VoteNet的性能.</p>
<h4 id="scannet-per-class-evaluation部分">ScanNet Per-class Evaluation部分</h4>
<p>3D object detection scores per category on the ScanNetV2 dataset, <u>evaluated with mAP@0.25 IoU</u>.</p>
<p>ScanNetV2数据集上每个类别的3D对象检测分数, 使用mAP@0.25 IoU评估方式.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131100180.png" srcset="/img/loading.gif" lazyload /></p>
<p>3D object detection scores per category on the ScanNetV2 dataset, <u>evaluated with mAP@0.5 IoU</u>.</p>
<p>ScanNetV2数据集上每个类别的3D对象检测分数, 使用mAP@0.5 IoU评估方式.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131100772.png" srcset="/img/loading.gif" lazyload /></p>
<p>Relying on purely geonetric data, <u>our method excels (esp. with mAP@0.25) in detecting objects where geometry is a strong cue for recognition</u>; and struggles with objects best recognized by texture and color like pictures.</p>
<p>作者提出的方法依靠纯粹的集合信息, 尤其是使用mAP@0.25这一指标进行物体检测时, 几何结构是识别的有力线索; 与使用纹理和颜色最能识别的物体就表现不佳.</p>
<h4 id="visualization-of-votes部分">Visualization of Votes部分</h4>
<p><u>Vote meeting point</u>. Left: ScanNet scene with votes coming from object points. Right: vote offsets from source seed-points to target-votes.</p>
<p>投票点可视化. 左: ScanNet场景中物体点的投票情况. 右: 从种子点到其投票点的位置偏移.</p>
<p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131055607.png" srcset="/img/loading.gif" lazyload /></p>
<p><u>Object votes are colored green</u>, and <u>non-object ones are colored red</u>.</p>
<p>物体投票为绿色, 非物体投票为红色.</p>
<p>See how <u>object points from all-parts of the object vote to form a cluster near the center</u>.</p>
<p>图中可以看到物体点如何在中心附近投票形成簇.</p>
<p>Non-object points, however, <u>either vote "nowhere" and therefore lack structure, or are near object and have gathered enough context to also vote properly</u>.</p>
<p>然而, 非对象点要么"无处投票", 因为缺乏结构信息, 要么靠近对象, 收集了足够的上下文来正确投票.</p>
<p>We clearly see that seed points on objects vote to object centers while <u>clutter points vote either to object enter as well (if the clutter point is close to the object) or to nowhere due to lack of structure in the clutter area</u>.</p>
<p>清楚地看到, 物体上的种子点投票给物体中心，噪音点也投票给物体中心(如果噪音点靠近物体), 或者由于噪音区域缺乏结构信息而不投票给任何地方.</p>
<h3 id="精读总结">精读总结</h3>
<blockquote>
<p>精读目标及效果自测：所读段落是否详细掌握，回答三个的终极问题(你是谁 - 论文提出/采用什么方法、细节是什么；从哪里来 - 论文要解决什么问题/任务、其启发点或借鉴之处在哪；到哪里去 - 论文方法达到什么效果)。</p>
</blockquote>
<p>论文结合基于点云的深度网络和传统的霍夫投票检测方法, 优化构建了一个基于霍夫投票的端到端的直接使用点云数据的三维室内物体检测器.</p>
<p>论文将传统的霍夫投票机制结合深度学习网络进行了端到端的改造, 解决了三维室内点云多覆盖在物体表面, 物体中心部分大概率为空的问题, 利用投票的方式汇聚集中物体特征, 从而实现物体检测.</p>
<p>论文设计的方案在只使用点云的情况下, 在现有的室内检测数据集上达到了SOTA并保持较快的预测速度.</p>
<h2 id="总结">总结</h2>
<blockquote>
<p>总结：总览全文，归纳总结，总结文中创新点，关键点，启发点（论文idea的来源或者说是论文的motivation）等重要信息。</p>
</blockquote>
<p>论文的创新点、关键点:</p>
<ul>
<li>使用深度神经网络和霍夫投票机制构建了一个点云深度霍夫投票检测器;</li>
<li>利用表面种子点投票的方法汇聚物体特征和信息到物体中心点以解决三维点云物体中心为空难以预测候选框的问题;</li>
<li>在目前主要的室内三维检测数据集上达到了SOTA.</li>
</ul>
<p>论文的启发点:</p>
<ul>
<li>传统霍夫投票机制;</li>
<li>直接使用点云数据进行三维物体检测.</li>
</ul>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/01/06/Text2Mesh-Text-Driven-Neural-Stylization-for-Meshes%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">
                        <span class="hidden-mobile">Text2Mesh: Text-Driven Neural Stylization for Meshes阅读笔记</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
