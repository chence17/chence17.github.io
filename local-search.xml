<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Deep Hough Voting for 3D Object Detection in Point Clouds阅读笔记</title>
    <link href="/2022/01/13/Deep-Hough-Voting-for-3D-Object-Detection-in-Point-Clouds%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/01/13/Deep-Hough-Voting-for-3D-Object-Detection-in-Point-Clouds%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="deep-hough-voting-for-3d-object-detection-in-point-clouds">Deep Hough Voting for 3D Object Detection in Point Clouds</h1><blockquote><p>读论文三步曲：泛读，精读，总结。</p></blockquote><h2 id="泛读">泛读</h2><blockquote><p>泛读：快速浏览，把握概要。重点读标题、摘要、结论、所有小标题。</p></blockquote><h3 id="title部分">Title部分</h3><p>Deep <u>Hough Voting</u> for <em>3D Object Detection</em> in <em>Point Clouds</em></p><ul><li>任务: 3D Object Detection</li><li>方法: Hough Voting</li><li>特点: Point Clouds</li></ul><h3 id="abstract部分">Abstract部分</h3><p>In this work, we return to ﬁrst principles to <u>construct a 3D detection pipeline for point cloud data and as generic as possible</u>.</p><p>作者目的是直接利用点云数据进行三维检测而不是将点云数据转换为其他中间类型.</p><p>However, <u>due to the sparse nature of the data</u> – samples from 2D manifolds in 3D space – we face a major challenge when directly predicting bounding box parameters from scene points: <u>a 3D object centroid can be far from any surface point thus hard to regress accurately in one step</u>.</p><p>由于点云数据的稀疏特性, 直接一步准确的回归边界框参数是困难的, 因为三维物体的中心往往距离表面点很远, 而点云扫描到的主要是表面点.</p><p>To address the challenge, we propose <u>VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting</u>.</p><p>因此, 作者设计了VoteNet, 一个端到端的结合深度点云网络和霍夫投票策略的直接使用点云输入的三维物体检测算法.</p><p>Our model <u>achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D</u> with a simple design, compact model size and high efﬁciency.</p><p>作者提出的架构在ScanNet和SUN RGB-D两个数据集上取得了SOTA并且设计简单、模型紧凑和运行高效.</p><p>Remarkably, VoteNet outperforms previous methods by using purely geometric information <u>without relying on color images</u>.</p><p>VoteNet仅对点进行操作, 没有使用任何额外的彩色图片.</p><h3 id="conclusion部分">Conclusion部分</h3><p>The network learns to <u>vote to object centroids directly from point clouds</u> and learns to <u>aggregate votes through their features and local geometry to generate high-quality object proposals</u>.</p><p>作者提出的网络学习直接从点云投票到物体中心点并且通过投票结果的特征和局部几何结构来融合投票结果从而生成高质量的物体候选框.</p><p>In future work we intend to <u>explore how to incorporate RGB images into our detection framework</u> and to <u>utilize our detector in downstream application such as 3D instance segmentation</u>.</p><p>未来, 作者考虑探索如何引入彩色信息到检测架构并且在一些下游应用(例如三维实例分割)中利用检测器.</p><p>We believe that the synergy of Hough voting and deep learning can be generalized to more applications such as <u>6D pose estimation</u>, <u>template based detection</u> etc. and expect to see more future research along this line.</p><p>作者认为霍夫投票和深度学习的方法可以拓展到其他应用(例如六维位姿估计和基于模板的检测)并且希望有更多的基于此类方法的研究.</p><h3 id="小标题分析">小标题分析</h3><ul><li>Introduction <em>[简介]</em></li><li>Related Work <em>[相关工作]</em></li><li>Deep Hough Voting <em>[<u>深度霍夫投票</u>]</em></li><li>VoteNet Architecture <em>[VoteNet网络架构]</em><ul><li>Learning to Vote in Point Clouds <em>[<u>从点云中学习投票</u>]</em></li><li>Object Proposal and Classiﬁcation from Votes <em>[<u>从投票中提取物体和类别</u>]</em></li><li>Implementation Details <em>[实现细节]</em></li></ul></li><li>Experiments <em>[实验]</em><ul><li>Comparing with State-of-the-art Methods <em>[与其他SOTA的比较]</em></li><li>Analysis Experiments <em>[分析实验]</em></li><li>Qualitative Results and Discussion <em>[定性分析的结果和讨论]</em></li></ul></li><li>Conclusion <em>[结论]</em></li><li>Supplement <em>[附加材料]</em><ul><li>Details on Architectures and Loss Functions <em>[<u>架构和损失函数的细节</u>]</em></li><li>More Analysis Experiments <em>[更多的分析实验]</em></li><li>ScanNet Per-class Evaluation <em>[ScanNet的每个类别的测试]</em></li><li>Visualization of Votes <em>[投票可视化]</em></li></ul></li></ul><h3 id="泛读总结">泛读总结</h3><blockquote><p>泛读目标及效果自测：1.论文要解决什么问题? 2.论文采用了什么方法? 3.论文达到什么效果?</p></blockquote><p>论文要解决什么问题? - 直接使用原始点云数据的三维物体检测.</p><p>论文采用了什么方法? - 为了解决点云中物体中心与物体表面点相距甚远的问题, 作者结合深度学习和霍夫投票策略进行物体检测.</p><p>论文达到什么效果? - VoteNet在ScanNet和SUN RGB-D两个数据集上达到了SOTA, VoteNet设计简单、模型紧凑和运行高效.</p><h2 id="精读">精读</h2><blockquote><p>精读：通过泛读找到需要精读重点部分和快速略过的部分，选出精华，仔细阅读。</p></blockquote><h3 id="introduction部分">Introduction部分</h3><p>More speciﬁcally, in this work, we aim to <u>estimate oriented 3D bounding boxes as well as semantic classes of objects from point clouds</u>.</p><p>作者的目标是从点云中估计有向三维边界框及物体的语义类别.</p><p>Compared to images, 3D point clouds provide <u>accurate geometry and robustness to illumination changes</u>.</p><p>相比于图像, 三维点云能够在光照变化的情况下具有准确的几何特征和鲁棒性.</p><p>On the other hand, <u>point clouds are irregular</u>.</p><p>同时点云是不规则的.</p><p>Thus <u>typical CNNs are not well suited to process them directly</u>.</p><p>因此, 经典的CNN无法有效的直接处理点云.</p><p>To avoid processing irregular point clouds, current 3D detection methods <u>heavily rely on 2D-based detectors</u> in various aspects.</p><p>为了避免处理不规则的点云, 大部分三维检测算法严重的依赖于二维检测器.</p><p>For example, some extend 2D detection frameworks such as the Faster/Mask R-CNN to 3D.</p><p>一些算法拓展二维检测器到三维.</p><p>They <u>voxelize the irregular point clouds to regular 3D grids and apply 3D CNN detectors</u>, which <u>fails to leverage sparsity in the data and suffer from high computation cost</u> due to expensive 3D convolutions.</p><p>这些算法先将不规则的点云体素化为规则的三维网格并使用基于三维卷积网络的检测器. 但是, 这些算法往往无法有效利用数据的稀疏性质并且需要很大的计算成本进行运算.</p><p>Alternatively, some projects point to <u>regular 2D bird’s eye view images</u> and then apply 2D detectors to localize objects.</p><p>另一些算法则使用常规的三维场景的二维鸟瞰图视角的图片并将二维检测器使用在这些图片上来检测物体.</p><p>This, however, sacriﬁces geometric details which may be critical in cluttered indoor environments.</p><p>这种做法牺牲了几何细节, 这些几何细节对于室内场景的检测至关重要.</p><p>More recently, others proposed a cascaded two-step pipeline by ﬁrstly detecting objects in front-view images and then localizing objects in frustum point clouds extruded from the 2D boxes, which however is <u>strictly dependent on the 2D detector and will miss an object entirely if it is not detected in 2D</u>.</p><p>最近有一种级联的两步骤方法, 先在前视图图像中进行二维检测检测物体, 之后通过二维图像检测结果定位点云中的三维物体, 这种方法依赖于二维检测器, 如果一个物体没有在二维检测器中检测到, 那么将会整个丢失.</p><p>In this work we <u>introduce a point cloud focused 3D detection framework that directly processes raw data and does not depend on any 2D detectors neither in architecture nor in object proposal</u>.</p><p>作者引入了一个基于点云的三维检测器, 直接处理原始的三维点云数据, 在整个系统和物体候选框的生成过程中, 不使用任何二维检测器.</p><p>Our detection network, VoteNet, is based on recent advances in 3D deep learning models for point clouds, and is inspired by the generalized <u>Hough voting process</u> for object detection.</p><p>作者提出的方法基于最近具有进展的基于点云的三维深度学习方法和基于霍夫投票过程的物体检测方法.</p><p>We <u>leverage PointNet++</u>, a hierarchical deep network for point cloud learning, <u>to mitigates the need to convert point clouds to regular structures</u>.</p><p>作者直接使用Point++结构来提取特征从而不用将点云转换为规则的数据结构.</p><p>By directly processing point clouds not only do we <u>avoid information loss by a quantization process</u>, but we also <u>take advantage of the sparsity in point clouds by only computing on sensed points</u>.</p><p>直接处理点云不仅避免了量化过程中的信息损失, 也利用了点云数据稀疏的特性.</p><p>While PointNet++ has shown success in object classiﬁcation and semantic segmentation, few research study how to detect 3D objects in point clouds with such architectures.</p><p>虽然PointNet++在物体分类和语义分割领域取得了成功, 但是如何将其应用到检测领域的研究却很少.</p><p>A naïve solution would be to follow common practice in 2D detectors and <u>perform dense object proposal to propose 3D bounding boxes directly from the sensed points</u> (with their learned features).</p><p>一个自然的思路是借用二维检测的方式使用有效的点直接进行稠密物体候选框的生成.</p><p>However, <u>the inherent sparsity of point clouds makes this approach unfavorable</u>.</p><p>然而, 点云数据的稀疏特性使这种方式不太有效.</p><p>In images there often exists a pixel near the object center, but it is often <u>not the case in point clouds</u>.</p><p>在图片中, 总存在一个接近物体中心的像素, 但是在点云中却不常见.</p><p>As depth sensors only capture surfaces of objects, 3D object centers are likely to be in empty space, <u>far away from any point</u>.</p><p>由于三维传感器往往只捕捉物体的表面, 三维物体的中心却常常原理表面, 在点云中, 三维物体的中心经常是空的.</p><p>As a result,point based networks <u>have difﬁculty aggregating scene context in the vicinity of object centers</u>.</p><p>因此, 基于点云的算法很难直接聚合场景上下文信息.</p><p><u>Simply increasing the receptive ﬁeld does not solve the problem</u> because as the network captures larger context, it also causes more inclusion of nearby objects and clutter.</p><p>简单的增大感受野不能解决这个问题, 因为增大感受野虽然能够获得更多的上下文信息, 其也会包含很多邻近物体和聚类.</p><p>To this end, we propose to <u>endow point cloud deep networks with a voting mechanism similar to the classical Hough voting</u>.</p><p>因此, 作者设计了一个基于霍夫投票机制的直接使用点云数据的深度网络来实现检测算法.</p><p>By voting we essentially <u>generate new points that lie close to objects centers</u>, which can be grouped and aggregated to generate box proposals.</p><p>通过投票, 作者能够获得靠近物体中心的新点, 这些点能够分组融合以生成候选框.</p><p>In contrast to traditional Hough voting with multiple separate modules that are difﬁcult to optimize jointly, <u>VoteNet is end-to-end optimizable</u>.</p><p>与具有独立模块的传统的霍夫投票算法不同, VoteNet是一个端到端的结构.</p><p>We evaluate our approach on two challenging 3D objectdetection datasets: SUN RGB-D and ScanNet.</p><p>作者在SUN RGB-D和ScanNet数据集上测试了算法.</p><p>On both datasets VoteNet, <u>using geometry only</u>, signiﬁcantly outperforms prior arts that use both RGB and geometry or even multi-view RGB images.</p><p>VoteNet的表现超过其他的算法, VoteNet只使用了几何信息, 其他算法即使用了几何信息也使用了颜色信息.</p><p>Our study shows that the voting scheme supports more effective context aggregation, and veriﬁes that VoteNet <u>offers the largest improvements when object centers are far from the object surface</u>.</p><p>作者的研究表明VoteNet能有效的聚合上下文信息并对中心点远离物体表面的物体检测的提升最大.</p><p>In summary, the contributions of our work are:</p><ul><li>A reformulation of Hough voting in the context of deep learning through an end-to-end differentiable architecture, which we dub VoteNet.</li><li>State-of-the-art 3D object detection performance on SUN RGB-D and ScanNet.</li><li>An in-depth analysis of the importance of voting for 3D object detection in point clouds.</li></ul><p>作者的贡献如下:</p><ul><li>霍夫投票机制在三维点云检测中基于深度学习算法的重构, 提出了VoteNet架构.</li><li>在SUN RGB-D和ScanNet上取得了SOTA.</li><li>详细分析了在基于点云的三维物体检测中使用霍夫投票的重要性.</li></ul><h3 id="related-work部分">Related Work部分</h3><p>Originally introduced in the late 1950s, <u>the Hough transform translates the problem of detecting simple patterns in point samples to detecting peaks in a parametric space</u>.</p><p>霍夫变换最初引入于20世纪50年代末, 它将检测样本中简单模式的问题转化为检测参数空间中的峰值问题.</p><p>The Generalized Hough Transform further <u>extends this technique to image patches as indicators for the existence of a complex object</u>.</p><p>广义霍夫变换进一步将该技术扩展到图像面片, 作为复杂对象的指示器.</p><h3 id="deep-hough-voting部分">Deep Hough Voting部分</h3><p>A traditional Hough voting 2D detector comprises <u>an ofﬂine and an online step</u>.</p><p>传统霍夫二维检测器包括一个在线步骤和一个离线步骤.</p><p>First, given a collection of images with annotated object bounding boxes, <u>a codebook is constructed with stored mappings between image patches (or their features) and their offsets to the corresponding object centers</u>.</p><p>首先, 给定一组带有注释对象边界框的图像, 通过存储图像面片(或其特征)及其到相应对象中心的偏移之间的映射来构造codebook.</p><p>At inference time, <u>interest points are selected from the image to extract patches around them</u>.</p><p>在推断时, 从图像中选择兴趣点以提取其周围的面片.</p><p>These patches are then <u>compared against patches in the codebook to retrieve offsets and compute votes</u>.</p><p>然后将这些面片与codebook中的面片进行比较, 以检索偏移量并计算投票.</p><p>As object patches will tend to <u>vote in agreement</u>, <u>clusters will form near object centers</u>.</p><p>由于面片倾向于一致投票, 因此在物体中心附近将形成簇.</p><p>Finally, the <u>object boundaries are retrieved by tracing cluster votes back to their generating patches</u>.</p><p>最后, 通过将成簇的投票回溯到其生成的面片来检索物体边界.</p><p>We identify two ways in which this technique is <u>well suited to our problem of interest</u>.</p><p>作者确定了这项技术非常适合基于点云的检测问题的两种方式.</p><p>First, <u>voting-based detection is more compatible with sparse sets than region-proposal networks (RPN) is</u>.</p><p>首先, 基于投票的检测比区域提议网络(RPN)更适合稀疏的数据集.</p><p>For the latter, the <u>RPN has to generate a proposal near an object center which is likely to be in an empty space, causing extra computation</u>.</p><p>对于后者, RPN必须在物体中心附近生成候选框, 物体中心可能位于空区域, 从而导致额外的计算.</p><p>Second, <u>it is based on a bottom-up principle where small bits of partial information are accumulated to form a conﬁdent detection</u>.</p><p>其次, 霍夫变换基于自底向上的原则, 即积累局部信息进行检测.</p><p>Even though neural networks can potentially aggregate context from a large receptive ﬁeld, <u>it may be still beneﬁcial to aggregate in the vote space</u>.</p><p>尽管神经网络可以潜在地从一个大的感受野聚合上下文信息, 但在投票空间的聚合仍然是有益的.</p><p>However, as traditional Hough voting comprises multiple separated modules, <u>integrating it into state-of-the-art point cloud networks is an open research topic</u>.</p><p>然而, 由于传统的霍夫投票方法包含多个分离的模块, 因此将其集成到最先进的点云网络中是一个开放的研究课题.</p><p>To this end, we <u>propose the following adaptations to the different pipeline ingredients</u>.</p><p>为此, 作者对不同的组件进行以下调整.</p><p><strong>Interest points</strong> are <u>described and selected by deep neural networks</u> instead of depending on hand-crafted features.</p><p><strong>兴趣点</strong>由深度神经网络描述和选择, 而不是依靠手工制作的特征.</p><p><strong>Vote generation</strong> is <u>learned by a network</u> instead of using a codebook.</p><p><strong>投票生成</strong>由网络学习, 而不是使用codebook.</p><p><u>Leveraging larger receptive ﬁelds</u>, voting can be made less ambiguous and thus more effective.</p><p>利用更大的接受范围, 投票可以减少歧义, 从而更有效.</p><p>In addition, <u>a vote location can be augmented with a feature vector allowing for better aggregation</u>.</p><p>此外, 投票位置可以通过允许更好聚合的特征向量来增加.</p><p><strong>Vote aggregation</strong> is <u>realized through point cloud processing layers with trainable parameters</u>.</p><p><strong>投票聚合</strong>通过具有可训练参数的点云处理层实现.</p><p>Utilizing the vote features, <u>the network can potentially ﬁlter out low quality votes and generate improved proposals</u>.</p><p>利用投票功能, 网络可以潜在地过滤低质量的投票, 并生成改进的提案.</p><p><strong>Object proposals</strong> in the form of: location, dimensions, orientation and even semantic classes can be directly generated from the aggregated features, mitigating the need to trace back votes’ origins.</p><p><strong>物体候选框</strong>的形式: 位置、维度、方向, 甚至语义类别, 可直接从聚合特征生成, 从而减少追溯投票来源的需要.</p><h3 id="votenet-architecture部分">VoteNet Architecture部分</h3><p><u>Illustration of the VoteNet architecture</u> for 3D object detection in point clouds.</p><p>下图是VoteNet的架构图.</p><figure><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201121501675.png" alt="image-20220112150055821" /><figcaption>image-20220112150055821</figcaption></figure><p>Given an input point cloud of <span class="math inline">\(N\)</span> points with XYZ coordinates, a backbone network (implemented with PointNet++ layers) subsamples and learns deep features on the points and <u>outputs a subset of <span class="math inline">\(M\)</span> points but extended by <span class="math inline">\(C\)</span>-dim features</u>.</p><p>输入为具有XYZ坐标的<span class="math inline">\(N\)</span>个点的点云, 主干网络(使用PointNet++层实现)对点的特征进行二次采样和学习, 输出<span class="math inline">\(M\)</span>个点组成的输入点云的子集(坐标和扩展了的<span class="math inline">\(C\)</span>维特征).</p><p>This subset of points are considered as <u>seed points</u>.</p><p>这<span class="math inline">\(M\)</span>个点被视为种子点.</p><p><u>Each seed independently generates a vote</u> through a voting module.</p><p>种子点通过投票模块独立生成投票点.</p><p>Then <u>the votes are grouped into clusters</u> and <u>processed by the proposal module to generate the ﬁnal proposals</u>.</p><p>然后将投票点分组, 由候选框生成模块进行处理, 生成候选框.</p><p>The <u>classiﬁed and NMSed proposals become the ﬁnal 3D bounding boxes output</u>.</p><p>分类和经过NMS后的候选框将成为最终的三维物体边界框输出.</p><p>The entire network can be split into two parts: <u>one that processes existing points to generate votes</u>; and <u>the other part that operates on virtual points – the votes – to propose and classify objects</u>.</p><p>整个网络可分为两部分: 一部分处理现有点以生成投票; 而另一个作用于虚拟点的部分(投票点)则是对物体进行候选框的生成和分类.</p><h4 id="learning-to-vote-in-point-clouds部分">Learning to Vote in Point Clouds部分</h4><p>From an input point cloud of size <span class="math inline">\(N\times 3\)</span>, with a 3D coordinate for each of the <span class="math inline">\(N\)</span> points, we aim to <u>generate <span class="math inline">\(M\)</span> votes, where each vote has both a 3D coordinate and a high dimensional feature vector</u>.</p><p>输入为<span class="math inline">\(N\times3\)</span>的点云, 每个点都有一个三维坐标, 生成<span class="math inline">\(M\)</span>个投票点, 其中每个投票点都有一个三维坐标和一个高维特征向量.</p><p>There are two major steps: <u>point cloud feature learning through a backbone network and learned Hough voting from seed points</u>.</p><p>有两个主要步骤: 通过骨干网络学习点云特征和从种子点学习霍夫投票.</p><h5 id="point-cloud-feature-learning">Point cloud feature learning</h5><p>Generating an accurate vote requires <u>geometric reasoning and contexts</u>.</p><p>生成准确的投票需要几何推理和上下文信息.</p><p>Instead of relying on hand-crafted features, we <u>leverage recently proposed deep networks on point clouds for point feature learning</u>.</p><p>作者不依赖手工制作的特征, 而是利用点云深度网络进行点特征学习.</p><p>While our method is not restricted to any point cloud network, we <u>adopt PointNet++ as our backbone</u> due to its simplicity and demonstrated success on tasks ranging from normal estimation, semantic segmentation to 3D object localization.</p><p>虽然作者的方法不局限于任何点云网络, 但由于其简单性, 作者采用PointNet++作为主干.</p><p>The backbone network has several set-abstraction layers and feature propagation (upsampling) layers with skip connections, which outputs <u>a subset of the input points with XYZ and an enriched <span class="math inline">\(C\)</span>-dimensional feature vector</u>.</p><p>主干网络具有多个集合抽象层和跳连的特征传播(上采样)层, 其输出具有XYZ坐标和<span class="math inline">\(C\)</span>维特征向量的输入点云的子集.</p><p>The results are <u><span class="math inline">\(M\)</span> seed points of dimension <span class="math inline">\((3+C)\)</span></u>.</p><p>输出维度为<span class="math inline">\((3+C)\)</span>的<span class="math inline">\(M\)</span>个种子点.</p><p>Each seed point <u>generates one vote</u>.</p><p>每个种子点产生一票.</p><h5 id="hough-voting-with-deep-networks">Hough voting with deep networks</h5><p>Compared to traditional Hough voting where the votes (offsets from local key-points) are determined by look ups in a pre-computed codebook, we <u>generate votes with a deep network based voting module, which is both more efﬁcient (without kNN look ups) and more accurate as it is trained jointly with the rest of the pipeline</u>.</p><p>在传统的霍夫投票中, 投票(局部关键点的偏移量)是通过在预先计算的codebook中搜索来确定的, 作者使用基于深度神经网络的投票模块生成投票, 该模块更有效(无kNN查找)且更准确, 因为它与其余部分联合训练.</p><p>Given a set of seed points <span class="math inline">\(\left\{s_{i}\right\}_{i=1}^{M}\)</span> where <span class="math inline">\(s_{i}=\left[x_{i} ; f_{i}\right]\)</span> with <span class="math inline">\(x_{i} \in \mathbb{R}^{3}\)</span> and <span class="math inline">\(f_{i} \in \mathbb{R}^{C}\)</span>, <u>a shared <em>voting module</em> generates votes from each seed independently</u>.</p><p>对于种子点集<span class="math inline">\(\left\{s_{i}\right\}_{i=1}^{M}\)</span>, 其中<span class="math inline">\(s_{i}=\left[x_{i} ; f_{i}\right]\)</span>, <span class="math inline">\(x_{i} \in \mathbb{R}^{3}\)</span>, <span class="math inline">\(f_{i} \in \mathbb{R}^{C}\)</span>, 权重共享的投票模块从每个种子点生成投票点.</p><p>Speciﬁcally, <u>the voting module is realized with a multi-layer perceptron (MLP) network</u> with fully connected layers, ReLU and batch normalization.</p><p>具体而言, 投票模块是通过多层感知器(MLP)网络实现的, 此模块具有全连接层、ReLU和批量标准化.</p><p>The MLP <u>takes seed feature <span class="math inline">\(f_i\)</span> and outputs the Euclidean space offset <span class="math inline">\(\Delta x_{i} \in \mathbb{R}^{3}\)</span> and a feature offset <span class="math inline">\(\Delta f_{i} \in \mathbb{R}^{C}\)</span></u> such that the vote <span class="math inline">\(v_{i}=\left[y_{i} ; g_{i}\right]\)</span> generated from the seed <span class="math inline">\(s_i\)</span> has <span class="math inline">\(y_{i}=x_{i}+\Delta x_{i}\)</span> and <span class="math inline">\(g_{i}=f_{i}+\Delta f_{i}\)</span>.</p><p>多层感知器接受种子点特征<span class="math inline">\(f_i\)</span>作为输入并输出位置偏移<span class="math inline">\(\Delta x_{i} \in \mathbb{R}^{3}\)</span>和特征残差<span class="math inline">\(\Delta f_{i} \in \mathbb{R}^{C}\)</span>, 于是可以从种子点<span class="math inline">\(s_i\)</span>生成投票点<span class="math inline">\(v_{i}=\left[y_{i} ; g_{i}\right]\)</span>, 其中<span class="math inline">\(y_{i}=x_{i}+\Delta x_{i}\)</span>, <span class="math inline">\(g_{i}=f_{i}+\Delta f_{i}\)</span>.</p><p>The predicted 3D offset <u><span class="math inline">\(\Delta x_i\)</span> is explicitly supervised by a regression loss</u>:</p><p>预测的三维偏移量<span class="math inline">\(\Delta x_i\)</span>是<u>由回归损失函数监督</u>的: <span class="math display">\[L_{\text {vote-reg }}=\frac{1}{M_{\text {pos }}} \sum_{i}\left\|\Delta x_{i}-\Delta x_{i}^{*}\right\| \mathbb{1}\left[s_{i} \text { on object}\right]\]</span> where <u><span class="math inline">\(\mathbb{1}[s_{i} \text { on object}]\)</span> indicates whether a seed point <span class="math inline">\(s_i\)</span> is on an object surface and <span class="math inline">\(M_\text{pos}\)</span> is the count of total number of seeds on object surface</u>.</p><p>其中<span class="math inline">\(\mathbb{1}[s_{i} \text { on object}]\)</span>表示种子点<span class="math inline">\(s_i\)</span>是否在物体的表面, <span class="math inline">\(M_\text{pos}\)</span>则是在物体表面种子点的总数.</p><p><span class="math inline">\(\Delta x_{i}^{*}\)</span> is the <u>ground truth displacement from the seed position <span class="math inline">\(x_i\)</span> to the bounding box center of the object it belongs to</u>.</p><p><span class="math inline">\(\Delta x_{i}^{*}\)</span>代表真实偏移量, 是对应的三维物体中心点与种子点位置<span class="math inline">\(x_i\)</span>之间的距离.</p><p>Votes are <u>the same as seeds in tensor representation but are no longer grounded on object surfaces</u>.</p><p>投票点与种子点使用相同的向量表示, 但投票点不再分布在物体表面.</p><p>A more essential difference though is their position – <u>votes generated from seeds on the same object are now closer to each other than the seeds are, which makes it easier to combine cues from different parts of the object</u>.</p><p>但更本质的区别在于它们的位置, 从同一对象上的种子点生成的投票点比种子点相互之间更接近, 这使得组合来自对象不同部分的特征更容易.</p><h4 id="object-proposal-and-classiﬁcation-from-votes部分">Object Proposal and Classiﬁcation from Votes部分</h4><h5 id="vote-clustering-through-sampling-and-grouping">Vote clustering through sampling and grouping</h5><p>While there can be many ways to cluster the votes, we opt for a simple strategy of <u>uniform sampling and grouping according to spatial proximity</u>.</p><p>虽然有许多方法可以对投票进行聚类, 但作者选择了一种简单的策略, 即根据空间接近程度进行平均采样和分组.</p><p>Speciﬁcally, from a set of votes <span class="math inline">\(\left\{v_{i}=\left[y_{i} ; g_{i}\right] \in \mathbb{R}^{3+C}\right\}_{i=1}^{M}\)</span> <u>using farthest point sampling based on <span class="math inline">\({y_i}\)</span> in 3D Euclidean space</u>, to get <span class="math inline">\({v_{i_k}}\)</span> with <span class="math inline">\(k =1,\cdots,K\)</span>.</p><p>对一个投票点集合<span class="math inline">\(\left\{v_{i}=\left[y_{i} ; g_{i}\right] \in \mathbb{R}^{3+C}\right\}_{i=1}^{M}\)</span>, 基于其坐标参数<span class="math inline">\({y_i}\)</span>进行最远点采样(FPS), 以此获取<span class="math inline">\({v_{i_k}}\)</span>, 其中<span class="math inline">\(k =1,\cdots,K\)</span>.</p><p>Then we <u>form <span class="math inline">\(K\)</span> clusters by ﬁnding neighboring votes to each of the <span class="math inline">\(v_{i_k}\)</span>’s 3D location</u>: <span class="math inline">\(\mathcal{C}_{k}=\left\{v_{i}^{(k)} \mid\left\|v_{i}-v_{i_{k}}\right\| \leq r\right\}\)</span> for <span class="math inline">\(k =1,\cdots,K\)</span>.</p><p>之后, 根据采样到的<span class="math inline">\(K\)</span>个点, 作者根据空间距离进行聚类产生<span class="math inline">\(K\)</span>类, 得到<span class="math inline">\(\mathcal{C}_{k}=\left\{v_{i}^{(k)} \mid\left\|v_{i}-v_{i_{k}}\right\| \leq r\right\}\)</span>, 其中<span class="math inline">\(k =1,\cdots,K\)</span>.</p><h5 id="proposal-and-classiﬁcation-from-vote-clusters">Proposal and classiﬁcation from vote clusters</h5><p>As a vote cluster is in essence a set of high-dim points, we can <u>leverage a generic point set learning network to aggregate the votes in order to generate object proposals</u>.</p><p>由于投票簇本质上是一组高维点, 作者利用点云神经网络来聚合投票点, 以生成物体候选框.</p><p>Compared to the back-tracing step of traditional Hough voting for identifying the object boundary, this procedure allows to <u>propose amodal boundaries even from partial observations, as well as predicting other parameters like orientation, class, etc</u>.</p><p>与传统的霍夫投票识别对象边界的回溯步骤相比, 此过程允许根据部分观测值预测边界, 同时预测其他参数, 如方向、类别等.</p><p>In our implementation, we <u>use a shared PointNet for vote aggregation and proposal in clusters</u>.</p><p>作者使用一个共享的PointNet来进行簇中的投票点聚合和候选框的生成.</p><p>Given a vote cluster <span class="math inline">\(\mathcal{C}=\left\{w_{i}\right\}\)</span> with <span class="math inline">\(i=1, \ldots, n\)</span> and its cluster center <span class="math inline">\(w_{j}\)</span>, where <span class="math inline">\(w_{i}=\left[z_{i} ; h_{i}\right]\)</span> with <u><span class="math inline">\(z_{i} \in \mathbb{R}^{3}\)</span> as the vote location and <span class="math inline">\(h_{i} \in \mathbb{R}^{C}\)</span> as the vote feature</u>.</p><p>投票点簇<span class="math inline">\(\mathcal{C}=\left\{w_{i}\right\}\)</span>, 其中<span class="math inline">\(i=1, \ldots, n\)</span>, 以及簇的中心<span class="math inline">\(w_{j}\)</span>, 且<span class="math inline">\(w_{i}=\left[z_{i} ; h_{i}\right]\)</span>, 其中<span class="math inline">\(z_{i} \in \mathbb{R}^{3}\)</span>是投票点的位置, <span class="math inline">\(h_{i} \in \mathbb{R}^{C}\)</span>是投票点特征.</p><p>To enable usage of local vote geometry, we <u>transform vote locations to a local normalized coordinate system by <span class="math inline">\(z_{i}^{\prime}=\left(z_{i}-z_{i}\right) / r\)</span></u>.</p><p>为了利用局部投票点的集合特征, 作者将投票点进行坐标归一化到局部坐标系, 归一化方式为<span class="math inline">\(z_{i}^{\prime}=\left(z_{i}-z_{i}\right) / r\)</span>.</p><p>Then an object proposal for this cluster <span class="math inline">\(p(C)\)</span> is generated by <u>passing the set input through a PointNet-like module</u>:</p><p>然后, 通过将投票点集合传递给类似PointNet的模块, 生成集群<span class="math inline">\(p(C)\)</span>的候选框: <span class="math display">\[p(\mathcal{C})=\operatorname{MLP}_{2}\left\{\max _{i=1, \ldots, n}\left\{\operatorname{MLP}_{1}\left(\left[z_{i}^{\prime} ; h_{i}\right]\right)\right\}\right\}\]</span> where votes from each cluster are <u>independently processed by a <span class="math inline">\(\text{MLP}_1\)</span> before being max-pooled (channel-wise) to a single feature vector and passed to <span class="math inline">\(\text{MLP}_2\)</span> where information from different votes are further combined</u>.</p><p>其中, 来自每个簇的投票点由<span class="math inline">\(\text{MLP}_1\)</span>独立处理, 之后通过最大池化(通道)到单个特征向量并传递到<span class="math inline">\(\text{MLP}_2\)</span>中, 其中来自不同投票点的信息进一步合并.</p><p>We represent the proposal <span class="math inline">\(p\)</span> as a multidimensional vector with <u>an objectness score, bounding box parameters (center, heading and scale) and semantic classiﬁcation scores</u>.</p><p>作者将候选框<span class="math inline">\(p\)</span>表示为一个多维向量, 带有物体性分数、边界框参数(中心、朝向角和尺寸)和语义分类分数.</p><h5 id="loss-function">Loss function</h5><p>The loss functions in the proposal and classiﬁcation stage consist of <u>objectness, bounding box estimation, and semantic classiﬁcation losses</u>.</p><p>候选框生成和分类阶段的损失函数包括对象性损失函数、边界框估计损失函数和语义分类损失函数.</p><p>We supervise the objectness scores for votes that are located <u>either close to a ground truth object center (within 0.3 meters) or far from any center (by more than 0.6 meters)</u>.</p><p>作者监督投票点的物体性分数, 这些投票点要么靠近物体边界框中心(0.3米以内), 要么远离任何中心(超过0.6米).</p><p>We consider proposals generated from those votes as <u>positive and negative proposals</u>, respectively.</p><p>作者认为从这些投票点中产生的候选框分别是积极的和消极的.</p><p>Objectness predictions for other proposals are <u>not penalized</u>.</p><p>其他候选框的无效预测不受惩罚.</p><p>Objectness is <u>supervised via a cross entropy loss normalized by the number of non-ignored proposals in the batch</u>.</p><p>物体性通过一个交叉熵损失函数进行监督, 该损失使用批中未被忽略的候选框的个数进行标准化.</p><p>For positive proposals we further <u>supervise the bounding box estimation and class prediction according to the closest ground truth bounding box</u>.</p><p>对于正向的候选框, 作者用与其最接近的真实值, 监督边界框估计和类别预测.</p><p>Speciﬁcally, we follow which <u>decouples the box loss to center regression, heading angle estimation and box size estimation</u>.</p><p>具体而言, 作者是将边界框损失解耦为中心点损失、朝向角损失和边界框尺寸损失.</p><p>For semantic classiﬁcation we <u>use the standard cross entropy loss</u>.</p><p>对于语义分类, 作者使用交叉熵损失函数.</p><p>In all regression in the detection loss we <u>use the Huber (smooth-<span class="math inline">\(L_1\)</span>) loss</u>.</p><p>在所有检测损失函数中, 作者使用Huber(<span class="math inline">\(L_1\)</span>平滑)损失.</p><h4 id="implementation-details部分">Implementation Details部分</h4><h5 id="input-and-data-augmentation">Input and data augmentation</h5><p>Input to our detection network is <u>a point cloud of <span class="math inline">\(N\)</span> points randomly sub-sampled from either a popped-up depth image (<span class="math inline">\(N = 20k\)</span>) or a 3D scan (mesh vertices, <span class="math inline">\(N = 40k\)</span>)</u>.</p><p>VoteNet的输入是从深度图像(<span class="math inline">\(N=20k\)</span>)或三维扫描(网格顶点, <span class="math inline">\(N=40k\)</span>)中随机采样<span class="math inline">\(N\)</span>点的点云.</p><p>In addition to XYZ coordinates, we also include <u>a height feature</u> for each point indicating its distance to the ﬂoor.</p><p>除XYZ坐标外, 输入还包括每个点的高度特征, 以指示其到地面的距离.</p><p>The ﬂoor height is <u>estimated as the <span class="math inline">\(1\%\)</span> percentile of all points’ heights</u>.</p><p>地面高度估计为所有点的高度的<span class="math inline">\(1\%\)</span>.</p><p>To augment the training data, we <u>randomly sub-sample the points from the scene points</u> on-the-ﬂy.</p><p>为了增加训练数据, 作者从场景点云上随机进行采样.</p><p>We also randomly ﬂip the point cloud in both horizontal direction, <u>randomly rotate the scene points by Uniform<span class="math inline">\([-5\degree, 5\degree]\)</span> around the upright-axis, and randomly scale the points by Uniform<span class="math inline">\([0.9, 1.1]\)</span></u>.</p><p>作者在旋转和尺度上随机调整点云, 围绕垂直轴以均匀分布<span class="math inline">\([-5\degree，5\degree]\)</span>随机旋转点云, 以均匀分布<span class="math inline">\([0.9，1.1]\)</span>随机缩放点云.</p><h5 id="network-architecture-details">Network architecture details</h5><p>The <u>backbone feature learning network is based on PointNet++</u>, which has four set abstraction (SA) layers and two feature propagation/upsamplng (FP) layers, where the SA layers have increasing receptive radius of <span class="math inline">\(0.2\)</span>, <span class="math inline">\(0.4\)</span>, <span class="math inline">\(0.8\)</span> and <span class="math inline">\(1.2\)</span> in meters while they sub-sample the input to <span class="math inline">\(2048\)</span>, <span class="math inline">\(1024\)</span>, <span class="math inline">\(512\)</span> and <span class="math inline">\(256\)</span> points respectively.</p><p>主干特征学习网络基于PointNet++, 它有四个集合抽象(SA)层和两个特征传播/上采样(FP)层，其中集合抽象层的感知半径为<span class="math inline">\(0.2\)</span>, <span class="math inline">\(0.4\)</span>, <span class="math inline">\(0.8\)</span>和<span class="math inline">\(1.2\)</span>, 以米为单位, 同时它们将输入分采样为<span class="math inline">\(2048\)</span>, <span class="math inline">\(1024\)</span>, <span class="math inline">\(512\)</span>和<span class="math inline">\(256\)</span>.</p><p>The two FP layers up-sample the 4th SA layer’s output back to <span class="math inline">\(1024\)</span> points <u>with 256-dim features and 3D coordinates</u>.</p><p>两个上采样层向上采样第四个集合抽象层的输出, 输出<span class="math inline">\(1024\)</span>个点, 具有<span class="math inline">\(256\)</span>维的特征和三维坐标.</p><p>The voting layer is realized through a multi-layer perceptron with FC output sizes of <span class="math inline">\(256\)</span>, <span class="math inline">\(256\)</span>, <span class="math inline">\(259\)</span>, where <u>the last FC layer outputs XYZ offset and feature residuals</u>.</p><p>投票层通过多层感知器实现, 全连接层输出大小为<span class="math inline">\(256\)</span>, <span class="math inline">\(256\)</span>, <span class="math inline">\(259\)</span>, 其中最后一个全连接层输出XYZ偏移量和特征残差.</p><p>The proposal module is implemented as <u>a set abstraction layer with a post processing <span class="math inline">\(\text{MLP}_2\)</span> to generate proposals after the max-pooling</u>.</p><p>候选框生成模块是一个带有后处理的<span class="math inline">\(\text{MLP}_2\)</span>的集合抽象层, 用于在最大池之后生成候选框.</p><p>The SA uses radius <span class="math inline">\(0.3\)</span> and <span class="math inline">\(\text{MLP}_1\)</span> with output sizes of <span class="math inline">\(128\)</span>, <span class="math inline">\(128\)</span>, <span class="math inline">\(128\)</span>.</p><p>SA使用的半径为<span class="math inline">\(0.3\)</span>, <span class="math inline">\(\text{MLP}_1\)</span>输出大小分别为<span class="math inline">\(128\)</span>, <span class="math inline">\(128\)</span>, <span class="math inline">\(128\)</span>.</p><p>The max-pooled features are further processed by <span class="math inline">\(\text{MLP}_2\)</span> with output sizes of <span class="math inline">\(128\)</span>, <span class="math inline">\(128\)</span>, <span class="math inline">\(5+2NH+4NS+NC\)</span> where the output consists of <u><span class="math inline">\(2\)</span> objectness scores, <span class="math inline">\(3\)</span> center regression values, <span class="math inline">\(2NH\)</span> numbers for heading regression (<span class="math inline">\(NH\)</span> heading bins) and <span class="math inline">\(4NS\)</span> numbers for box size regression (<span class="math inline">\(NS\)</span> box anchors) and <span class="math inline">\(NC\)</span> numbers for semantic classiﬁcation</u>.</p><p>最大池特征由<span class="math inline">\(\text{MLP}_2\)</span>进一步处理, 输出大小为<span class="math inline">\(128\)</span>, <span class="math inline">\(128\)</span>, <span class="math inline">\(5+2NH+4NS+NC\)</span>, 其中输出包括<span class="math inline">\(2\)</span>个对象性分数, <span class="math inline">\(3\)</span>个中心回归值, 朝向回归为<span class="math inline">\(2NH\)</span>个数值(<span class="math inline">\(NH\)</span>个朝向锚点), 检测框大小回归为<span class="math inline">\(4NS\)</span>个数值(<span class="math inline">\(NS\)</span>个锚框), 语义分类结果为<span class="math inline">\(NC\)</span>个数值.</p><h5 id="training-the-network">Training the network</h5><p>We train the entire network end-to-end and from scratch with an <u>Adam optimizer</u>, <u>batch size <span class="math inline">\(8\)</span> and an initial learning rate of <span class="math inline">\(0.001\)</span></u>.</p><p>作者使用Adam优化器对整个网络进行从头开始的端到端培训, 批量大小为<span class="math inline">\(8\)</span>, 初始学习率为<span class="math inline">\(0.001\)</span>.</p><p>The learning rate is <u>decreased by <span class="math inline">\(10\times\)</span> after <span class="math inline">\(80\)</span> epochs and then decreased by another <span class="math inline">\(10\times\)</span> after <span class="math inline">\(120\)</span> epochs</u>.</p><p>学习率在<span class="math inline">\(80\)</span>代后除<span class="math inline">\(10\)</span>, 在<span class="math inline">\(120\)</span>代后再除<span class="math inline">\(10\)</span>.</p><p>Training the model to convergence on one Volta Quadro GP100 GPU takes <u>around <span class="math inline">\(10\)</span> hours on SUN RGB-D and less than <span class="math inline">\(4\)</span> hours on ScanNetV2</u>.</p><p>在单张Volta Quadro GP100 GPU上训练模型, SUN RGB-D大约需要<span class="math inline">\(10\)</span>小时, ScanNetV2不到<span class="math inline">\(4\)</span>小时.</p><h5 id="inference">Inference</h5><p>Our VoteNet is able to <u>take point clouds of the entire scenes and generate proposals in one forward pass</u>.</p><p>VoteNet能够获取整个场景的点云并在一次前向传播中生成物体候选框.</p><p>The proposals are post-processed by a 3D NMS module <u>with an IoU threshold of <span class="math inline">\(0.25\)</span></u>.</p><p>三维候选框经过三维NMS模块进行后处理, IoU阈值为<span class="math inline">\(0.25\)</span>.</p><h3 id="experiments部分">Experiments部分</h3><h4 id="comparing-with-state-of-the-art-methods部分">Comparing with State-of-the-art Methods部分</h4><p>SUN RGB-D is a <u>single-view RGB-D dataset for 3D scene understanding</u>.</p><p>SUN RGB-D是用于三维场景理解的单视图RGB-D数据集.</p><p>It consists of ~5K RGB-D training images annotated with amodal oriented 3D bounding boxes for <u>37 object categories</u>.</p><p>它由约5K个RGB-D训练图像组成, 这些图像有向三维边界框标注37个对象类别.</p><p>To feed the data to our network, we ﬁrstly <u>convert the depth images to point clouds using the provided camera parameters</u>.</p><p>为了将数据提供给网络, 作者首先使用提供的相机参数将深度图像转换为点云.</p><p>We follow a standard evaluation protocol and <u>report performance on the 10 most common categories</u>.</p><p>作者报告10个最常见类别的性能.</p><p>ScanNetV2 is a richly annotated dataset of <u>3D reconstructed meshes of indoor scenes</u>.</p><p>ScanNetV2是一个具有丰富注释的室内场景三维重建网格数据集.</p><p>It contains 1.2K training examples collected from hundreds of different rooms, and <u>annotated with semantic and instance segmentation for 18 object categories</u>.</p><p>它包含从数百个不同房间收集的1.2K训练示例, 并对18个对象类别进行语义和实例分割注释.</p><p>Compared to partial scans in SUN RGB-D, <u>scenes in ScanNetV2 are more complete and cover larger areas with more objects on average</u>.</p><p>与SUN RGB-D中的部分扫描相比, ScanNetV2中的场景更完整, 平均覆盖面积更大, 对象更多.</p><p>We <u>sample vertices from the reconstructed meshes as our input point clouds</u>.</p><p>作者从重建的网格中采样顶点作为输入点云.</p><p>Since ScanNetV2 does not provide amodal or oriented bounding box annotation, we aim to <u>predict axis-aligned bounding boxes instead</u>.</p><p>由于ScanNetV2不提供有向边界框注释, 因此作者的目标是预测轴对齐的边界框.</p><p><u>3D object detection results on SUN RGB-D val set</u>. Evaluation metric is average precision with 3D IoU threshold 0.25.</p><p>在SUN RGB-D验证集上的三维对象检测结果. 评估指标是3D IoU阈值为0.25的mAP.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131028819.png" /></p><p><u>3D object detection results on ScanNetV2 val set</u>.</p><p>ScanNetV2验证集上的三维对象检测结果.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131028926.png" /></p><p>VoteNet <u>outperforms all previous methods</u> by at least 3.7 and 18.4 mAP increase in SUN RGB-D and ScanNet respectively.</p><p>VoteNet在SUN RGB-D和ScanNet中的mAP分别至少增加了3.7和18.4, 优于所有以前的方法.</p><p>Notably, we achieve such improvements when we <u>use geometric input (point clouds) only</u> while they used both geometry and RGB images.</p><p>值得注意的是, 作者只使用几何输入(点云).</p><p>Importantly, <u>the same set of network hyperparameters was used in both datasets</u>.</p><p>重要的是, 两个数据集中使用了相同的网络超参数进行训练.</p><h4 id="analysis-experiments部分">Analysis Experiments部分</h4><h5 id="to-vote-or-not-to-vote">To Vote or Not To Vote?</h5><p>A straightforward baseline to VoteNet is a network that <u>directly proposes boxes from sampled scene points</u>.</p><p>VoteNet的一个简单的基线是一个直接从采样的场景点提出边界框的网络.</p><p>Such a baseline – which we refer to as <em>BoxNet</em> – is <u>essential to distill the improvement due to voting</u>.</p><p>这样一个基线, 作者称之为<em>BoxNet</em>, 这对于对比投票带来的改进至关重要.</p><p>The BoxNet has the same backbone as the VoteNet but instead of voting, <u>it directly generates boxes from the seed points</u>.</p><p>BoxNet与VoteNet具有相同的主干, 但它没有投票模块, 而是直接从种子点生成边界框.</p><p>The following table shows <u>voting boosts the performance by a signiﬁcant margin of ~5 mAP on SUN RGB-D and &gt;13 mAP on ScanNet</u>.</p><p>在SUN RGB-D上, 投票显著提高了约5点的mAP的性能, 在ScanNet上, 投票显著提高了约13点的mAP的性能.</p><p><u>Comparing VoteNet with a no-vote baseline</u>. Metric is 3D object detection mAP.</p><p>将VoteNet与BoxNet进行比较. 评价指标是三维目标检测mAP.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131029273.png" /></p><p>In what ways, then, <u>does voting help</u>?</p><p>那么，投票在哪些方面有帮助呢?</p><p>We argue that since in sparse 3D point clouds, existing scene points are often far from object centroids, <u>a direct proposal may have lower conﬁdence and inaccurate amodal boxes</u>.</p><p>作者认为, 由于在稀疏的3D点云中, 现有的场景点通常远离对象质心, 因此直接建议可能具有较低的可信度和不准确三维边界框.</p><p>Voting, instead, brings closer together these lower conﬁdence points and <u>allows to reinforce their hypothesis though aggregation</u>.</p><p>相反, 投票使这些较低的置信点更紧密地结合在一起并允许通过聚集来强化其假设.</p><p>Voting helps <u>increase detection contexts</u>.</p><p>投票有助于增加检测上下文.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131031595.png" /></p><p>Seed points that generate good boxes (BoxNet), or good votes (VoteNet) which in turn generate good boxes, are overlaid (in blue) <u>on top of a representative ScanNet scene</u>.</p><p>生成好框的种子点(BoxNet)或生成好框的好投票(VoteNet)在ScanNet场景上的覆盖情况(蓝色).</p><p>As the voting step effectively increases context, VoteNet <u>demonstrates a much denser cover of the scene</u>, therefore increasing the likelihood of accurate detection.</p><p>由于投票步骤有效地增加了上下文信息, VoteNet展示了更密集的场景覆盖, 因此增加了准确检测的可能性.</p><p>We proceed with a second analysis in following figure showing on the same plot (in separate scales), for each SUN RGB-D category: <u>(in blue dots) gains in mAP between VoteNet and BoxNet, and (in red squares) closest distances between object points (on their surfaces) and their amodal box centers, averaged per category and normalized by the mean class size (a large distance means the object center is usually far from its surface)</u>.</p><p>作者在下图中继续进行第二次分析, 在同一个图上(以单独的比例)显示每个SUN RGB-D类别:(蓝色点)VoteNet和BoxNet之间的mAP增益, 以及(红色正方形)物体点(在其表面上)与其有向边界框中心之间的最近距离, 按类别平均并按平均类大小标准化(较大的距离意味着对象中心通常远离其表面).</p><p><u>Sorting the categories according to the former</u>, we see a strong correlation.</p><p>根据前者对类别进行排序, 作者发现有很强的相关性.</p><p>Namely, <u>when object points tend to be further from the amodal box center, voting helps much more</u>.</p><p>也就是说, 当对象点趋向于离边界框中心更远时, 投票的帮助更大.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131033019.png" /></p><h5 id="effect-of-vote-aggregation">Effect of Vote Aggregation</h5><p>In following figure (right), we show that <u>vote aggregation with a learned Pointnet and max pooling achieves far better results than manually aggregating the vote features in the local regions due to the existence of clutter votes</u> (i.e. votes from non-object seeds).</p><p>在下图(右图)中, 由于存在噪音票(即来自非物体种子点的投票), 使用PointNet和最大池的投票聚合比手动聚合局部区域中的投票特征获得更好的结果.</p><p>In following figure (left), we show <u>how vote aggregation radius affects detection</u> (tested with PointNet using max pooling).</p><p>在下图(左图)中, 展示了投票聚合半径如何影响检测结果(使用包含最大池的PointNet进行测试).</p><p><u>Vote aggregation analysis</u>.</p><p>投票聚合分析.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131036132.png" /></p><p>Left: <u>mAP@0.25 on SUN RGB-D for varying aggregation radii when aggregating via Pointnet (max)</u>.</p><p>左: mAP@0.25, 在SUN RGB-D上, 在通过PointNet(最大值池化)聚合时改变聚合半径.</p><p>Right: <u>Comparisons of different aggregation methods (radius = 0.3 for all methods)</u>.</p><p>右: 不同聚合方法的比较(所有方法的聚合半径为0.3m).</p><p><u>Using a learned vote aggregation is far more effective than manually pooling the features</u> in a local neighborhood.</p><p>使用学习的投票聚合比手动聚合特征要有效得多.</p><h5 id="model-size-and-speed">Model Size and Speed</h5><p>Our proposed model is very efﬁcient since <u>it leverages sparsity in point clouds and avoids search in empty space</u>.</p><p>作者提出的模型非常有效, 因为它利用了点云中的稀疏性, 避免了在空白空间中搜索.</p><p>Compared to previous best methods, our model is more than <span class="math inline">\(4\times\)</span> smaller than FPointNet (the prior art on SUN RGB-D) in size and more than <span class="math inline">\(20\times\)</span> times faster than 3D-SIS (the prior art on ScanNetV2) in speed.</p><p>与以前的最佳方法相比, 作者的模型在尺寸上比FPointNet(SUN RGB-D上的现有技术)小4倍多, 在速度上比3D-SIS(ScanNetV2上的现有技术)快20倍多.</p><p><u>Model size and processing time (per frame or scan)</u>.</p><p>模型大小和处理时间(每帧或扫描).</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131039905.png" /></p><h4 id="qualitative-results-and-discussion部分">Qualitative Results and Discussion部分</h4><p><u>Qualitative results of 3D object detection in ScanNetV2</u>. Left: our VoteNet, Right: ground-truth.</p><p>ScanNetV2中三维目标检测的定性结果. 左: 作者的检测结果, 右: 真实值.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131041972.png" /></p><p><u>Qualitative results on SUN RGB-D</u>. Both left and right panels show (from left to right): an image of the scene (not used by our network), 3D object detection by VoteNet, and ground-truth annotations.</p><p>SUN RGB-D上的定性结果. 左右均显示(从左到右): 场景图像(VoteNet未使用)、VoteNet的三维对象检测结果和真实值.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131042169.png" /></p><p>There are still <u>limitations in our method</u> though.</p><p>不过，作者的方法仍然存在局限性.</p><p>Common failure cases include <u>misses on very thin objects</u> like doors, windows and pictures denoted in black bounding boxes in the top scene.</p><p>常见的失败情况包括对非常薄的对象(如门、窗和顶部场景中用黑色边框表示的图片)检测失败.</p><p>As we do <u>not make use of RGB information</u>, detecting these categories is almost impossible.</p><p>由于我们不使用RGB信息, 检测这些类别几乎是不可能的.</p><p>A less successful amodal prediction is shown in the bottom right scene where an extremely partial observation of a very large table is given.</p><p>在SUN RGB-D定性结果的右下角的场景中显示了一个不太成功的有向边界框预测, 其中给出了一个非常大的桌子的极小部分观测数据.</p><h3 id="supplement部分">Supplement部分</h3><h4 id="details-on-architectures-and-loss-functions部分">Details on Architectures and Loss Functions部分</h4><h5 id="votenet-architecture-details">VoteNet architecture details</h5><p>The backbone network, based on the PointNet++ architecture, has four set abstraction layers and two feature up-sampling layers.</p><p>主干网络基于PointNet++的体系结构, 有四个集合抽象层和两个特征上采样层.</p><p>The <u>detailed layer parameters are shown as follow</u>.</p><p>详细的图层参数如下所示.</p><p><u>Backbone network architecture</u>: layer specifications.</p><p>主干网络架构.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131349652.png" /></p><p>Each set abstraction (SA) layer has a receptive ﬁeld speciﬁed by a ball-region radius <span class="math inline">\(r\)</span>, a MLP network for point feature transform <span class="math inline">\(M L P\left[c_{1}, \ldots, c_{k}\right]\)</span> where <u><span class="math inline">\(c_i\)</span> is output channel number of the <span class="math inline">\(i\)</span>-th layer in the MLP</u>.</p><p>每个集合抽象(SA)层都有一个由半径<span class="math inline">\(r\)</span>指定的感受野, 一个用于点特征变换的MLP网络<span class="math inline">\(M L P\left[c_{1}, \ldots, c_{k}\right]\)</span>, 其中<span class="math inline">\(c_i\)</span>是MLP中第<span class="math inline">\(i\)</span>层的输出通道数.</p><p>The SA layer also subsamples the input point cloud with farthest point sampling to <span class="math inline">\(n\)</span> points.</p><p>SA层还对输入点云进行二次采样, 使用最远点采样为<span class="math inline">\(n\)</span>个点.</p><p>Each SA layer is speciﬁed by <span class="math inline">\(\left(n, r,\left[c_{1}, \ldots, c_{k}\right]\right)\)</span>.</p><p>每个SA层由<span class="math inline">\(\left(n, r,\left[c_{1}, \ldots, c_{k}\right]\right)\)</span>指定.</p><p>We also normalize the XYZ scale of points in each local region by the region radius.</p><p>通过区域半径标准化每个局部区域中点的XYZ尺度.</p><p>Each set feature propagation (FP) layer upsamples the point features by <u>interpolating the features on input points to output points</u> (each output point’s feature is weighted average of its three nearest input points’ features).</p><p>每个集合特征传播(FP)层通过将输入点上的特征插值到输出点来上采样点特征(每个输出点的特征是其最近三个输入点特征的加权平均值).</p><p>It also combines the skip-linked features through a MLP (interpolated features and skip-linked features are concatenated before fed into the MLP).</p><p>它还通过MLP组合跳接的特征(插入的特征和跳接的特征在进入到MLP之前连接在一起).</p><p>Each FP layer is speciﬁed by <span class="math inline">\(\left[c_{1}, \ldots, c_{k}\right]\)</span> where <span class="math inline">\(c_i\)</span> is the output of the <span class="math inline">\(i\)</span>-th layer in the MLP.</p><p>每个FP层由<span class="math inline">\(\left[c_{1}, \ldots, c_{k}\right]\)</span>指定, 其中<span class="math inline">\(c_i\)</span>是MLP中第<span class="math inline">\(i\)</span>层的输出.</p><p>The voting module as mentioned in the main paper is a MLP that <u>transforms seeds’ features to votes including a XYZ offset and a feature offset</u>.</p><p>投票模块是一个MLP, 它将种子点的特征转换为投票点, 包括XYZ偏移和特征偏移.</p><p>The seed points are outputs of the fp2 layer.</p><p>种子点是fp2层的输出.</p><p>The voting module MLP has output sizes of 256, 256, 259 for its fully connected layers.</p><p>投票模块MLP的全连接层的输出大小为256、256、259.</p><p>The last fully connected layer does not have ReLU or BatchNorm.</p><p>最后一个完全连接的层没有ReLU或BatchNorm.</p><p>The proposal module as mentioned in the main paper is a SA layer followed by another MLP after the max-pooling in each local region.</p><p>候选框生成模块是一个SA层, 在每个局部区域的最大池之后是另一个MLP.</p><p>The layer’s output has <span class="math inline">\(5 + 2N H + 4N S + N C\)</span> channels where <u><span class="math inline">\(N H\)</span> is the number of heading bins (we predict a classiﬁcation score for each heading bin and a regression offset for each bin – relative to the bin center and normalized by the bin size)</u>, <u><span class="math inline">\(N S\)</span> is the number of size templates (we predict a classiﬁcation score for each size template and 3 scale regression offsets for height, width and length)</u> and <u><span class="math inline">\(N C\)</span> is the number of semantic classes</u>.</p><p>该层的输出具有<span class="math inline">\(5+2N H+4N s+N C\)</span>个通道，其中<span class="math inline">\(N H\)</span>是朝向角类别数(预测每个朝向角的分类分数和回归偏移量), <span class="math inline">\(N S\)</span>是尺寸类别数(预测每个尺寸模板的分类分数以及高度、宽度和长度的3个尺度回归偏移量), 而<span class="math inline">\(N C\)</span>是语义类别的数量.</p><p>In SUN RGB-D: <span class="math inline">\(N H = 12\)</span>, <span class="math inline">\(N S = N C = 10\)</span>, in ScanNet: <span class="math inline">\(N H = 12\)</span>, <span class="math inline">\(N S = N C = 18\)</span>.</p><p>在SUN RGB-D中: <span class="math inline">\(N H=12\)</span>, <span class="math inline">\(N S=N C=10\)</span>, 在ScanNet中: <span class="math inline">\(N H=12\)</span>, <span class="math inline">\(N S=N C=18\)</span>.</p><p>In the ﬁrst 5 channels, the ﬁrst two are for objectness classiﬁcation and the rest three are for center regression (relative to the vote cluster center).</p><p>在前5个通道中, 前两个用于对象分类, 其余三个用于中心回归(相对于投票聚类中心).</p><h5 id="votenet-loss-function-details">VoteNet loss function details</h5><p>The network is trained end-to-end with a multi-task loss including <u>a voting loss, an objectness loss, a 3D bounding box estimation loss and a semantic classiﬁcation loss</u>.</p><p>网络通过多损失进行端到端训练, 包括投票损失、物体分数损失、三维边界框估计损失和语义分类损失.</p><p>We weight the losses such that they are in similar scales with <span class="math inline">\(\lambda_1 = 0.5\)</span>, <span class="math inline">\(\lambda_2 = 1\)</span> and <span class="math inline">\(\lambda_3 = 0.1\)</span>.</p><p>作者对损失进行加权, 使其处于类似的比例, 分别为<span class="math inline">\(\lambda_1=0.5\)</span>、<span class="math inline">\(\lambda_2=1\)</span>和<span class="math inline">\(\lambda_3=0.1\)</span>. <span class="math display">\[L_{\text {VoteNet }}=L_{\text {vote-reg }}+\lambda_{1} L_{\text {obj-cls }}+\lambda_{2} L_{\text {box }}+\lambda_{3} L_{\text {sem-cls }}\]</span></p><p>Among the losses, the vote regression loss is as deﬁned in the main paper (with L1 distance).</p><p>在损失中, 投票回归损失如正文中的定义(L1距离).</p><p>For ScanNet we <u>compute the ground truth votes as offset from the mesh vertices of an instances to the centers of the axis-aligned tight bounding boxes of the instances</u>.</p><p>对于ScanNet, 计算从实例的mesh顶点到实例的轴对齐紧边界框中心的偏移作为真实投票点的偏移.</p><p>Note that since the bounding box is not amodal, they can vary in sizes due to scan completeness (e.g. a chair may have a ﬂoating bounding box if its leg is not recovered from the reconstruction).</p><p>请注意，由于扫描的完整程度, 其大小可能会有所不同(例如, 如果椅子的腿未从重建中恢复则椅子可能有一个悬浮的边界框).</p><p>For SUN RGB-D since <u>the dataset does not provide instance segmentation annotations but only amodal bounding boxes</u>, we cannot compute a ground truth vote directly (as we don’t know which points are on objects).</p><p>对于SUN RGB-D, 由于数据集不提供实例分割注释, 而只提供有向边界框, 因此无法直接计算真实投票点(因为不知道对象上有哪些点).</p><p>Instead, we <u>consider any point inside an annotated bounding box as an object point (required to vote) and compute its offset to the box center as the ground truth</u>.</p><p>相反, 作者将注释框中的任何点视为投票点并将其偏移到框中心作为真实值.</p><p>In cases that a point is in multiple ground truth boxes, we keep a set of up to three ground truth votes, and <u>consider the minimum distance between the predicted vote and any ground truth vote in the set during vote regression on this point</u>.</p><p>在一个点位于多个真实边界框的情况下, 作者保持最多三个真实投票点并考虑在预测投票点和聚合过程中的任何具有与真实投票点之间的最小距离的预测投票点回归在这一真实投票点上.</p><p>The <u>objectness loss is just a cross-entropy loss for two classes</u> and the <u>semantic classiﬁcation loss is also a cross-entropy loss of <span class="math inline">\(N C\)</span> classes</u>.</p><p>物体分数损失是两个类的交叉熵损失, 语义分类损失也是<span class="math inline">\(N C\)</span>类的交叉熵损失.</p><p>The box loss follows (but without the corner loss regularization for simplicity) and is <u>composed of center regression, heading estimation and size estimation sub-losses</u>.</p><p>边界框损失(但为了简单起见, 没有角度损失正则化)由边界框中心回归、朝向角估计和尺寸估计等三个子损失组成.</p><p>In all regression in the box loss we use the <u>robust L1-smooth loss</u>.</p><p>在所有边界框回归损失中, 作者使用稳健的L1平滑损失.</p><p>Both the box and semantic losses are <u>only computed on positive vote clusters</u> and <u>normalized by the number of positive clusters</u>.</p><p>边界框和语义损失仅在正向投票点簇上计算并通过正向投票点簇的数量进行归一化. <span class="math display">\[L_{\text {box }} =L_{\text {center-reg }}+0.1 L_{\text {angle-cls }}+L_{\text {angle-reg }} +0.1 L_{\text {size-cls }}+L_{\text {size-reg }}\]</span> One difference though is that, instead of a naïve regression loss, we <u>use a <em>Chamfer loss</em> for <span class="math inline">\(L_{\text{center-reg}}\)</span> (between regressed centers and ground truth box centers)</u>.</p><p>不过, 一个不同之处是作者对<span class="math inline">\(L_{\text{center reg}}\)</span>(在回归中心点和真实中心点之间)使用Chamfer损失<span class="math inline">\(L_{\text{center-reg}}\)</span>而不是简单的回归损失.</p><p>It requires that each positive proposal is close to a ground truth object and each ground truth object center has a proposal near it.</p><p>这要求每个正向提案靠近一个真实物体, 每个真实物体中心在其附近有一个候选框.</p><p>The latter part also inﬂuences the voting in the sense that it encourages non-object seed points near the object to also vote for the center of the object, which <u>helps further increase contexts in detection</u>.</p><p>后一部分也影响投票, 因为它鼓励靠近物体的噪音种子点也投票给物体的中心, 这有助于进一步增加检测的上下文信息.</p><h5 id="boxnet-architecture-details">BoxNet architecture details</h5><p>Our baseline network without voting, BoxNet, shares most parts with the VoteNet.</p><p>BoxNet与VoteNet共享大部分内容.</p><p>They <u>share the same backbone architecture</u>.</p><p>它们共享相同的主干架构.</p><p>But instead of voting from seeds, the BoxNet <u>directly proposes bounding boxes and classiﬁes object classes from seed points’ features</u>.</p><p>但是, BoxNet没有从种子中投票, 而是直接预测边界框并根据种子点的特征对物体进行分类.</p><p>To make the BoxNet and VoteNet have similar capacity we also <u>include a SA layer for the proposal in BoxNet</u>.</p><p>为了使BoxNet和VoteNet具有类似的容量, 作者在BoxNet中的候选框生成模块中添加了SA层.</p><p>However this SA layer takes seed clusters instead of vote clusters i.e. it samples seed points and then combines neighboring seeds with MLP and max-pooling.</p><p>但是, 该SA层采用种子点簇而不是投票点簇, 即它对种子点进行采样, 然后将相邻种子点与MLP和最大值池相结合.</p><p>This SA layer has exactly the same layer parameters with that in the VoteNet, followed by the same <span class="math inline">\(\text{MLP}_2\)</span>.</p><p>此SA层与VoteNet中的层参数完全相同, 后跟相同的<span class="math inline">\(\text{MLP}_2\)</span>.</p><h5 id="boxnet-loss-function-details">BoxNet loss function details</h5><p>BoxNet has the same loss function as VoteNet, <u>except it is not supervised by vote regression</u>.</p><p>BoxNet与VoteNet具有相同的损失函数, 只是它不受投票回归损失的监督.</p><p>There is also <u>a slight difference in how objectness labels</u> (used to supervise objectness classiﬁcation) are computed.</p><p>物体分数标签(用于监督物体分数分类)的计算方式也略有不同.</p><p>As seed points (on object surfaces) are often far from object centroids, <u>it no longer works well to use the distances between seed points and object centroids to compute the objectness labels</u>.</p><p>由于种子点(在物体表面)通常远离物体中心, 因此使用种子点和物体中心之间的距离来计算物体分数不再有效.</p><p>In BoxNet, we <u>assign positive objectness labels to seed points that are on objects</u> (those belonging to the semantic categories we consider) and <u>negative labels to all the other seed points on clutter</u> (e.g. walls, ﬂoors).</p><p>在BoxNet中, 作者将正向物体分数标签指定给物体上的种子点(属于作者考虑的语义类别的种子点), 将负向标签指定给其他噪音种子点(例如墙、地板). <span class="math display">\[L_{\mathrm{BoxNet}}=\lambda_{1} L_{\mathrm{obj}-\mathrm{cls}}+\lambda_{2} L_{\mathrm{box}}+\lambda_{3} L_{\mathrm{sem}-\mathrm{cls}}\]</span></p><h4 id="more-analysis-experiments部分">More Analysis Experiments部分</h4><h5 id="average-precision-and-recall-plots">Average precision and recall plots</h5><p><u>Number of proposals per scene v.s. Average Precision (AP) and Average Recall (AR) on SUN RGB-D.</u></p><p>SUN RGB-D数据集上每个场景的候选框数量 v.s. 平均精度(AP)和平均召回率(AR).</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131409342.png" /></p><p>The AP and AR are <u>averaged across the 10 classes</u>.</p><p>AP和AR在10个类别中取平均值.</p><p>The recall is <u>maximum recall</u> given a fixed number of detection per scene.</p><p>召回率是给定每个场景固定数量的检测结果的最大召回率.</p><p>We report two ways of using the proposals: <u>joint and per-class</u>.</p><p>两种使用候选框的方式: 联合和每类.</p><p>The "joint proposal" means that we <u>assign each proposal to a single class</u> (the class with the highest classification score).</p><p>"联合候选"是指将每个候选框分配给一个类别(分类分数最高的类别).</p><p>The "per-class proposal" means that we <u>assign each proposal to all the 10 classes</u> (the objectness score is multiplied by the semantic classification probability).</p><p>"每类候选"指将每个候选框分配给所有10个类别(对象性分数乘以语义分类概率).</p><p>For the joint proposal we <u>propose <span class="math inline">\(K\)</span> objects’ bounding boxes for all the 10 categories</u>, where we consider each proposal as the semantic class it has the largest conﬁdence in, <u>and use their objectness scores to rank them</u>.</p><p>对于联合候选, 对于10个类别的<span class="math inline">\(K\)</span>个物体边界框, 每个候选框视为具有最大置信度的语义类别并使用物体分数对其进行排序(意思是先分类, 然后根据物体分数进行排序和NMS).</p><p>For the per-class proposal we <u>duplicate the <span class="math inline">\(K\)</span> proposal 10 times thus have <span class="math inline">\(K\)</span> proposals per class where we use the multiplication of semantic probability for that class and the objectness probability to rank them</u>.</p><p>对于每类建议, 作者将<span class="math inline">\(K\)</span>个物体框在每个类里进行语义类别预测, 每类有<span class="math inline">\(K\)</span>个候选框, 使用物体框在该类的语义概率和物体分数的乘积作为依据对它们进行排序(意思是按照每个框类别概率与物体分数的乘积作为依据进行排序和NMS).</p><p>The latter way of using proposals gives us <u>a slight improvement on AP and a big boost on AR</u>.</p><p>后一种使用建议的方式对AP有轻微的改进, 对AR有很大的提升.</p><h5 id="context-of-voting">Context of voting</h5><p>One difference of a deep Hough voting scheme with the traditional Hough voting is that we can <u>take advantage of deep features, which can provide more context knowledge for voting</u>.</p><p>深度霍夫投票方案与传统霍夫投票方案的一个不同之处在于可以利用深度学习的特征为投票提供更多的上下文信息.</p><p>In following table we show <u>how features from different levels of the PointNet++ affects detection performance</u> (from SA2 to FP3, the network has increasing contexts for voting).</p><p>在下表中, 作者展示了PointNet++不同级别的特性如何影响检测性能(从SA2到FP3, 网络中用于投票的上下文信息不断增加).</p><p><u>Effects of seed context for 3D detection</u>. Evaluation metric is mAP@0.25 on SUN RGB-D.</p><p>种子点上下文对三维检测的影响. 评估指标为mAP@0.25, 基于数据集SUN RGB-D.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131411338.png" /></p><p>FP3 layer is extended from the FP2 with a MLP of output sizes 256 and 256 with 2048 output points (the same set of XYZ as that output by SA1).</p><p>FP3层从FP2扩展而来, MLP的输出大小为256和256, 有2048个输出点(与SA1输出的XYZ相同).</p><h5 id="multiple-votes-per-seed">Multiple votes per seed</h5><p>In default we just <u>generate one vote per seed</u> since we ﬁnd that with large enough context there is little need to generate more than one vote to resolve ambiguous cases.</p><p>默认情况下, 只为每个种子生成一票, 因为在足够大的上下文中, 几乎不需要生成多个投票来解决歧义的情况.</p><p>However, it is still possible to <u>generate more than one vote with our network architecture</u>.</p><p>然而, VoteNet网络架构仍然可以产生多个投票.</p><p>Yet to break the symmetry in multiple vote generation, one has to <u>introduce some bias to different votes to prevent then from pointing to the same place</u>.</p><p>然而, 为了打破多重投票的对称性, 必须对不同的投票引入一些偏置, 以防止它们指向同一个地方.</p><p>In experiments, we ﬁnd that <u>one vote per seed achieves the best results</u>, as shown in following table.</p><p>在实验中, 作者发现每个种子点投一票可以获得最佳结果, 如下表所示.</p><p><u>Effects of number of votes per seed</u>. Evaluation metric is mAP@0.25 on SUN RGB-D.</p><p>每个种子点的投票数的影响. 评估指标为mAP@0.25, 使用SUN RGB-D数据集.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131413119.png" /></p><p>We ablate by using a vote factor of 3, where <u>the voting module generates 3 votes per seed with a MLP layer spec: [256, 256, 259 ∗ 3])</u>.</p><p>作者设计了投3张票的消融实验, 其中投票模块使用MLP层[256、256、259 * 3]为每个种子点生成3张投票.</p><p>In computing the vote regression loss on a seed point, we <u>consider the minimum distance between any predicted votes to the ground truth vote</u> (in case of SUN RGB-D where we may have a set of ground truth votes for a seed, we compute the minimum distance among any pair of predicted vote and ground truth vote).</p><p>在计算种子点的投票回归损失时, 作者考虑任何预测投票点与真实投票点之间的最小距离.</p><p>To break symmetry, we <u>generate 3 random numbers</u> and inject them to the second last features from the MLP layer.</p><p>为了打破对称性, 作者生成3个随机数并将它们注入MLP层的倒数第二个特征.</p><p>We show results both with and without this procedure which shows <u>no observable difference</u>.</p><p>作者展示了使用和不使用随机数的结果, 没有明显差异.</p><h5 id="on-proposal-sampling">On proposal sampling</h5><p>In the proposal step, <u>to generate <span class="math inline">\(K\)</span> proposals from the votes, we need to select <span class="math inline">\(K\)</span> vote clusters</u>.</p><p>在候选框生成这一步, 要从投票点中生成<span class="math inline">\(K\)</span>个候选框, 需要选择<span class="math inline">\(K\)</span>个投票点簇.</p><p>How to select those clusters is a design choice we study here (<u>each cluster is simply a group of votes near a center vote</u>).</p><p>对于如何选择这些簇, 作者进行了如下实验(每个簇是靠近中心投票点的一组投票点).</p><p><u>Effects of proposal sampling</u>. Evaluation metric is mAP@0.25 on SUN RGB-D.</p><p>不同候选框抽样方法的影响. 评估指标为mAP@0.25, 数据集为SUN RGB-D.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131416235.png" /></p><p><u>256 proposals</u> are used for all evaluations.</p><p>所有测试都是基于生成的256个候选框.</p><p>Our method is <u>not sensitive to how we choose centers for vote groups/clusters</u>.</p><p>作者提出的方法对如何选择投票组/簇中心不敏感.</p><p>From 1024 vote clusters, <u>vote FPS samples <span class="math inline">\(K\)</span> clusters based on votes’ XYZ</u>.</p><p>在1024个投票点簇中, 基于投票点的最远点采样根据投票点的XYZ获取<span class="math inline">\(K\)</span>簇.</p><p>Seed FPS ﬁrstly samples on seed XYZ and then <u>ﬁnds the votes corresponding to the sampled seeds</u>.</p><p>基于种子点的最远点采样首先在种子点XYZ上采样, 然后找到与采样种子点对应的投票点.</p><p>Random sampling simply <u>selects a random set of <span class="math inline">\(K\)</span> votes and take their neighborhoods for proposal generation</u>.</p><p>随机抽样只需选择一组随机的<span class="math inline">\(K\)</span>个投票点并将他们的邻近投票点用于候选框生成.</p><p>Note that <u>the results from above table are from the same model</u> trained with vote FPS to select proposals.</p><p>上表中的结果来自同一个模型, 该模型使用基于投票点的最远点采样进行训练, 以生成候选框.</p><p>We can see that while seed FPS gets the best number in mAP, the difference caused by different sampling strategies is small, <u>showing the robustness of our method</u>.</p><p>虽然基于种子点的最远点采样在mAP中最优, 但是不同的采样策略造成的差异很小, 这表明了作者使用的方法的鲁棒性.</p><h5 id="effects-of-the-height-feature">Effects of the height feature</h5><p>In point clouds from indoor scans, <u>point height is a useful feature in recognition</u>.</p><p>在室内扫描的点云中, 点高度是一个有用的识别特征.</p><p>As mentioned in the main paper, we can use <span class="math inline">\(1\%\)</span> of the <span class="math inline">\(Z\)</span> values (<span class="math inline">\(Z\)</span>-axis is up-right) of all points from a scan as an approximate as the ﬂoor height <span class="math inline">\(z_\text{ﬂoor}\)</span>, and then <u>compute the a point <span class="math inline">\((x, y, z)\)</span>’s height as <span class="math inline">\(z − z_\text{ﬂoor}\)</span></u>.</p><p>作者使用扫描中所有点的<span class="math inline">\(Z\)</span>值(<span class="math inline">\(Z\)</span>-轴向上向右)中的<span class="math inline">\(1\%\)</span>作为地面高度<span class="math inline">\(z_\text{floor}\)</span>的近似值, 然后将点<span class="math inline">\((x, y, z)\)</span>的高度使用<span class="math inline">\(z− z_\text{floor}\)</span>计算.</p><p>Effects of the height feature. Evaluation metric is mAP@0.25 on both datasets.</p><p>高度特征的影响. 评估指标为mAP@0.25.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131418704.png" /></p><p>We see that <u>adding the height feature consistently improves performance</u> in both SUN RGB-D and ScanNet.</p><p>添加高度特征可以提高SUN RGB-D和ScanNet数据集上VoteNet的性能.</p><h4 id="scannet-per-class-evaluation部分">ScanNet Per-class Evaluation部分</h4><p>3D object detection scores per category on the ScanNetV2 dataset, <u>evaluated with mAP@0.25 IoU</u>.</p><p>ScanNetV2数据集上每个类别的3D对象检测分数, 使用mAP@0.25 IoU评估方式.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131100180.png" /></p><p>3D object detection scores per category on the ScanNetV2 dataset, <u>evaluated with mAP@0.5 IoU</u>.</p><p>ScanNetV2数据集上每个类别的3D对象检测分数, 使用mAP@0.5 IoU评估方式.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131100772.png" /></p><p>Relying on purely geonetric data, <u>our method excels (esp. with mAP@0.25) in detecting objects where geometry is a strong cue for recognition</u>; and struggles with objects best recognized by texture and color like pictures.</p><p>作者提出的方法依靠纯粹的集合信息, 尤其是使用mAP@0.25这一指标进行物体检测时, 几何结构是识别的有力线索; 与使用纹理和颜色最能识别的物体就表现不佳.</p><h4 id="visualization-of-votes部分">Visualization of Votes部分</h4><p><u>Vote meeting point</u>. Left: ScanNet scene with votes coming from object points. Right: vote offsets from source seed-points to target-votes.</p><p>投票点可视化. 左: ScanNet场景中物体点的投票情况. 右: 从种子点到其投票点的位置偏移.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201131055607.png" /></p><p><u>Object votes are colored green</u>, and <u>non-object ones are colored red</u>.</p><p>物体投票为绿色, 非物体投票为红色.</p><p>See how <u>object points from all-parts of the object vote to form a cluster near the center</u>.</p><p>图中可以看到物体点如何在中心附近投票形成簇.</p><p>Non-object points, however, <u>either vote "nowhere" and therefore lack structure, or are near object and have gathered enough context to also vote properly</u>.</p><p>然而, 非对象点要么"无处投票", 因为缺乏结构信息, 要么靠近对象, 收集了足够的上下文来正确投票.</p><p>We clearly see that seed points on objects vote to object centers while <u>clutter points vote either to object enter as well (if the clutter point is close to the object) or to nowhere due to lack of structure in the clutter area</u>.</p><p>清楚地看到, 物体上的种子点投票给物体中心，噪音点也投票给物体中心(如果噪音点靠近物体), 或者由于噪音区域缺乏结构信息而不投票给任何地方.</p><h3 id="精读总结">精读总结</h3><blockquote><p>精读目标及效果自测：所读段落是否详细掌握，回答三个的终极问题(你是谁 - 论文提出/采用什么方法、细节是什么；从哪里来 - 论文要解决什么问题/任务、其启发点或借鉴之处在哪；到哪里去 - 论文方法达到什么效果)。</p></blockquote><p>论文结合基于点云的深度网络和传统的霍夫投票检测方法, 优化构建了一个基于霍夫投票的端到端的直接使用点云数据的三维室内物体检测器.</p><p>论文将传统的霍夫投票机制结合深度学习网络进行了端到端的改造, 解决了三维室内点云多覆盖在物体表面, 物体中心部分大概率为空的问题, 利用投票的方式汇聚集中物体特征, 从而实现物体检测.</p><p>论文设计的方案在只使用点云的情况下, 在现有的室内检测数据集上达到了SOTA并保持较快的预测速度.</p><h2 id="总结">总结</h2><blockquote><p>总结：总览全文，归纳总结，总结文中创新点，关键点，启发点（论文idea的来源或者说是论文的motivation）等重要信息。</p></blockquote><p>论文的创新点、关键点:</p><ul><li>使用深度神经网络和霍夫投票机制构建了一个点云深度霍夫投票检测器;</li><li>利用表面种子点投票的方法汇聚物体特征和信息到物体中心点以解决三维点云物体中心为空难以预测候选框的问题;</li><li>在目前主要的室内三维检测数据集上达到了SOTA.</li></ul><p>论文的启发点:</p><ul><li>传统霍夫投票机制;</li><li>直接使用点云数据进行三维物体检测.</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Text2Mesh: Text-Driven Neural Stylization for Meshes阅读笔记</title>
    <link href="/2022/01/06/Text2Mesh-Text-Driven-Neural-Stylization-for-Meshes%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/01/06/Text2Mesh-Text-Driven-Neural-Stylization-for-Meshes%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="text2mesh-text-driven-neural-stylization-for-meshes阅读笔记">Text2Mesh: Text-Driven Neural Stylization for Meshes阅读笔记</h1><blockquote><p>读论文三步曲：泛读，精读，总结。</p></blockquote><h2 id="泛读">泛读</h2><blockquote><p>泛读：快速浏览，把握概要。重点读标题、摘要、结论、所有小标题。</p></blockquote><h3 id="title部分">Title部分</h3><p>Text2Mesh: <u>Text-Driven</u> Neural <em>Stylization for Meshes</em></p><ul><li>任务: Stylization for Meshes [三维Mesh的风格变换]</li><li>特点: Text-Driven [文本驱动的]</li></ul><h3 id="abstract部分">Abstract部分</h3><p>Our framework, <u>Text2Mesh</u>, <u>stylizes a 3D mesh</u> by <u>predicting color and local geometric details</u> which <u>conform to a target text prompt</u>.</p><p>作者提出的架构Text2Mesh可以通过预测符合目标文本描述的颜色和局部几何细节来变换一个三维mesh的风格.</p><p>We consider <u>a disentangled representation of a 3D object</u> using <u>a ﬁxed mesh input (content)</u> coupled with <u>a learned neural network</u>, which we term <u>neural style field network</u>.</p><p>作者将三维物体的表示方法进行了解耦, 一个三维物体由固定的点云输入(内容)和一个与之相关联的神经网络, 作者称这个神经网络为NSF(Neural Style Field)网络.</p><p>In order to modify style, we obtain <u>a similarity score between a text prompt (describing style) and a stylized mesh</u> by harnessing the representational power of <a href="https://openai.com/blog/clip/">CLIP</a>.</p><p>作者利用<a href="https://openai.com/blog/clip/">CLIP</a>来获取文本描述和mesh风格之间的相似度分数.</p><p>Text2Mesh requires <u>neither a pre-trained generative model nor a specialized 3D mesh dataset</u>.</p><p>Text2Mesh既不需要一个预先训练好的生成模型, 也不需要一个特殊的三维mesh数据集.</p><p>It can handle <u>low-quality meshes</u> (non-manifold, boundaries, etc.) with arbitrary genus, and <u><em>does not require UV parameterization</em></u>.</p><p>Text2Mesh可以处理低质量的任意类型的mesh并且不需要任何UV参数.</p><p>Our code and results are available in our <u>project webpage</u>: <a href="https://threedle.github.io/text2mesh/">https://threedle.github.io/text2mesh/</a>.</p><p>Text2Mesh的项目主页是<a href="https://threedle.github.io/text2mesh/">https://threedle.github.io/text2mesh/</a>.</p><h4 id="clip"><a href="https://openai.com/blog/clip/">CLIP</a></h4><blockquote><p>参考资料: <a href="https://mileistone.github.io/work/2021/01/14/thought-on-connecting-text-and-images/">对Connecting Text and Images的理解</a>、<a href="https://blog.csdn.net/Only_Wolfy/article/details/112675777">CLIP: Connecting Text and Images 介绍</a>、<a href="https://github.com/exacity/deeplearningbook-chinese">Deep Learning 中文翻译</a></p></blockquote><p>动机:</p><ul><li>CV领域数据集标注成本高昂;</li><li>CV模型一般只能胜任一个任务, 迁移到新任务上非常困难;</li><li>CV模型泛化能力较差.</li></ul><p>解决方案:</p><ul><li>互联网上较容易搜集到大量成对的文本和图像, 对于任何一个图像文本对而言, 文本可以认为是图像的标签, 从而解决数据集的问题.</li><li>互联网上存在的这些成对的文本和图像, 数量大且差异大, 当我们在这样的数据集上训练一个表达能力足够强的模型时, 这个模型就能具备较强的泛化能力, 可以较容易迁移到其他新任务上.</li></ul><p>特点:</p><ul><li>zero-shot做得好, 在不同的数据集上表现还可以, 可以自定义任务, 而且效率很高.</li><li>高效, 虽然GPT3做zero-shot也很好, 但是CLIP吃的资源少, 计算量少, 训练效率高.</li><li>灵活和通用, 直接从自然语言中学习广泛的视觉概念, CLIP明显比现有的ImageNet模型更灵活和通用.</li></ul><p>CLIP的zero-shot:</p><ul><li>通过CLIP训练出来一个模型之后，满足以下条件的新任务都可以直接zero-shot进行识别.<ul><li>能够用文字描述清楚这个新分类任务中每个类别;</li><li>这个描述对应的概念在CLIP的训练集中出现过.</li></ul></li><li>CLIP把分类转换为了跨模态检索, 模型足够强的情况下, 检索会比分类扩展性强. CLIP的zero-shot其实就是把分类问题转化为了检索问题.</li><li>CLIP能够zero-shot, 而且效果不错的原因如下.<ul><li>训练集够大, zero-shot任务的图像分布在训练集中有类似的, zero-shot任务的concept在训练集中有相近的;</li><li>将分类问题转换为检索问题.</li></ul></li></ul><p>zero-shot和one-shot:</p><ul><li>只有一个标注样本的迁移任务被称为one-shot学习;</li><li>没有标注样本的迁移任务被称为zero-shot学习.</li></ul><h4 id="uv-parameterization">UV parameterization</h4><blockquote><p>参考资料: <a href="https://en.wikipedia.org/wiki/UV_mapping">UV mapping</a></p></blockquote><p>UV参数是指在进行三维纹理贴图的时候使用UV贴图的时候需要用到的参数.</p><p>UV贴图的过程是将二维的纹理特征转化为UV图, 之后将UV图转换到三维空间上.</p><p>常见的UV贴图是将二维纹理特征转换到三维球的空间上, 之后使用三维纹理球将纹理添加到三维物体上.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201051717992.png" /></p><p>对于三维纹理球上的任何一点<span class="math inline">\(P\)</span>, 假设向量<span class="math inline">\(\vec{d}\)</span>是球心到点<span class="math inline">\(P\)</span>对应的向量, 假设球的两极与<span class="math inline">\(y\)</span>轴对齐, UV图坐标范围为<span class="math inline">\([0, 1]\)</span>, 那么换算关系如下: <span class="math display">\[\begin{aligned}&amp;u=0.5+\frac{\arctan 2\left(d_{x}, d_{z}\right)}{2 \pi} \\&amp;v=0.5-\frac{\arcsin \left(d_{y}\right)}{\pi}\end{aligned}\]</span></p><h3 id="conclusion部分">Conclusion部分</h3><p>It can <u>predict structured textures</u> (e.g. bricks), <u>without a directional field or mesh parameterization</u>.</p><p>Text2Mesh在不需要方向域或mesh参数的情况下可以预测结构化的纹理特征(例如砖块).</p><p>Traditionally, the direction of texture patterns over 3D surfaces has been <u>guided by 3D shape analysis techniques</u>.</p><p>传统上三维物体表面的纹理形式的方向需要使用三维形状分析技术.</p><p>In this work, the texture direction is <u>driven by 2D rendered images</u>, which <u>capture the semantics of how textures appear in the real world</u>.</p><p>Text2Mesh中纹理的方向则是由二维渲染图片决定的, 二维渲染图片可以捕捉到纹理如何出现在真实世界中的这一语义信息.</p><p>Our system is capable of <u>generating out-of-domain stylized outputs</u>.</p><p>Text2Mesh能够生成out-of-domain的不同风格的输出.</p><p>Our framework uses a pre-trained CLIP model, which <u>has been shown to contain bias</u>.</p><p>Text2Mesh使用了一个预先训练的CLIP模型, 这个模型包含了一定的偏置.</p><p>We postulate that our proposed method can be used to <u>visualize, understand, and interpret such model biases in a more direct and transparent way</u>.</p><p>Text2Mesh可以以一种直接而透明的方式可视化和理解这一偏差并与之交互.</p><p>As future work, our framework could be used to <u>manipulate 3D content</u> as well.</p><p>未来, 作者希望Text2Mesh可以利用三维信息.</p><p>Instead of modifying a given input mesh, one could learn to <u>generate meshes from scratch</u> driven by a text prompt.</p><p>作者希望能够直接从零开始生成符合文本描述的mesh, 而不是修改输入的mesh.</p><p>Moreover, our NSF (Neural Style Field) is <u>tailored to a single 3D mesh</u>.</p><p>Text2Mesh是为生成单个mesh量身定制的.</p><p>It may be possible to <u>train a network to stylize a collection of meshes towards a target style</u> in a feed-forward manner.</p><p>未来可以尝试训练一个能够更改一系列mesh到指定风格的前馈神经网络.</p><h3 id="小标题分析">小标题分析</h3><ul><li>Introduction <em>[简介]</em></li><li>Related Work <em>[相关工作]</em></li><li>Method <em>[方法]</em><ul><li>Neural Style Field Network <em><u>[NSF网络]</u></em></li><li>Text-based correspondence <em><u>[基于文本的相关性]</u></em></li><li>Viewpoints and Augmentations <em><u>[视角和增强]</u></em></li></ul></li><li>Experiments <em>[实验]</em><ul><li>Neural Stylization and Controls <em>[风格转换与控制]</em></li><li>Text2Mesh Priors <em><u>[Text2Mesh先验]</u></em></li><li>Stylization Fidelity <em><u>[风格准确性]</u></em></li><li>Beyond Textual Stylization <em>[文本指定的风格之外]</em></li><li>Incorporating Symmetries <em><u>[吸收对称性]</u></em></li><li>Limitations <em><u>[局限性]</u></em></li></ul></li><li>Conclusion <em>[结论]</em></li><li>Supplement <em>[附加材料]</em><ul><li>Additional Results <em>[更多的结果]</em></li><li>High Resolution Stylization <em>[高分辨率的风格转换]</em></li><li>Choice of anchor view <em><u>[锚点视角的选择]</u></em></li><li>Training and Implementation Details <em>[训练和实现细节]</em><ul><li>Network Architecture <em><u>[网络架构]</u></em></li><li>Training <em>[训练]</em></li></ul></li><li>Baseline Comparison and User Study <em>[基线比较和用户调研]</em></li><li>Societal Impact <em>[对社会的冲击]</em></li></ul></li></ul><h3 id="泛读总结">泛读总结</h3><blockquote><p>泛读目标及效果自测：1.论文要解决什么问题? 2.论文采用了什么方法? 3.论文达到什么效果?</p></blockquote><p>论文要解决什么问题? - 基于文本的三维Mesh的风格变换</p><p>论文采用了什么方法? - 将三维物体解耦成固定的mesh和NSF(Neural Style Field)网络, 通过NSF网络来控制和修改mesh的风格. 利用<a href="https://openai.com/blog/clip/">CLIP</a>来获取文本描述和mesh风格之间的相似度分数, 用以监督网络的训练.</p><p>论文达到什么效果? - Text2Mesh可以较为准确的将文本指定的风格应用到输入的mesh上.</p><h2 id="精读">精读</h2><blockquote><p>精读：通过泛读找到需要精读重点部分和快速略过的部分，选出精华，仔细阅读。</p></blockquote><h3 id="introduction部分">Introduction部分</h3><p>We consider <u><em>content</em> as the global structure prescribed by a 3D mesh</u>, which <u>defines the overall shape surface and topology</u>.</p><p>作者考虑选择三维mesh作为全局结构的content, 三维mesh定义了全局的形状表面和拓扑结构.</p><p>We consider <u><em>style</em> as the object’s particular appearance or affect</u>, as <u>determined by its color and ﬁne-grained (local) geometric details</u>.</p><p>作者认为风格是物体特殊的外在形式和效果, 风格由颜色和好的局部细节决定.</p><p>We propose <u>expressing the desired style through natural language (a text prompt)</u>, similar to how a commissioned artist is provided a verbal or textual description of the desired work.</p><p>作者使用自然语言作为选择风格的方式.</p><p><u>A natural cue for modifying the appearance of 3D shapes is through 2D projections</u>, as they correspond with how humans and machines perceive 3D geometry.</p><p>修改三维物体外表的一个自然的想法是通过其二维投影, 因为这和人类和机器观测三维几何结构的方式相关.</p><p>We use a neural network to <u>synthesize color and local geometric details over the 3D input shape</u>, which we refer to as a <em>neural style field</em> (NSF).</p><p>作者使用神经网络来在全局三维物体上合成颜色和局部几何细节, 这些颜色和局部几何细节被称之为神经元风格域(NSF).</p><p>The weights the NSF network are optimized such that <u>the resulting 3D stylized mesh adheres to the style described by text</u>.</p><p>NSF网络的优化目标是让输出的三维mesh的风格与输入的文本相匹配.</p><p>In particular, our neural optimization is guided by <u>multiple 2D (CLIP-embedded) views of the stylized mesh matching our target text</u>.</p><p>NSF网络的优化方式是借助CLIP网络让输出三维mesh的多视角的二维图片的风格与输入文本相匹配.</p><p>Text2Mesh produces <u>color and geometric details</u> over a variety of source meshes, driven by a target text prompt.</p><p>Text2Mesh可以在输入的三维mesh基础上合成符合输入文本描述的颜色和几何细节.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061101377.png" /></p><p>Our method produces <u>different colors and local deformations for the same 3D mesh</u> content to match the speciﬁed text.</p><p>作者提出的方法能够对相同的三维mesh合成不同的颜色和局部变形来满足不同的文本描述.</p><p>Moreover, Text2Mesh produces structured textures that are aligned with salient features, <u>without needing to estimate sharp 3D curves or a mesh parameterization</u>.</p><p>Text2Mesh可以合成与突出特征对齐的结构化的纹理且不需要估计尖锐的三维曲线或mesh参数.</p><p>Given a source mesh (gray), our method produces stylized meshes (<u>containing color and local geometric displacements</u>) which conform to various target texts.</p><p>输入一个mesh, Text2Mesh可以合成颜色和局部的几何变换来满足不同的文本描述.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061103977.png" /></p><p>Our method also demonstrates global understanding; e.g. in following figure human body parts are stylized in accordance with their semantic role.</p><p>Text2Mesh具有全局性的理解能力, 例如在下图中, 人体的各个部位分别与语义描述的对象的各个部位相对应.</p><p>Given the same input bare mesh, our neural style ﬁeld network produces <u>deformations for outerwear of various types</u> (capturing ﬁne details such as creases in clothing and complementary accessories), and <u>distinct features such as muscle and hair</u>.</p><p>输入相同的mesh, Text2Mesh可以为不同的类型的衣物(捕捉到好的细节, 例如衣服的褶皱和装饰物)和显著的特征(例如毛发和肌肉)生成相对应的形变.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061105095.png" /></p><p>We use the weights of the NSF network to encode a stylization (e.g. color and displacements) over the <em>explicit</em> mesh surface.</p><p>作者使用NSF网络来编码在显示的mesh表面的风格样式.</p><p>Meshes faithfully portray 3D shapes and can accurately represent sharp, extrinsic features using a high level of detail.</p><p>三维mesh可以很好的描绘三维形状并且可以利用高层级的细节准确的再现形状和外在特征.</p><p>Our neural style ﬁeld is <u><em>complementary</em> to the mesh content</u>, and appends colors and small displacements to the input mesh.</p><p>NSF是对mesh的一种补充, NSF包含有为mesh加入颜色和微小形变的信息.</p><p>Speciﬁcally, our neural style ﬁeld network <u>maps points on the mesh surface to style attributes</u> (i.e., RGB colors and displacements).</p><p>NSF可以将mesh表面的点与其风格属性进行匹配(例如颜色和偏移).</p><p>We guide the NSF network by <u>rendering the stylized 3D mesh from multiple 2D views and measuring the similarity of those views against the target text</u>, using CLIP’s embedding space.</p><p>作者使用风格化后的三维mesh的多视角图片在CLIP上计算的与目标文本的相似度分数来监督NSF网络的训练.</p><p>However, a straightforward optimization of the 3D stylized mesh which maximizes the CLIP similarity score <u>converges to a degenerate (i.e. noisy) solution</u>.</p><p>直接优化使得基于CLIP的相似度分数最大会让模型收敛到一个退化的结果.</p><p>Speciﬁcally, we observe that the joint text-image embedding space <u>contains an abundance of <em>false positives</em></u>, where a valid target text and a degenerate image (i.e. noise, artifacts) result in a high similarity score.</p><p>作者观察到文本-图像空间包含很多假阳性案例, 意思是一个有效的文本与一张退化的图片(例如有噪音和瑕疵)可以得到一个高的相似度分数.</p><p>Therefore, <u>employing CLIP for stylization requires careful regularization</u>.</p><p>使用CLIP来做风格转换需要很小心的正则化.</p><p>We <u>leverage multiple <em>priors</em> to effectively guide our NSF network</u>.</p><p>作者使用多重先验信息来指导NSF网络的训练.</p><p>The <u>3D mesh input acts as a <em>geometric prior</em></u> that imposes global shape structure, as well as local details that indicate the appropriate position for stylization.</p><p>输入的三维mesh作为包含全局形状结构信息和局部细节的几何先验.</p><p>The <u>weights of the NSF network act as a <em>neural prior</em></u> (i.e. regularization technique), which tends to favor smooth solutions.</p><p>NSF网络的权重作为神经元先验(例如正则化技术)来获取光滑的结果.</p><p>In order to produce accurate styles which contain high-frequency content with high ﬁdelity, we <u>use a frequency-based positional encoding</u>.</p><p>作者使用基于频率的位置编码来产生正确的包含高可信度的高频内容的风格.</p><p>We garner a strong signal about the quality of the neural style ﬁeld by <u>rendering the stylized mesh from multiple 2D views and then applying 2D augmentations</u>.</p><p>作者通过获取三维风格化mesh的多视角图片并将其进行图像增强来获取一个评估NSF生成质量的信号.</p><p>This results in a system which can <u>effectively avoid degenerate solutions</u>, while still <u>maintaining high-ﬁdelity results</u>.</p><p>这能够让系统避免陷入退化的结果同时能够保证高可信度的结果.</p><p>The focus of our work is <u>text-driven stylization</u>, since text is easily modiﬁable and can effectively express complex concepts related to style.</p><p>作者的目标是设计一个文本驱动的风格化网络, 因为文本可以被容易地编辑并且可以表达复杂风格概念.</p><p>Beyond text, our framework extends to <u>additional target modalities, such as images, 3D meshes, or even cross-modal combinations</u>.</p><p>除了文本, 作者提出的架构可以拓展到其他模态, 比如图片、三维mesh甚至跨模态的结合体.</p><p>In summary, we present <u>a technique for the semantic manipulation of style for 3D meshes</u>, harnessing the representational power of CLIP.</p><p>作者提出了一个基于CLIP使用语义信息修改三维mesh风格的技术.</p><p>Our system <u>combines the advantages of <em>explicit</em> mesh surfaces and the generality of neural ﬁelds to facilitate intuitive control for stylizing 3D shapes</u>.</p><p>作者提出的系统融合了显式表达的mesh表面和NSF对于三维形状风格的生成能力的优势.</p><p>A notable advantage of our framework is <u>its ability to handle low-quality meshes (e.g., non-manifold) with arbitrary genus</u>.</p><p>作者提出的系统的另一大优点是能够处理任意类别的低质量的mesh.</p><p>We show that <u>Text2Mesh can stylize a variety of 3D shapes with many different target styles</u>.</p><p>作者展示了Text2Mesh可以将大量不同的三维mesh进行不同的风格化.</p><h3 id="related-work部分">Related Work部分</h3><h4 id="text-driven-manipulation">Text-Driven Manipulation</h4><p>The above techniques (StyleCLIP, StyleGAN, VQGAN-CLIP, etc.) leverage <u>a pre-trained generative network or a dataset</u> to avoid the degenerate solutions common when using CLIP for synthesis.</p><p>这些技术(StyleCLIP、StyleGAN、VQGAN-CLIP等等)都利用预训练模型或者数据集来避免CLIP在合成上带来的退化结果.</p><p>The ﬁrst to leverage CLIP for synthesis <u>without the need for a pre-trained network or dataset</u> is CLIPDraw.</p><p>CLIPDraw是第一个没有使用预训练模型或数据集来避免这一问题的算法.</p><p>Concurrent work uses CLIP to <u>optimize over parameters of the SMPL human body model to create digital creatures</u>.</p><p>最近的一些工作使用CLIP来优化SMPL人体参数来创建数字生物.</p><p>Prior to CLIP, <u>text-driven control for deforming 3D shapes was explored using specialized 3D datasets</u>.</p><p>在CLIP之前, 文本驱动的三维形状控制一般是建立在特殊化的数据集上.</p><h4 id="geometric-style-transfer-in-3d">Geometric Style Transfer in 3D</h4><p>Some approaches <u>analyze 3D shapes and identify similarly shaped geometric elements and parts which differ in style</u>.</p><p>一些方法通过分析三维物体的一致性和差异性来进行几何风格迁移.</p><p>Others transfer geometric style based on <u>content/style separation</u>.</p><p>另一些方法则是基于内容/风格分离的思路.</p><p>Other approaches are <u>speciﬁc to categories</u> of furniture, 3D collages, LEGO, and portraits.</p><p>另一些方法则是基于特定的类别.</p><p>The above methods <u>rely on 3D datasets</u>, while other techniques <u>use a single mesh exemplar for synthesizing geometric textures or producing mesh reﬁnements</u>.</p><p>上述的方法都需要三维数据集, 其他的技术也需要使用一个mesh样例来合成几何纹理和mesh修复.</p><p>Shapes can be edited to contain <u>cubic stylization</u>, or <u>stripe patterns</u>.</p><p>形状可以被修改来包含立方体风格形式或者条纹样式.</p><p>Unlike these methods, we consider <u>a wide range of styles</u>, guided by an intuitive and compact (text) speciﬁcation.</p><p>与这些方法不同, 作者提出的方法能够使用符合直觉的精炼的文字描述来指定生成大范围的不同的风格.</p><h4 id="texture-transfer-in-3d">Texture Transfer in 3D</h4><p>Aspects of a 3D mesh style can be controlled by <u>texturing a surface through mesh parameterization</u>.</p><p>三维mesh的风格可以被mesh参数定义的表面纹理所控制.</p><p>However, most parameterization approaches <u>place strict requirements on the quality of the input mesh</u> (e.g., a manifold, non-intersecting, and low/zero genus), which do not hold for most meshes in the wild.</p><p>但是这种方式对mesh的质量要求很高, 大多数mesh达不到这种要求.</p><p>We avoid parameterization altogether and opt to modify appearance using a neural ﬁeld which <u>provides a style value (i.e., an RGB value and a displacement) for every vertex on the mesh</u>.</p><p>作者提出的方法避免了这一问题, 作者提出的方法直接使用NFS为每个mesh顶点生成对应的风格数值(颜色和偏移)从而达到修改mesh外表的目的.</p><p>Recent work explored a neural representation of texture, here we consider <u>both color and local geometry changes</u> for the manipulation of style.</p><p>近期的一些工作探索了纹理的神经元表示, 作者同时考虑与风格对应的颜色和几何变化.</p><h4 id="neural-priors-and-neural-fields">Neural Priors and Neural Fields</h4><p>Our framework <u>leverages the inductive bias of neural networks to act as a prior which guides Text2Mesh away from degenerate solutions present in the CLIP embedding space</u>.</p><p>作者提出的架构利用神经网络的偏置作为先验信息来防止Text2Mesh收敛到一个退化的结果.</p><p>Speciﬁcally, our stylization network acts as a neural prior, which <u>leverages positional encoding to synthesize ﬁne-grained stylization details</u>.</p><p>作者提出的网络可以利用位置编码来合成好的风格化的细节.</p><p>NeRF and follow ups have demonstrated success on 3D scene modeling.</p><p>NeRF及其后继者在三维场景建模领域获得了巨大的成功.</p><p>They leverage a neural ﬁeld to represent 3D objects using network weights.</p><p>他们利用神经元域使用网络权重来表示三维物体.</p><p>However, <u>neural ﬁelds commonly entangle geometry and appearance, which limits separable control of content and style</u>.</p><p>神经元域通常会融合几何和外表特征, 这限制了分别控制这两种特征的内容和风格的能力.</p><p>Moreover, <u>they struggle to accurately portray sharp features, are slow to render, and are difﬁcult to edit</u>.</p><p>神经元域还会生成很锐利的特征, 这些特征渲染速度很慢并且很难编辑.</p><p>Instead, our method <u>uses a disentangled representation of a 3D object using an explicit mesh representation of shape and a neural style ﬁeld which controls appearance</u>.</p><p>作者提出的方法使用解耦的表达, 将物体分为显式的mesh形状和基于NSF的外表.</p><p>This formulation <u>avoids parametrization</u>, and can be used to <u>easily manipulate appearance and generate high resolution outputs</u>.</p><p>这种方式避免了参数化并且可以轻松的控制外表和生成高分辨率的结果.</p><h3 id="method部分">Method部分</h3><p>Text2Mesh <u>modiﬁes an input mesh to conform to the target text</u> by predicting color and geometric details.</p><p>Text2Mesh通过修改输入mesh的几何特征和颜色来使之满足目标文本的需求.</p><p><u>The weights of the neural style network are optimized by rendering multiple 2D images and applying 2D augmentations</u>, which are given a similarity score to the target from the CLIP-based semantic loss.</p><p>通过渲染多幅二维图像并使用图像增强技术来优化NSF网络的权重, 这是通过基于CLIP的语义损失函数实现的.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061421943.png" /></p><p>As an overview, <u>the 3D object <em>content</em> is deﬁned by an input mesh <span class="math inline">\(M\)</span> with vertices <span class="math inline">\(V \in \mathbb{R}^{n \times 3}\)</span> and faces <span class="math inline">\(F \in\{1, \ldots, n\}^{m \times 3}\)</span>, and is ﬁxed throughout training</u>.</p><p>三维物体内容是由输入mesh定义的, 其顶点<span class="math inline">\(V \in \mathbb{R}^{n \times 3}\)</span>, 面片<span class="math inline">\(F \in\{1, \ldots, n\}^{m \times 3}\)</span>, 这在整个训练过程中都是固定的.</p><p><u>The object’s style (color and local geometry) is modiﬁed to conform to a target text prompt <span class="math inline">\(t\)</span></u>, resulting in a stylized mesh <span class="math inline">\(M^S\)</span>.</p><p>生成的物体风格需要符合目标文本<span class="math inline">\(t\)</span>的描述, 记格式化的mesh为<span class="math inline">\(M^S\)</span>.</p><p>The NSF learns to <u>map points on the mesh surface <span class="math inline">\(p \in V\)</span> to an RGB color and displacement along the normal direction</u>.</p><p>NSF需要学习mesh的表面点<span class="math inline">\(p \in V\)</span>与最终风格的颜色和沿着法线方向的偏移量之间的映射.</p><p>We <u>render <span class="math inline">\(M^S\)</span> from multiple views and apply 2D augmentations</u> that are embedded using CLIP.</p><p>作者使用多视角渲染<span class="math inline">\(M^S\)</span>并使用增强技术增强图片, 之后将其嵌入到CLIP空间中.</p><p>The <u>CLIP similarity between the rendered and augmented images and the target text is used as a signal to update the neural network weights</u>.</p><p>将渲染图片和增强图片与目标文本之间的CLIP相似度分数用作更新神经网络权重的信号.</p><h4 id="neural-style-field-network部分">Neural Style Field Network部分</h4><p>Our NSF network <u>produces a style attribute for every vertex which results in a <em>style ﬁeld</em> deﬁned over the entire shape surface</u>.</p><p>NSF网络为风格域上的每个表面点分别计算风格属性.</p><p>Our style ﬁeld is represented as an MLP, which <u>maps a point <span class="math inline">\(p \in V\)</span> on the mesh surface <span class="math inline">\(M\)</span> to a color and displacement along the surface normal <span class="math inline">\((c_p, d_p) \in (\mathbb{R}^{3}, \mathbb{R})\)</span></u>.</p><p>风格域被一个多重感知器描述, 将mesh的面上一点<span class="math inline">\(p \in V\)</span>与颜色和沿着表面法线方向的偏移<span class="math inline">\((c_p, d_p) \in (\mathbb{R}^{3}, \mathbb{R})\)</span>建立映射关系.</p><p>In practice, we <u>treat the given vertices of <span class="math inline">\(M\)</span> as query points into this ﬁeld</u>, and <u>use a differentiable renderer to visualize the style over the given triangulation</u>.</p><p>作者将<span class="math inline">\(M\)</span>中的表面点视作在风格域中进行查询的点, 使用一个差分渲染器来可视化输出的风格.</p><p><u>Increasing the number of triangles in <span class="math inline">\(M\)</span></u> for the purposes of learning a neural ﬁeld with ﬁner granularity is trivial.</p><p>可以尝试增加<span class="math inline">\(M\)</span>中三角面片的数量来获取更好的表现.</p><p>Even using a standard GPU (11GB of VRAM) our method handles meshes with up to 180K triangles.</p><p>使用一个常规的GPU, 作者的方法能够处理180K个三角面片.</p><p>Since our NSF uses low-dimensional coordinates as input to an MLP, <u>this exhibits a spectral bias toward smooth solutions</u>.</p><p>由于NSF使用低维度的坐标作为多重感知器的输入, 因此会有倾向于光滑结果的频谱偏置.</p><p>To synthesize high-frequency details, we <u>apply a positional encoding using fourier feature mappings</u>, which enables MLPs to overcome the spectral bias and learn to interpolate high-frequency functions.</p><p>为了能够合成高频信息, 作者使用了傅里叶特征的位置编码, 这帮助多重感知器克服频谱偏置并学习到高频信息.</p><p><u>For every point <span class="math inline">\(p\)</span> its positional encoding <span class="math inline">\(\gamma(p)\)</span> is given by:</u></p><p>对于任意一点<span class="math inline">\(p\)</span>, 其位置编码<span class="math inline">\(\gamma(p)\)</span>定义如下: <span class="math display">\[\gamma(p)=[\cos (2 \pi \mathbf{B} p), \sin (2 \pi \mathbf{B} p)]^{\mathrm{T}}\]</span></p><p>where <u><span class="math inline">\(B \in \mathbb{R}^{n \times 3}\)</span> is a random Gaussian matrix where each entry is randomly drawn from <span class="math inline">\(\mathcal{N}\left(0, \sigma^{2}\right)\)</span></u>.</p><p>其中<span class="math inline">\(B \in \mathbb{R}^{n \times 3}\)</span>是一个高斯随机矩阵, 其分布由<span class="math inline">\(\mathcal{N}\left(0, \sigma^{2}\right)\)</span>确定.</p><p><u>The value of <span class="math inline">\(\sigma\)</span> is chosen as a hyperparameter which controls the frequency of the learned style function.</u></p><p><span class="math inline">\(\sigma\)</span>是一个控制风格频率的超参数.</p><p>First, we <u>normalize the coordinates <span class="math inline">\(p \in V\)</span> to lie inside a unit bounding box</u>.</p><p>首先, 作者将<span class="math inline">\(p \in V\)</span>归一化到一个单位大小的正方体内.</p><p>Then, the <u>per-vertex positional encoding features <span class="math inline">\(\gamma(p)\)</span> are passed as input to an MLP <span class="math inline">\(N_s\)</span>, which then branches out to MLPs <span class="math inline">\(N_d\)</span> and <span class="math inline">\(N_c\)</span></u>.</p><p>之后, 每个顶点的位置编码<span class="math inline">\(\gamma(p)\)</span>会被传到多重感知器<span class="math inline">\(N_s\)</span>里, 然后会分出<span class="math inline">\(N_d\)</span>和<span class="math inline">\(N_c\)</span>两个多重感知器.</p><p>Speciﬁcally, <u>the output of <span class="math inline">\(N_c\)</span> is a color <span class="math inline">\(c_p \in [0, 1]^3\)</span></u>, and <u>the output of <span class="math inline">\(N_d\)</span> is a displacement along the vertex normal <span class="math inline">\(d_p\)</span></u>.</p><p><span class="math inline">\(N_c\)</span>的输出是颜色<span class="math inline">\(c_p \in [0, 1]^3\)</span>, <span class="math inline">\(N_d\)</span>的输出是沿着顶点的法线方向的位移量<span class="math inline">\(d_p\)</span>.</p><p>To <u>prevent content-altering displacements</u>, we constrain <span class="math inline">\(d_p\)</span> to be in the range <span class="math inline">\((−0.1, 0.1)\)</span>.</p><p>为了防止越界和形状变化过大, 作者限制<span class="math inline">\(d_p\)</span>的范围为<span class="math inline">\((−0.1, 0.1)\)</span>.</p><p>To obtain our stylized mesh prediction <span class="math inline">\(M^S\)</span>, <u>every point <span class="math inline">\(p\)</span> is displaced by <span class="math inline">\(d_p \cdot \vec{n}_p\)</span> and colored by <span class="math inline">\(c_p\)</span></u>.</p><p>为了获取最终的<span class="math inline">\(M^S\)</span>, 所有的点<span class="math inline">\(p\)</span>都需要位移<span class="math inline">\(d_p \cdot \vec{n}_p\)</span>并更新颜色<span class="math inline">\(c_p\)</span>.</p><p>Vertex colors propagate over the entire mesh surface <u>using an interpolation-based differentiable renderer</u>.</p><p>顶点颜色使用基于插值的差分渲染的方式传播到整个表面.</p><p>During training we also consider <u>the displacement-only mesh <span class="math inline">\(M_{\text{displ}}^S\)</span>, which is the same as <span class="math inline">\(M^S\)</span> without the predicted vertex colors (replaced by gray)</u>.</p><p>在训练期间, 作者也考虑仅有偏移量的<span class="math inline">\(M_{\text{displ}}^S\)</span>, 其与<span class="math inline">\(M^S\)</span>的几何结构一模一样只是没有预测的颜色(全部被灰色替代).</p><p>Without the use of <span class="math inline">\(M_{\text{displ}}^S\)</span> in our ﬁnal loss formulation, <u>the learned geometric style is noisier</u>.</p><p>如果不在损失函数中使用<span class="math inline">\(M_{\text{displ}}^S\)</span>会导致生成结果的几何结构包含许多噪音.</p><h4 id="text-based-correspondence部分">Text-based correspondence部分</h4><p>Our neural optimization is guided by the <u>multi-modal embedding space provided by a pre-trained CLIP model</u>.</p><p>作者提出的网络的优化是基于将多模态表示使用预训练的CLIP模型嵌入到CLIP空间的方式进行的.</p><p>Given the stylized mesh <span class="math inline">\(M^S\)</span> and the displaced mesh <span class="math inline">\(M_{\text{displ}}^S\)</span>, we <u>sample <span class="math inline">\(n_\theta\)</span> views around a pre-deﬁned anchor view and render them using a differentiable renderer</u>.</p><p>对于格式化后的mesh输出<span class="math inline">\(M^S\)</span>及其对应的几何结构<span class="math inline">\(M_{\text{displ}}^S\)</span>, 作者基于预先定义好的锚点视角随机采样<span class="math inline">\(n_\theta\)</span>个视角, 根据这些视角使用差分渲染器获取渲染的图片.</p><p>For each view, <span class="math inline">\(\theta\)</span>, we <u>render two 2D projections of the surface, <span class="math inline">\(I_{\theta}^{\text{full}}\)</span> for <span class="math inline">\(M^S\)</span> and <span class="math inline">\(I_{\theta}^{\text{displ}}\)</span> for <span class="math inline">\(M_{\text{displ}}^S\)</span></u>.</p><p>对每个视角<span class="math inline">\(\theta\)</span>, 作者获取两张渲染图, <span class="math inline">\(M^S\)</span>对应的<span class="math inline">\(I_{\theta}^{\text{full}}\)</span>和<span class="math inline">\(M_{\text{displ}}^S\)</span>对应的<span class="math inline">\(I_{\theta}^{\text{displ}}\)</span>.</p><p>Next, we draw <u>a 2D augmentation <span class="math inline">\(\psi_{\text{global}} \in \Psi_{\text{global}}\)</span> and <span class="math inline">\(\psi_{\text{local}} \in \Psi_{\text{local}}\)</span></u>.</p><p>之后, 作者使用两个图像增强方法<span class="math inline">\(\psi_{\text{global}} \in \Psi_{\text{global}}\)</span>和<span class="math inline">\(\psi_{\text{local}} \in \Psi_{\text{local}}\)</span>.</p><p>We <u>apply <span class="math inline">\(\psi_{\text{global}}\)</span>, <span class="math inline">\(\psi_{\text{local}}\)</span> to the full view and <span class="math inline">\(\psi_{\text{local}}\)</span> to the uncolored view, and embed them into CLIP space</u>.</p><p>作者将<span class="math inline">\(\psi_{\text{global}}\)</span>和<span class="math inline">\(\psi_{\text{local}}\)</span>应用到<span class="math inline">\(I_{\theta}^{\text{full}}\)</span>, 将<span class="math inline">\(\psi_{\text{local}}\)</span>应用到<span class="math inline">\(I_{\theta}^{\text{displ}}\)</span>, 并将增强后的结果嵌入到CLIP空间.</p><p>Finally, we <u>average the embeddings across all views</u>:</p><p>最后, 作者计算所有视角嵌入CLIP空间的平均值. <span class="math display">\[\begin{aligned}\hat{S}^{\text {full }} &amp;=\frac{1}{n_{\theta}} \sum_{\theta} E\left(\psi_{\text {global }}\left(I_{\theta}^{\text {full }}\right)\right) \in \mathbb{R}^{512} \\\hat{S}^{\text {local }} &amp;=\frac{1}{n_{\theta}} \sum_{\theta} E\left(\psi_{\text {local }}\left(I_{\theta}^{\text {full }}\right)\right) \in \mathbb{R}^{512} \\\hat{S}^{\text {displ }} &amp;=\frac{1}{n_{\theta}} \sum_{\theta} E\left(\psi_{\text {local }}\left(I_{\theta}^{\text {displ }}\right)\right) \in \mathbb{R}^{512}\end{aligned}\]</span></p><p>That is, we <u>consider an augmented representation of our input mesh as the average of its encoding from multiple augmented views</u>.</p><p>作者将输入mesh的一个增强的表现形式看作其多个增强后的视角编码的平均值.</p><p>The <u>target <span class="math inline">\(t\)</span> is similarly embedded through CLIP by <span class="math inline">\(\phi_{\text{target}} = E (t) \in \mathbb{R}^{512}\)</span></u>.</p><p>目标<span class="math inline">\(t\)</span>也类似的嵌入CLIP空间<span class="math inline">\(\phi_{\text{target}} = E (t) \in \mathbb{R}^{512}\)</span>.</p><p>Our loss is then:</p><p>损失函数如下: <span class="math display">\[\mathcal{L}_{\mathrm{sim}}=\sum_{\hat{S}} \operatorname{sim}\left(\hat{S}, \phi_{\mathrm{target}}\right)\]</span> where <span class="math inline">\(\hat{S} \in\left\{\hat{S}^{\text {full }}, \hat{S}^{\text {displ }}, \hat{S}^{\text {local }}\right\}\)</span> and <span class="math inline">\(\operatorname{sim}(a, b)=\frac{a \cdot b}{|a| \cdot|b|}\)</span> is the <u>cosine similarity between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span></u>.</p><p>其中<span class="math inline">\(\hat{S} \in\left\{\hat{S}^{\text {full }}, \hat{S}^{\text {displ }}, \hat{S}^{\text {local }}\right\}\)</span>, 并且<span class="math inline">\(\operatorname{sim}(a, b)=\frac{a \cdot b}{|a| \cdot|b|}\)</span>是<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>之间的余弦相似度.</p><p>We <u>repeat the above with new sampled augmentations <span class="math inline">\(n_{\text{aug}}\)</span> times for each iteration</u>.</p><p>每次迭代, 作者重复上述的采样增强过程<span class="math inline">\(n_{\text{aug}}\)</span>次.</p><p>We note that the terms <u>using <span class="math inline">\(\hat{S}^{\text{full}}\)</span> and <span class="math inline">\(\hat{S}^{\text{local}}\)</span> update <span class="math inline">\(N_s\)</span>, <span class="math inline">\(N_c\)</span> and <span class="math inline">\(N_d\)</span> while the term using <span class="math inline">\(\hat{S}^{\text{displ}}\)</span> only updates <span class="math inline">\(N_s\)</span> and <span class="math inline">\(N_d\)</span></u>.</p><p>作者使用<span class="math inline">\(\hat{S}^{\text{full}}\)</span>和<span class="math inline">\(\hat{S}^{\text{local}}\)</span>更新<span class="math inline">\(N_s\)</span>、<span class="math inline">\(N_c\)</span>和<span class="math inline">\(N_d\)</span>, 使用<span class="math inline">\(\hat{S}^{\text{displ}}\)</span>只更新<span class="math inline">\(N_s\)</span>和<span class="math inline">\(N_d\)</span>.</p><p>The <u>separation into a geometry-only loss and geometry-and-color loss is an effective tool for encouraging meaningful changes in geometry</u>.</p><p>分别计算纯几何损失函数和几何及颜色损失函数是一个有效促进有意义的几何变化的方式.</p><h4 id="viewpoints-and-augmentations部分">Viewpoints and Augmentations部分</h4><p>Given an input 3D mesh and target text, we ﬁrst ﬁnd an <u>anchor view</u>.</p><p>对于给定的三维mesh和目标文本, 先要找到锚定视角.</p><p>We render the 3D mesh at <u>uniform intervals around a sphere</u> and <u>obtain the CLIP similarity for each view and target text</u>.</p><p>首先按照均匀间隔绕着一个球用不同的视角渲染三维mesh, 然后将渲染的图片与目标文本进行匹配计算CLIP相似度分数.</p><p>We <u>select the view with the highest (i.e. best) CLIP similarity as the anchor view</u>.</p><p>选择分数最高的视角作为锚点视角.</p><p>Often there are <u>multiple high-scoring views around the object</u>, and using any of them as the anchor will produce an effective and meaningful stylization.</p><p>通常有很多个高分视角, 从中任意选取一个即可.</p><p>We <u>render multiple views of the object from randomly sampled views using a Gaussian distribution centered around the anchor view (with <span class="math inline">\(\sigma=\pi / 4\)</span>)</u>.</p><p>在获取生成mesh的渲染图片时, 以锚点为均值, <span class="math inline">\(\pi / 4\)</span>为方差, 计算高斯分布, 随机采样多个视角进行渲染.</p><p>We <u>average over the CLIP-embedded views prior</u> to feeding them into our loss, which encourages the network to leverage view consistency.</p><p>将这些视角的图片嵌入到CLIP空间中并求取平均值, 然后放入损失函数中, 这样有助于网络利用视角的一致性.</p><p>For all our experiments, <u><span class="math inline">\(n_{\theta}=5\)</span> (number of sampled views)</u>.</p><p>对于所有的实验, 采样的视角数设置为<span class="math inline">\(n_{\theta}=5\)</span>.</p><p>The 2D augmentations generated using <span class="math inline">\(\psi_{\text{global}}\)</span> and <span class="math inline">\(\psi_{\text{local}}\)</span> are critical for our method to <u>avoid degenerate solutions</u>.</p><p>由<span class="math inline">\(\psi_{\text{global}}\)</span>和<span class="math inline">\(\psi_{\text{local}}\)</span>定义的二维增强方法对于防止退化的情况至关重要.</p><p><span class="math inline">\(\psi_{\text{global}}\)</span> involves a random perspective transformation and <span class="math inline">\(\psi_{\text{local}}\)</span> generates both a random perspective and a random crop that is <span class="math inline">\(10\%\)</span> of the original image.</p><p><span class="math inline">\(\psi_{\text{global}}\)</span>包括一个随机的透视变换. <span class="math inline">\(\psi_{\text{local}}\)</span>包括一个随机的透视变换和一个随机裁剪原图的<span class="math inline">\(10\%\)</span>的子图.</p><p>Cropping allows the network to <u>focus on localized regions</u> when making ﬁne grained adjustments to the surface geometry and color.</p><p>裁剪有助于帮助网络获取局部的细节用于优化表面的几何结构和颜色.</p><h3 id="experiments部分">Experiments部分</h3><p>We consider a variety of sources including: <u>COSEG, Thingi10K, Shapenet, Turbo Squid, and ModelNet</u>.</p><p>作者考虑了一批不同来源的数据: COSEG、Thingi10K、Shapenet、Turbo Squid和ModelNet.</p><p>Our method requires <u>no particular quality constraints or preprocessing of inputs</u>, and the breadth of shapes we stylize in this paper and in our project webpage illustrates its ability to handle low-quality meshes.</p><p>作者提出的方法对于输入mesh没有任何限制, 因此即使是低质量的mesh也可以被处理.</p><p>Our method takes <u>less than 25 minutes to train on a single GPU</u>, and <u>high quality results usually appear in less than 10 minutes</u>.</p><p>作者的方法只需要不到25分钟即可在单张GPU上训练好, 预测高质量的结果也只需要不到10分钟.</p><h4 id="neural-stylization-and-controls部分">Neural Stylization and Controls部分</h4><h5 id="fine-grained-controls">Fine Grained Controls</h5><p>Our network leverages a <u>positional encoding where the range of frequencies can be directly controlled by the standard deviation <span class="math inline">\(\sigma\)</span> of the <span class="math inline">\(\mathbf{B}\)</span> matrix</u>.</p><p>作者使用了位置编码, 因此纹理频率可以直接由<span class="math inline">\(\mathbf{B}\)</span>矩阵的标准差<span class="math inline">\(\sigma\)</span>控制.</p><p>In following figure, we show the results of <u>three different frequency values</u> when stylizing a source mesh of a torus towards the target text 'stained glass donut'.</p><p>下图展示了不同频率值对于相同的输入的影响, 文本输入是"彩色玻璃甜甜圈".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062111336.png" /></p><p>Increasing the frequency value <u>increases the frequency of style details</u> on the mesh and <u>produces sharper and more frequent displacements along the normal direction</u>.</p><p>增大频率值增加了纹理的出现频率也让mesh表面更加锐利, 沿着法线方向的偏移出现地也越频繁.</p><p>We further demonstrate our method’s ability to successfully <u>synthesize styles of varying levels of speciﬁcity</u>.</p><p>作者提出的方法还能够生成不同层级地风格.</p><p><u>Increasing the target text prompt granularity for a source mesh of a lamp and iron.</u> Top row targets: (a). 'Lamp', (b).'Luxo lamp', (c).'Blue steel luxo lamp', (d).'Blue steel luxo lamp with corrugated metal'. Bottom row targets: (a).'Clothes iron', (b).'Clothes iron made of crochet', (c).'Golden clothes iron made of crochet', (d).'Shiny golden clothes iron made of crochet'.</p><p>增加灯和熨斗的目标文本的层级。第一行: (a)."灯", (b)."Luxo灯", (c)."蓝色钢制Luxo灯", (d)."带波纹金属的蓝色钢制Luxo灯". 最后一行：(a)."衣服熨斗", (b)."用钩针制成的熨斗", (c)."用钩针制成的金熨斗", (d)."用钩针制成的闪亮的金色衣服的熨斗".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062113223.png" /></p><p>Though the primary mode of style control is through the <u>text prompt</u>, we explore the way the network adapts to <u>the geometry of the source shape</u>.</p><p>虽然样式控制主要是通过文本进行的, 但输入的mesh形状也有影响.</p><p>In following figure, the target text prompt is ﬁxed to 'cactus'.</p><p>在下图中, 文本描述被限定为"仙人掌".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062115512.png" /></p><p>We consider different input source spheres with <u>increasing protrusion frequency</u>.</p><p>突起频率的增加的不同的球形mesh进行输入.</p><p>Observe that both the frequency and structure of the generated style changes to <u>align with the pre-existing structure of the input surface</u>.</p><p>这些不同球形mesh的输出各不相同, 且输出的突起频率与输入类似.</p><p>This shows that our method has the ability to <u>preserve the content of the input mesh without compromising the quality of the stylization</u>.</p><p>这表明作者的方法能够保留输入mesh的结构, 而不会影响样式化的质量.</p><p>Meshes with corresponding connectivity can be used to <u>morph between two surfaces</u>.</p><p>具有相应连接性的网格可用于在两个曲面之间变形.</p><p>Thus, <u>our ability to modify style while preserving the input mesh enables morphing</u>.</p><p>因此, 作者提出的方法能够在保留输入mesh结构的同时修改样式, 从而可以实现变形.</p><p>To morph between meshes, we apply <u>linear interpolation between the style value</u> (RGB and displacement) of every point on the mesh, for each instance of the stylized mesh.</p><p>在两个不同的风格的mesh之间使用线性插值的方式(对每个点的颜色和偏移量进行插值)获得两个不同风格之间的渐变效果.</p><p>Morphing between two different stylizations (geometry and color). Left:'wooden chair', right:'colorful crochet chair'.</p><p>两种不同样式(几何结构和颜色)之间的变形.左: "木椅", 右: "彩色钩针椅".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062118366.png" /></p><h4 id="text2mesh-priors部分">Text2Mesh Priors部分</h4><p>Our method incorporates a number of priors that allow us to perform stylization <u>without a pre-trained GAN</u>.</p><p>作者提出的方法使用了一系列先验, 因此不需要使用预训练GAN.</p><p><u>Ablation on the priors used in our method (full) for a candle mesh and target 'Candle made of bark'</u>: w/o our style ﬁeld network (−net), w/o 2D augmentations (−aug), w/o positional encoding (−FFN), w/o crop augmentations for <span class="math inline">\(\psi_{\text{local}}\)</span> (−crop), w/o the geometry-only component of <span class="math inline">\(L_{\text{sim}}\)</span> (−displ), and learning over a 2D plane in 3D space (−3D). We show the CLIP score (<span class="math inline">\(\text{sim}(\hat{S}^{\text{full}}, \phi_{\text{target}})\)</span>).</p><p>先验信息的消融实验结果, 输入时一个蜡烛的mesh和风格文本"树皮做的蜡烛": 没有NSF网络(-net)、没有二维图像增强(-aug)、没有位置编码(-FFN)、没有裁剪的<span class="math inline">\(\psi_{\text{local}}\)</span>(-crop)、没有几何结构损失函数<span class="math inline">\(L_{\text{sim}}\)</span>(-displ)和只学习二维平面(-3D). 通过<span class="math inline">\(\text{sim}(\hat{S}^{\text{full}}, \phi_{\text{target}})\)</span>计算的CLIP分数在图中最下面展示.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062043618.png" /></p><p>Removing the style ﬁeld network (−net), and instead directly optimizing the vertex colors and displacements, results in <u>noisy and arbitrary displacements over the surface</u>.</p><p>移除NSF网络会导致表面随机且充满噪音的出现偏移.</p><p><u>Random 2D augmentations are necessary</u> to generate meaningful CLIP-guided drawings.</p><p>随机的二维增强对于利用CLIP来说时十分重要的.</p><p>We observe the same phenomena in our method, whereby removing 2D augmentations results in <u>a stylization completely unrelated to the target text prompt</u>.</p><p>在没有使用图像增强的情况下, 风格化的结果与目标文本描述相差甚远.</p><p>Without fourier feature encoding (−FFN), the generated style <u>loses all ﬁne-grained details</u>.</p><p>不使用位置编码会导致所有的细节缺失.</p><p>With the cropping augmentation removed (−crop), the output is similarly <u>unable to synthesize the ﬁne-grained style details</u> that deﬁne the target.</p><p>不进行局部裁剪会导致无法生成满足目标文本描述的细节.</p><p>Removing the geometry-only component of <span class="math inline">\(L_{\text{sim}}\)</span> (−displ) <u>hinders geometric reﬁnement</u>, and the network instead compensates by simulating geometry through shading.</p><p>不使用几何损失函数导致几何结构的优化出现问题, 网络倾向于使用阴影来模拟几何结构.</p><p>Without a geometric prior (−3D) there is no source mesh to impose global structure, thus, <u>the 2D plane in 3D space is treated as an image canvas</u>.</p><p>在没有三维输入的时候, 三维空间的一个二维平面被视为一张图像的画布.</p><p>Our method obtains the <u>highest score</u> across different ablations.</p><p>在消融实验中, 作者提出的方法获得了最高的分数.</p><p>Ideally, there is <u>a correlation between visual quality and CLIP scores</u>.</p><p>在理想情况下, 视觉效果和CLIP分数存在一定的相关性.</p><p>However, <u>-3D manages to achieve a high CLIP similarity</u>, despite its zero regard for global content semantics.</p><p>然而, 在没有三维输入的情况下, 仍然能够获得很高的CLIP相似度分数.</p><p>This shows an example of <u>how CLIP may naively prefer degenerate solutions</u>, while our geometric prior steers our method away from these solutions.</p><p>这从侧面展示了CLIP倾向于退化的结果, 但是作者提出的先验有效避免了这个.</p><h5 id="interplay-of-geometry-and-color">Interplay of Geometry and Color</h5><p>Our method utilizes the <u>interplay between geometry and color for effective stylization</u>.</p><p>作者提出的方法有效利用了颜色和几何特征.</p><p>Interplay between geometry and color for stylization. <em>Full</em> - our method, <em>Color</em> - only color changes, and <em>Geometry</em> - only geometric changes. We also display the CLIP similarity.</p><p>下图展示了几何结构和颜色在风格转换中的作用. <em>Full</em>代表全部使用, <em>Color</em>代表只使用颜色变化, <em>Geometry</em>代表只使用几何变化. CLIP相似度分数也在最下面展示.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062050060.png" /></p><p>Learning to predict only geometric manipulations produces inferior geometry compared to learning geometry and color together, as the network <u>attempts to simulate shading by generating displacements for self shadowing</u>.</p><p>只是用几何特征相比于使用颜色和几何特征表现更差, 因为在只使用几何特征的情况下, 网络倾向于生成几何纹理来模拟阴影.</p><p>Similarly learning to predict only color results in the network <u>attempting to hallucinate geometric detail through shading</u>, leading to a ﬂat and unrealistic texture that nonetheless is capable of achieving a relatively high CLIP score when projected to 2D.</p><p>只是用颜色特征也表现不好, 网络会错误的将阴影认为时几何特征, 从而导致几何结构的缺失. 因为在光滑的表面进行投影能够获取更高的CLIP分数.</p><h4 id="stylization-fidelity部分">Stylization Fidelity部分</h4><p>Our method performs the task of <u>general text-driven stylization of meshes</u>.</p><p>作者提出的方法是基于文本的mesh风格化.</p><p>Given that no approaches exist for this task, we evaluate our method’s performance by <u>extending VQGAN-CLIP</u>.</p><p>由于没有其他的现存算法针对这个任务, 作者拓展了VQGAN-CLIP算法作为基准.</p><p>This baseline <u>synthesizes color inside a binary 2D mask projected from the 3D source shape</u> (without 3D deformations) guided by CLIP.</p><p>这个基准通过CLIP指导合成从三维物体投影的二维二值模板内部的颜色.</p><p>Further, the baseline is <u>initialized with a rendered view of the 3D source</u>.</p><p>这个基准需要使用三维物体的一个视角的渲染图片来初始化.</p><p>We conduct a user study to <u>evaluate the perceived quality of the generated outputs, the degree to which they preserve the source content, and how well they match the target style</u>.</p><p>作者进行了一次用户调研来评估生成mesh的质量、生成mesh在多大程度上保持了源物体的结构以及生成的mesh在多大程度上满足了格式化的要求.</p><p>We had <u>57 users evaluate 8 random source meshes and style text prompt combinations</u>.</p><p>一共有57个用户在8组不同的结果上进行测试.</p><p>For each combination, we <u>display the target text and the stylized output in pairs</u>.</p><p>作者在调研的时候成对地放置风格文本和输出地mesh.</p><p>The users are then asked to assign a score (1-5) to three factors:</p><ul><li>(Q1) "How natural is the output depiction of {content} + {style}?"</li><li>(Q2) "How well does the output match the original {content}?"</li><li>(Q3) "How well does the output match the target {style}?"</li></ul><p>用户使用分数1到5回答下列问题:</p><ul><li>输出在多大程度上自然地描述了{输入点云}+{风格文本}?</li><li>输出在多大程度上与原始{输入点云}保持一致?</li><li>输出在多大程度上与{风格文本}地描述保持一致?</li></ul><p>We report the mean opinion <u>scores with standard deviations</u> in parentheses for each factor averaged across all style outputs for our method and the baseline in following table.</p><p>下表显示了用户调研地结果, 作者地方法和基线地方法在每个问题上地分数通过均值(标准差)的形式表示.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062019507.png" /></p><p>We include <u>three control questions where the images and target text do not match</u>, and obtain a mean control score of <span class="math inline">\(1.16\)</span>.</p><p>作者设置了控制问题(图像和目标文本完全不匹配的例子), 这些控制问题的得分是<span class="math inline">\(1.16\)</span>.</p><p>Our method <u><em>outperforms the VQGAN baseline across all questions</em></u>, with a difference of <span class="math inline">\(1.07\)</span>, <span class="math inline">\(0.44\)</span>, and <span class="math inline">\(1.32\)</span> for Q1-Q3, respectively.</p><p>作者提出的方法在Q1-Q3都超过了基准方法, 超过的分数依次是<span class="math inline">\(1.07\)</span>、<span class="math inline">\(0.44\)</span>和<span class="math inline">\(1.32\)</span>.</p><p>Though VQGAN is somewhat <u>effective at representing the natural content</u> in our prompts, perhaps due to the <u>implicit content signal it receives from the mask</u>, it struggles to synthesize these representations with style in a meaningful way.</p><p>VQGAN在表现自然内容方面是有效的, 但是由于其从模板接受隐式内容信号, 导致其无法有效的合成相关的表达.</p><h4 id="beyond-textual-stylization部分">Beyond Textual Stylization部分</h4><p>Beyond text-based stylization, our method can be used to <u>stylize a mesh toward different target modalities</u> such as a 2D image or even a 3D object.</p><p>除了使用文本进行格式化, 作者提出的框架还能够使用不同的模态进行格式化, 例如图像甚至三维物体.</p><p>For a target 2D image <span class="math inline">\(I_t\)</span>, <u><span class="math inline">\(\phi_{\text{target}}\)</span> represents the image-based CLIP embedding of <span class="math inline">\(I_t\)</span></u>.</p><p>对于图像<span class="math inline">\(I_t\)</span>, <span class="math inline">\(\phi_{\text{target}}\)</span>代表将图像<span class="math inline">\(I_t\)</span>嵌入CLIP空间.</p><p>Stylization <u>driven by an image target</u>.</p><p>基于图像的格式化.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062012132.png" /></p><p>For a target mesh <span class="math inline">\(T\)</span>, <u><span class="math inline">\(\phi_{\text{target}}\)</span> is the average embedding, in CLIP space, of the 2D renderings of <span class="math inline">\(T\)</span></u>, where the views are the same as those sampled for the source mesh.</p><p>对于<span class="math inline">\(T\)</span>代表的mesh, <span class="math inline">\(\phi_{\text{target}}\)</span>代表了将<span class="math inline">\(T\)</span>按照与输入mesh一致的视角进行二维渲染得到的图片嵌入CLIP空间的均值.</p><p>Beyond different modalities, we can <u>combine targets across different modalities by simply summing <span class="math inline">\(\mathcal{L}_{\text {sim}}\)</span> over each target</u>.</p><p>除了不同的模态, 作者认为还可以融合多个模态, 通过简单的叠加每个模态的<span class="math inline">\(\mathcal{L}_{\text {sim}}\)</span>.</p><p>Neural stylization <u>driven by mesh targets</u>. (a) &amp; (c) are styled using Targets 1 &amp; 2, respectively. (b) &amp; (d) are styled with text in addition to the mesh targets: (b) 'a cactus that looks like a cow', (d) 'a mouse that looks like a duck'.</p><p>基于mesh的格式化. (a)和(c)分别使用Target 1和Target 2进行格式化. (b) &amp; (d)在分别使用Target 1和Target 2进行格式化的同时还使用文本进行格式化: (b)"像奶牛一样的仙人掌", (d)"像鸭子一样的老鼠".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062013015.png" /></p><h4 id="incorporating-symmetries部分">Incorporating Symmetries部分</h4><p>We can make use of prior knowledge of the <u>input shape symmetry</u> to <u>enforce style consistency across the axis of symmetry</u>.</p><p>作者提出的框架可以利用输入形状的对称性作为先验来强制保证对称轴对称部分的一致性.</p><p>Such symmetries can be introduced into our model by <u>modifying the input to our positional encoding</u>.</p><p>这种对称性可以通过修改位置编码的方式实现.</p><p>For instance, <u>given a point <span class="math inline">\(p = (x, y, z)\)</span> and a shape with bilateral symmetry across the <span class="math inline">\(X-Y\)</span> plane</u>, one can <u>apply a function prior to the the positional encoding such that <span class="math inline">\(\gamma (x, y, \abs{z})\)</span></u>.</p><p>例如, 对于沿着<span class="math inline">\(X-Y\)</span>平面对称的物体的任意一点<span class="math inline">\(p = (x, y, z)\)</span>, 可以修改位置编码为<span class="math inline">\(\gamma (x, y, \abs{z})\)</span>.</p><p>Effect of the <u>symmetry prior on a UFO mesh input</u> with text prompt: 'colorful UFO'.</p><p>下图是对一个UFO的mesh有无使用对称性先验的生成结果, 输入的文本是"colorful UFO".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061951971.png" /></p><p>This prior is <u>effective even when the triangulation is not perfectly symmetrical</u>, since the function is applied in Euclidean space.</p><p>由于这种对称先验使用的函数是在欧几里得空间中进行操作, 这个先验对于那些不是严格对称的物体仍然有效.</p><p>A full investigation into <u>incorporating additional symmetries within positional encoding</u> is an interesting direction for future work.</p><p>将对称性融入位置编码的更完善的研究是未来的一个有意思的研究方向.</p><h4 id="limitations部分">Limitations部分</h4><p>Our method implicitly <u>assumes there exists a synergy between the input 3D geometry and the target style prompt</u>.</p><p>作者提出的方法隐式的假设输入的三维物体和目标风格之间存在一定的联系.</p><p>If the target style is <u>unrelated to the 3D mesh content</u>, the stylization may <u>ignore the 3D content</u>. Results are improved when including the content in the target text prompt.</p><p>当目标风格与三维物体无关的时候, 作者提出的方法会忽略三维物体的形状. 当风格中包含三维物体的描述生成的效果会更好.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061907420.png" /></p><p>Therefore, <u>in order to preserve the original content</u> when editing towards a mismatched target prompt, we <u>simply include the object category in the text prompt</u> (e.g., stained glass dragon) which <u>adds a content preservation constraint into the target</u>.</p><p>因此, 为了保护原有的三维形状, 作者简单的在风格文本中加入三维物体的描述, 这有效的保护了三维物体的形状.</p><h3 id="supplement部分">Supplement部分</h3><h4 id="additional-results部分">Additional Results部分</h4><p>Please refer to <u>our project webpage</u> additional results.</p><p>作者的项目网站上有更多的生成结果.</p><h4 id="high-resolution-stylization部分">High Resolution Stylization部分</h4><p>Our method is effective even on coarse inputs, and one can always <u>increase the resolution of a mesh <span class="math inline">\(M\)</span> to learn a neural ﬁeld with ﬁner granularity</u>.</p><p>作者提出的方法在粗糙的输入有很好的表现, 作者认为还可以使用插入顶点的方式增加<span class="math inline">\(M\)</span>对应的三维mesh的分辨率.</p><p>In following figure, we <u>upsample the mesh by inserting a degree-3 vertex in the barycenter of each triangle face of the mesh</u>.</p><p>在如下的图片中, 作者上采样输入的mesh, 通过在三角面片的中心位置插入入度为3的顶点来提升分辨率.</p><p>Style results over a coarse torus (left) and the same mesh with each triangle barycenter inserted as an additional vertex (right). Prompt: 'a donut made of cactus'.</p><p>左边是粗糙的mesh输入, 右边是左边的mesh经过上采样之后的高分辨率mesh输入. 输入的文本是"仙人掌制作的甜甜圈".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061747482.png" /></p><p>The network is <u>able to synthesize a ﬁner style by leveraging these additional vertices</u>.</p><p>网络可以利用这些新插入的点合成更好的结果.</p><p>作者使用的上采样过程如下图所示.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061750755.png" /></p><h4 id="choice-of-anchor-view部分">Choice of anchor view部分</h4><p>As mentioned in the main text, we select <u>the view with the highest (i.e. best) CLIP similarity to the content as the anchor</u>.</p><p>作者选取CLIP相似度最高的视角作为锚点视角.</p><p>There are often <u>many possible views that can be chosen as the anchor</u> that will allow a high-quality stylization.</p><p>通常可以作为锚点的视角不止一个.</p><p>找锚点视角的过程: 首先获取三维mesh每个点的法线方向渲染的二维图片, 然后将每张图片嵌入到CLIP空间中, 之后将这个三维mesh物体的文本名称嵌入到CLIP空间中, 比较这两个嵌入形式可以得到一个相似度分数, 选取相似度分数最高的几个视角之一作为锚点视角即可.</p><p>The CLIP score exhibits a strong positive correlation with views that are semantically meaningful, and thus can be used for automatic anchor view selection, as described in the main paper.</p><p>CLIP相似度分数通常与视角图片的语义信息是否有意义正相关.</p><p>This metric is <u>limited in expressiveness</u>, however, as <u>demonstrated by the constrained range</u> that the scores fall within for all the views around the mesh.</p><p>这种方式在表达能力上有所限制, 因为锚点视角只能看到物体的一个范围, 在所有锚点视角上相似度分数较高, 如果考虑其他视角的话, 相似度分数会下降.</p><p><span class="math inline">\(n_{\theta}\)</span>, <u>the number of sampled views</u>, is set to <span class="math inline">\(5\)</span>.</p><p>采样的视角数量<span class="math inline">\(n_{\theta}\)</span>被设置为<span class="math inline">\(5\)</span>.</p><p>We show in following figure that <u>increasing the number of views beyond 5 does little to change the quality of the output stylization</u>. Prompt: 'A horse made of cactus'.</p><p>下图表明增加采样的视角数量对最终生成的结果影响不大. 输入的文本是"仙人掌制作的马".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061739346.png" /></p><h4 id="training-and-implementation-details部分">Training and Implementation Details部分</h4><h5 id="network-architecture部分">Network Architecture部分</h5><p>We <u>map a vertex <span class="math inline">\(p \in \mathbb{R}^{3}\)</span> to a 256-dimensional Fourier feature</u>.</p><p>作者将一个顶点<span class="math inline">\(p \in \mathbb{R}^{3}\)</span>映射到一个256-dimensional的傅里叶特征.</p><p><u>Typically <span class="math inline">\(5.0\)</span> is used as the standard deviation for the entries of the Gaussian matrix <span class="math inline">\(\mathbf{B}\)</span></u>, although this can be set to the preference of the user.</p><p>作者设置高斯矩阵<span class="math inline">\(\mathbf{B}\)</span>中的标准差为<span class="math inline">\(5.0\)</span>, 也可以设置为其他值.</p><p>The <u>shared MLP layers <span class="math inline">\(N_s\)</span> consist of 4 256-dimensional linear layers with ReLU activation</u>.</p><p>MLP层<span class="math inline">\(N_s\)</span>包含4个256维的带有ReLU激活函数的线性层.</p><p>The branched layers, <span class="math inline">\(N_d\)</span> and <span class="math inline">\(N_c\)</span>, each consist of two 256-dimensional linear layers with ReLU activation.</p><p><span class="math inline">\(N_d\)</span>和<span class="math inline">\(N_c\)</span>这两个分支层各包含2个256维的带有ReLU激活函数的线性层.</p><p>After the ﬁnal linear layer, a tanh activation is applied to each branch.</p><p>在最后一层, tanh激活函数会应用在每个分支.</p><p>The <u>weights of the ﬁnal linear layer of each branch are initialized to zer</u>o so that the original content mesh is unaltered at initialization.</p><p>所有的权重初始化都为0以保证初始时原始的mesh没有被改变.</p><p>We <u>divide the output of <span class="math inline">\(N_c\)</span> by <span class="math inline">\(2\)</span> and add it to <span class="math inline">\([0.5, 0.5, 0.5]\)</span></u>.</p><p>作者将<span class="math inline">\(N_c\)</span>经过tanh激活层的输出除<span class="math inline">\(2\)</span>并加上<span class="math inline">\([0.5, 0.5, 0.5]\)</span>.</p><p>This <u>enforces the final color prediction <span class="math inline">\(c_p\)</span> to be in range <span class="math inline">\((0.0, 1.0)\)</span></u>.</p><p>这个操作保证最终的颜色预测值<span class="math inline">\(c_p\)</span>在范围<span class="math inline">\((0.0, 1.0)\)</span>内.</p><p>We ﬁnd that <u>initializing the mesh color to <span class="math inline">\([0.5, 0.5, 0.5]\)</span> (grey) and adding the network output as a residual helps prevent undesirable solutions in the early iterations of training</u>.</p><p>作者法线初始化mesh的颜色为<span class="math inline">\([0.5, 0.5, 0.5]\)</span>并且以残差的形式连接网络输出和输入的mesh可以防止在早期训练中出现不想要的情况.</p><p>For the branch <span class="math inline">\(N_d\)</span>, we <u>multiply the ﬁnal tanh layer by <span class="math inline">\(0.1\)</span> to get displacements in the range <span class="math inline">\((−0.1, 0.1)\)</span></u>.</p><p>对于<span class="math inline">\(N_d\)</span>层的输出, 作者在tanh激活层之后将其输出乘上<span class="math inline">\(0.1\)</span>来保证偏移距离在<span class="math inline">\((−0.1, 0.1)\)</span>内.</p><h5 id="training部分">Training部分</h5><p>We use the <u>Adam optimizer with an initial learning rate of <span class="math inline">\(5e−4\)</span></u>, and <u>decay the learning rate by a factor of <span class="math inline">\(0.9\)</span> every <span class="math inline">\(100\)</span> iterations</u>.</p><p>作者使用Adam优化器, 初始学习率是<span class="math inline">\(5e−4\)</span>, 每<span class="math inline">\(100\)</span>次迭代学习率乘上系数<span class="math inline">\(0.9\)</span>.</p><p>We train for <span class="math inline">\(1500\)</span> iterations on a single Nvidia GeForce RTX2080Ti GPU, which <u>takes around <span class="math inline">\(25\)</span> minutes to complete</u>.</p><p>作者在单张Nvidia GeForce RTX2080Ti GPU上迭代训练<span class="math inline">\(1500\)</span>次, 大约需要<span class="math inline">\(25\)</span>分钟完成训练.</p><p>For augmentations <span class="math inline">\(\Psi_{\text{global}}\)</span>, we use <u>a random perspective transformation</u>.</p><p>对于图像增强<span class="math inline">\(\Psi_{\text{global}}\)</span>, 作者使用一个随机透视变换.</p><p>For <span class="math inline">\(\Psi_{\text{local}}\)</span>, we randomly crop the image to <span class="math inline">\(10\%\)</span> of its original size and then apply a random perspective transformation.</p><p>对于图像增强<span class="math inline">\(\Psi_{\text{local}}\)</span>, 作者随机裁剪图像到原始尺寸的<span class="math inline">\(10\%\)</span>然后使用一个随机透视变换.</p><p>Before encoding images with CLIP, we normalize per-channel by mean <span class="math inline">\((0.48145466, 0.4578275, 0.40821073)\)</span> and standard deviation <span class="math inline">\((0.26862954, 0.26130258, 0.27577711)\)</span>.</p><p>在使用CLIP编码图片之前, 作者正则化图像的三个通道, 使之均值为<span class="math inline">\((0.48145466, 0.4578275, 0.40821073)\)</span>, 标准差为<span class="math inline">\((0.26862954, 0.26130258, 0.27577711)\)</span>. [这是一组常用的正则化参数.]</p><h4 id="baseline-comparison-and-user-study部分">Baseline Comparison and User Study部分</h4><h4 id="societal-impact部分">Societal Impact部分</h4><p>Our framework utilizes a pre-trained CLIP embedding space, <u>which has been shown to contain bias</u>.</p><p>作者提出的架构利用了预训练的CLIP嵌入空间, 而CLIP嵌入空间已经被证明存在偏置.</p><p>Since our system is capable of synthesizing a style driven by a target text prompt, <u>it enables visualizing such biases in a direct and transparent way</u>.</p><p>由于作者提出的系统能够合成符合文本描述的风格特征, 此系统可以直接透明的将这个偏置可视化出来.</p><p>For example, <u>the nurse style in following figure is biased towards adding female features to the input male shape</u>. Given a human male input, and target prompt: ‘a nurse’, we observe a gender bias in CLIP to favor female shapes.</p><p>例如, 给定一个男性mesh, 限定文字输入为护士, 我们可以观察到输出的mesh在男性mesh上增加了许多女性特征, 这表示CLIP存在偏差, CLIP在护士这一类别上更偏好女性.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061653413.png" /></p><p>An important future work may leverage our proposed system in <u>helping create a datasheet for CLIP in addition to future image-text embedding models</u>.</p><p>作者设计的系统也可以帮助未来准备使用CLIP的图像文本嵌入模型制作数据集.</p><h3 id="精读总结">精读总结</h3><blockquote><p>精读目标及效果自测：所读段落是否详细掌握，回答三个的终极问题(你是谁 - 论文提出/采用什么方法、细节是什么；从哪里来 - 论文要解决什么问题/任务、其启发点或借鉴之处在哪；到哪里去 - 论文方法达到什么效果)。</p></blockquote><p>论文构建了一个基于文本的三维物体风格转换器.</p><p>论文使用残差的方式解耦三维物体和风格参数.</p><p>论文将风格参数划分为表面顶点的颜色和沿法线方向的位置偏移, 分别进行回归计算.</p><p>论文采用了多种先验信息(位置编码、图像增强、几何结构等等)构建损失函数, 优化网络权重, 避免获得退化结果.</p><p>论文使用CLIP进行多模态融合, 实现了多模态定义的风格变换.</p><p>论文设计的风格变换架构能够生成质量更好的不同风格的三维物体.</p><h2 id="总结">总结</h2><blockquote><p>总结：总览全文，归纳总结，总结文中创新点，关键点，启发点（论文idea的来源或者说是论文的motivation）等重要信息。</p></blockquote><p>论文的创新点、关键点:</p><ul><li>用残差的方式解耦三维物体和风格参数;</li><li>将风格参数划分为表面顶点的颜色和沿法线方向的位置偏移, 分别进行回归计算;</li><li>采用了多种先验信息(位置编码、图像增强、几何结构等等)构建损失函数, 优化网络权重, 避免获得退化结果;</li><li>使用CLIP进行多模态融合, 实现了多模态定义的风格变换;</li><li>设计的风格变换架构能够生成质量更好的不同风格的三维物体.</li></ul><p>论文的启发点:</p><ul><li>CLIP的多模态融合;</li><li>残差解耦三维物体和风格参数;</li><li>多重先验信息防止网络退化.</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection阅读笔记</title>
    <link href="/2022/01/02/FCAF3D-Fully-Convolutional-Anchor-Free-3D-Object-Detection%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/01/02/FCAF3D-Fully-Convolutional-Anchor-Free-3D-Object-Detection%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="fcaf3d-fully-convolutional-anchor-free-3d-object-detection阅读笔记">FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection阅读笔记</h1><blockquote><p>读论文三步曲：泛读，精读，总结。</p></blockquote><h2 id="泛读">泛读</h2><blockquote><p>泛读：快速浏览，把握概要。重点读标题、摘要、结论、所有小标题。</p></blockquote><h3 id="title部分">Title部分</h3><p>FCAF3D: <u>Fully Convolutional</u> <strong>Anchor-Free</strong> <em>3D Object Detection</em></p><ul><li>任务: 3D Object Detection</li><li>方法: Fully Convolution</li><li>特点: Anchor-Free</li></ul><h4 id="什么是anchor">什么是Anchor?</h4><blockquote><p><em>参考资料: <a href="https://zhuanlan.zhihu.com/p/55824651">目标检测中的Anchor</a></em></p></blockquote><p>在目标检测中, Anchor指锚点, Anchor Box指锚框.</p><p>目标检测需要解决<strong>在哪里有什么</strong>的问题, 具体来说, 检测目标的类别、数量、位置、尺度都是不确定的.</p><p>传统非深度学习方法和早期深度学习方法都要结合<strong>金字塔多尺度</strong>和<strong>遍历滑窗</strong>的方式, 逐尺度逐位置判断<strong>这个尺度的这个位置处有没有认识的目标</strong>, 非常耗时.</p><p>近期顶尖(SOTA)的目标检测方法几乎都用了anchor技术. 首先预设一组不同尺度不同位置的固定参考框, 覆盖几乎所有位置和尺度, 每个参考框负责检测与其交并比大于阈值(训练预设值，常用0.5或0.7)的目标，anchor技术将问题转换为<strong>这个固定参考框中有没有认识的目标、目标框偏离参考框多远</strong>, 不再需要多尺度遍历滑窗, 真正实现了又好又快。</p><p>使用Anchor技术的算法称为Anchor-based算法, 不使用Anchor技术的算法称为Anchor-free的算法.</p><p>Anchor-based的算法一般需要先在训练集上统计一组不同尺寸检测框的集合, 这个集合代表目标框主要分布的尺度, 在推理生成的特征图上使用这一组框滑动判断框内有无目标得到候选框, 最后聚合所有的候选框得到最终的检测框结果.</p><h3 id="abstract部分">Abstract部分</h3><p>FCAF3D - a ﬁrst-in-class fully convolutional anchor-free <u>indoor</u> 3D object detection method.</p><p>细化使用场景为室内.</p><p>It is a simple yet effective method that uses a <u>voxel representation</u> of a point cloud and processes voxels with <u>sparse convolutions</u>.</p><p>使用点云的体素化表示方法并使用稀疏卷积对体素进行操作.</p><p>Existing 3D object detection methods make <u>prior assumptions on the geometry of objects</u>, and we argue that it <u>limits their generalization ability</u>.</p><p>作者认为目前的三维检测方法对物体的几何尺寸有预先假设, 这限制了模型的生成能力.</p><p>To get rid of any prior assumptions, we propose <u>a novel parametrization of oriented bounding boxes</u>.</p><p>为了不依赖任何先验假设, 作者提出了一种有向边界框的参数化方法.</p><p>The proposed method achieves state-of-the-art 3D object detection results in terms of <span class="math inline">\(mAP@0.5\)</span> on ScanNet V2 (+4.5), SUN RGB-D (+3.5), and S3DIS (+20.5) datasets.</p><p>作者提出的方法在三大室内数据集(ScanNet V2、SUN RGB-D、S3DIS)上均达到了SOTA.</p><p>The code and models are available at <a href="https://github.com/samsunglabs/fcaf3d">https://github.com/samsunglabs/fcaf3d</a>.</p><p>代码在<a href="https://github.com/samsunglabs/fcaf3d">GitHub</a>.</p><h3 id="conclusion部分">Conclusion部分</h3><p>The proposed method signiﬁcantly outperforms the previous state-of-the-art on the challenging indoor SUN RGB-D, ScanNet, and S3DIS benchmarks in terms of both <u>mAP</u> and <u>inference speed</u>.</p><p>作者提出的方法在mAP和预测速度这两个指标上都超过之前的算法.</p><p>We have also proposed a novel oriented bounding box parametrization and shown that it improves detection accuracy for <u>several existing 3D object detection methods</u>.</p><p>作者提出的新的向边界框的参数化方法能够提升已有算法的准确性.</p><p>Moreover, the proposed parametrization allows avoiding any prior assumptions about objects, thus <u>reducing the number of hyperparameters</u>.</p><p>作者提出的新的有向边界框的参数化方法能够减少超参数的数量, 因为其避免使用物体框的先验信息.</p><h3 id="小标题分析">小标题分析</h3><ul><li>Introduction <em>[简介]</em></li><li>Related Work <em>[相关工作]</em></li><li>Proposed Method <em>[提出的方法]</em><ul><li>Sparse Neural Network <em>[<u>FCAF3D结构</u>]</em></li><li>Bounding Box Parametrization <em>[<u>新的有向边界框的参数化方法</u>]</em></li></ul></li><li>Experiments <em>[实验]</em><ul><li>Datasets <em>[数据集]</em></li><li>Implementation Details <em>[实现细节]</em></li></ul></li><li>Results <em>[实验结果]</em><ul><li>Comparison with State-of-the-art Methods <em>[与其他SOTA的比较]</em></li><li>Object Geometry Priors <em>[<u>物体结构先验</u>]</em></li><li>Ablation Study <em>[<u>消融实验</u>]</em></li><li>Inference Speed <em>[预测速度]</em></li></ul></li><li>Conclusion <em>[结论]</em></li><li>Supplement <em>[附加材料]</em><ul><li>Additional Comments on Mobius Parametrization <em>[<u>新的有向边界框的参数化方法的附加说明</u>]</em></li><li>Per-category results <em>[每个类别的实验结果]</em></li><li>Visualization <em>[可视化结果]</em></li></ul></li></ul><h3 id="泛读总结">泛读总结</h3><blockquote><p>泛读目标及效果自测：1.论文要解决什么问题? 2.论文采用了什么方法? 3.论文达到什么效果?</p></blockquote><p>论文要解决什么问题? - 室内场景的三维检测任务.</p><p>论文采用了什么方法? - 使用Anchor-Free的全卷积方法, 输入为三维点云经过体素化后的体素数据, 使用稀疏卷积对体素进行操作. 同时, 作者认为物体的几何尺寸的先验限制了模型的生成能力. 为了不依赖任何先验假设, 作者提出了一种有向边界框的参数化方法.</p><p>论文达到什么效果? - 作者提出的新的参数化方法不仅能够提升已有检测方法的准确性, 而且由于避免使用形状先验, 这种新的参数化方法减少了需要的超参数的数量. FCAF3D在mAP和预测速度这两个指标上、在三大室内数据集(ScanNet V2、SUN RGB-D、S3DIS)上均达到了SOTA.</p><h2 id="精读">精读</h2><blockquote><p>精读：通过泛读找到需要精读重点部分和快速略过的部分，选出精华，仔细阅读。</p></blockquote><h3 id="introduction部分">Introduction部分</h3><p>3D methods are challenged by <u>irregular unstructured 3D data of arbitrary volume</u>.</p><p>三维物体检测的难点在于三维数据是不规则的、非结构化的、容量可变的.</p><p>All convolutional methods for 3D object detection have <u>scalability issues</u>: large-scale scenes either require an impractical amount of computational resources or take too much time to process.</p><p>三维物体检测方法一般都面临拓展性的问题, 对于大规模场景需要耗费很高的计算成本或花费很长时间.</p><p>Other methods opt for voxel data representation and employ sparse convolutions; however, these methods solve scalability problems at the cost of detection accuracy.</p><p>有一些使用稀疏卷积的方法优化处理体素数据, 通过牺牲检测准确度的方式解决拓展性问题.</p><p>Besides being scalable and accurate, an ideal 3D object detection method should also be able to handle <u>diverse objects of arbitrary shapes and sizes</u> without additional hacks and hand-tuned hyperparameters.</p><p>理想的三维检测器应该能够预测各种大小和形状的多种多样的物体.</p><p>Moreover, we introduce a novel oriented bounding box (OBB) parametrization inspired by a <u>Mobius strip</u> that reduces the number of hyperparameters.</p><p>作者提出的新的有向边界框的参数化方法是受到莫比乌斯带的启发.</p><p>Overall, our contribution is three-fold: 1. To our knowledge, we propose a ﬁrst-in-class fully convolutional anchor-free 3D object detection method (FCAF3D) for indoor scenes. 2. We present a novel OBB parametrization and prove it to boost accuracy of several existing 3D object detection methods on SUN RGB-D. 3. Our method signiﬁcantly outperforms the previous state-of-the-art on challenging large-scale indoor ScanNet, SUN RGB-D, and S3DIS datasets in terms of mAP while being faster on inference.</p><p>作者的贡献:</p><ol type="1"><li>提出了FCAF3D, 一个anchor-free的全卷积的室内三维检测算法.</li><li>提出了一种新的有向边界框的参数化方法并证明了这种方法能够显著提升已有的三维检测算法的表现.</li><li>FCAF3D不仅mAP大幅超过之前的SOTA还比之前的算法更快在预测的时候.</li></ol><h3 id="related-work部分">Related Work部分</h3><p>Indoor and outdoor methods have been developing almost <u>independently</u>, <u>applying domain-speciﬁc solutions to address data issues</u>.</p><p>室内和室外三维检测方法基本上是分开独立发展的, 因为这样可以利用domain-speciﬁc的特点解决数据带来的问题.</p><p>Currently, three approaches dominate the ﬁeld of 3D object detection - <u>voting-based</u>, <u>transformer-based</u>, and <u>3D convolutional</u>.</p><p>目前, 室内三维检测主要有三种方式: <u>voting-based</u>、<u>transformer-based</u>和<u>3D convolutional</u>.</p><h4 id="voting-based-methods">Voting-based methods</h4><p>Voting-based方法: VoteNet, BRNet, MLCVNet, H3DNet, VENet.</p><p>All the voting-based methods <u>inherited from VoteNet</u> are limited by design.</p><p>所有的voting-based方法都继承自VoteNet.</p><p>First, their performance depends on <u>the amount of input data</u>; thus, they tend to slow down if given larger scenes and demonstrate <u>poor scalability</u>.</p><p>Voting-based方法的表现依赖于输入数据的数量, 因此在处理大量数据的时候会很慢, 这产生了拓展性的问题.</p><p>Moreover, many voting-based methods <u>implement voting and grouping strategies as complex custom layers</u> making it <u>difﬁcult to reproduce or debug these methods or port them to mobile devices</u>.</p><p>Voting-based方法一般将投票和分组策略实现成复杂的定制的层, 这使得再现和调试变得很困难并且难以将这些方法移植到移动设备上.</p><h4 id="transformer-based-methods">Transformer-based methods</h4><p>Transformer-based方法: GroupFree, 3DETR.</p><p>However, similar to early voting-based methods, more advanced transformer-based methods still experience <u>scalability issues</u>.</p><p>Transformer-based方法也面临拓展性的问题.</p><p>Differently, our method is fully-convolutional thus being <u>faster and signiﬁcantly easier</u> to implement compared to both voting-based and tranformer-based methods.</p><p>FCAF3D是全卷积的方法, 与voting-based方法和transformer-based方法相比, 运行更快且更易实现.</p><h4 id="d-convolutional-methods">3D convolutional methods</h4><p>3D convolutional方法: GSDN.</p><p>Difﬁculties with handling cubically growing sparse 3D data can be overcome by <u>using voxel representation</u>.</p><p>使用体素表达可以解决稀疏的三维数据的立方增长的问题.</p><p>However, <u>dense volumetric features still consume much memory</u>, and <u>3D convolutions are computationally expensive</u>.</p><p>但是稠密的体素特征仍然消耗许多内存, 三维卷积的计算量也很大.</p><p>Overall, processing large scenes requires a lot of resources and <u>cannot be done within a single pass</u>.</p><p>处理大场景的数据需要大量的资源并且无法一次性完成.</p><p>At the same time, our method is anchor-free while taking all advantages of sparse 3D convolutions.</p><p>FCAF3D是一种anchor-free的方法并且有效利用了稀疏三维卷积.</p><h4 id="rgb-based-anchor-free-object-detection">RGB-based anchor-free object detection</h4><p>二维检测中的anchor-free算法: FCOS.</p><p>FCOS3D和ImVoxelNet借鉴了FCOS的思想.</p><p>We adapt the ideas from aforementioned anchor-free methods to address the sparse irregular data.</p><p>作者借鉴了上述的三种方法来解决稀疏的不规则的数据带来的问题.</p><h3 id="proposed-method部分">Proposed Method部分</h3><p>Following the standard 3D detection problem statement, FCAF3D <u>accepts <span class="math inline">\(N_{\text{pts}}\)</span> RGB-colored points</u> and <u>outputs a set of 3D object bounding boxes</u>.</p><p>FCOS3D输入为<span class="math inline">\(N_{\text{pts}}\)</span>个包含RGB颜色信息的点, 输出为三维物体边界框构成的集合.</p><p>The architecture of FCAF3D consists of three parts: a backbone, a neck, and a head.</p><p>FCOS3D架构包含三部分: backbone、neck和head.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202112302104766.png" /></p><p>All convolutions and transposed convolutions are <u>three-dimensional</u> and <u>sparse</u>.</p><p>所有的卷积和转置卷积都是三维稀疏卷积和三维稀疏转置卷积.</p><h4 id="sparse-neural-network部分">Sparse Neural Network部分</h4><h5 id="backbone">Backbone</h5><p>The backbone in FCAF3D is <u>a sparse modiﬁcation of ResNet</u> where all 2D convolutions are replaced with sparse 3D convolutions.</p><p>FCAF3D的backbone是ResNet的一个修改版本, 所有的二维卷积都换成了三维稀疏卷积.</p><p>For brevity, we refer to the family of <u>sparse high-dimensional versions</u> of ResNet as to <u>HDResNet</u>.</p><p>作者称稀疏高维版本的ResNet为HDResNet.</p><h5 id="neck">Neck</h5><p>The neck is <u>a simpliﬁed decoder</u> from GSDN.</p><p>FCAF3D的neck是一个简化后的GSDN的解码器.</p><p>Features on each level are processed with <u>one sparse transposed 3D convolution</u> and <u>one sparse 3D convolution</u>.</p><p>各个层级的特征会被一个稀疏转置三维卷积操作和一个稀疏三位卷积操作处理.</p><p>Each <u>transposed sparse 3D convolution</u> with a <u>kernel size of <span class="math inline">\(2\)</span></u> can increase the number of non-zero values <u>by <span class="math inline">\(2^3\)</span> times</u>.</p><p>每个稀疏转置三维卷积的核大小是<span class="math inline">\(2\)</span>, 处理后的非零元素值的数目会变为原来的<span class="math inline">\(2^3\)</span>倍.</p><p>To <u>prevent the rapid growth of required memory</u>, GSDN introduces the <u>pruning layer</u> that <u>ﬁlters all elements of input with a probability mask</u>.</p><p>为避免内存的急速增长, GSDN引入剪枝层, 接用概率模板来过滤输入元素.</p><p>In GSDN, feature level-wise probabilities are calculated with <u>an additional convolutional scoring layer</u>.</p><p>GSDN使用额外的卷积层来获取不同特征层级的概率模板.</p><p>This layer is trained with a special loss that <u>encourages consistency between the predicted sparsity and anchors</u>.</p><p>GSDN使用一个特别的损失函数来保证预测稀疏程度和锚点的一致性.</p><p>Speciﬁcally, voxel sparsity is set to be positive <u>if any of the subsequent anchors associated to the current voxel is positive</u>.</p><p>体素稀疏度会被置为1, 如果此体素中任何一个子部分与锚点相关联.</p><p>However, using this loss may be <u>suboptimal</u>, as <u>distant voxels</u> of an object might get assigned with a <u>low probability</u>.</p><p>这种获取概率模板的方法是次优的, 因为隔物体较远的体素可能会得到一个很低的概率.</p><p>For simplicity, we <u>remove the scoring layer</u> with the corresponding loss and <u>use probabilities from the classiﬁcation layer in the head instead</u>.</p><p>为了简化, 作者去除了这个计算概率模板的卷积层以及针对这个卷积层设计的损失函数, 作者使用head中的分类层来替代这个卷积层的功能.</p><p>We <u>do not tune the probability threshold</u> but <u>keep at most <span class="math inline">\(N_{\text{vox}}\)</span> voxels to control the sparsity level</u>, where <span class="math inline">\(N_{\text{vox}}\)</span> equals the number of input points <span class="math inline">\(N_{\text{pts}}\)</span>.</p><p>作者不使用概率模板, 而是直接根据概率从高到低排序选择前<span class="math inline">\(N_{\text{vox}}\)</span>个体素, <span class="math inline">\(N_{\text{vox}}\)</span>与输入点的个数<span class="math inline">\(N_{\text{pts}}\)</span>相同.</p><p>We claim this to be a simple yet elegant way to prevent sparsity growth since reusing the same hyperparameter makes the process more transparent and consistent.</p><p>作者认为这种简单的方法能够保持稀疏性, 重复利用超参数让整个处理过程更加透明和一致.</p><h5 id="head">Head</h5><p>The anchor-free head of FCAF3D consists of <u>three parallel sparse convolutional layers with weights shared across feature levels</u>.</p><p>FCAF3D的head部分是anchor-free的, 由三个平行的卷积层组成, 不同的特征层级使用的head共享同一个权重.</p><p>For each location <span class="math inline">\((\hat{x}, \hat{y}, \hat{z})\)</span>, these sparse convolutional layers output <u>classiﬁcation probabilities <span class="math inline">\(\hat{\boldsymbol{p}}\)</span></u>, <u>bounding box regression parameters <span class="math inline">\(\boldsymbol{\delta}\)</span></u>, and <u>centerness <span class="math inline">\(\hat{c}\)</span></u>, respectively.</p><p>对每个位置<span class="math inline">\((\hat{x}, \hat{y}, \hat{z})\)</span>, 这三个平行的卷积层分别输出分类概率<span class="math inline">\(\hat{\boldsymbol{p}}\)</span>、边界框回归参数<span class="math inline">\(\boldsymbol{\delta}\)</span>和中心度(感受野中心与目标物中心的靠近程度)<span class="math inline">\(\hat{c}\)</span>.</p><p>This design is similar to the simple and light-weight head of <u>FCOS</u> but adapted to 3D data.</p><p>FCAF3D的head部分与FCOS的head设计类似.</p><h5 id="multi-level-location-assignment">Multi-level location assignment</h5><p>During training, FCAF3D outputs <u>locations <span class="math inline">\(\{(\hat{x}, \hat{y}, \hat{z})\}\)</span></u> for different feature levels, which should be <u>assigned to ground truth boxes <span class="math inline">\(\{\boldsymbol{b}\}\)</span></u> so the loss can be calculated.</p><p>为了计算损失函数, 我们需要给FCAF3D在不同特征层级的位置<span class="math inline">\(\{(\hat{x}, \hat{y}, \hat{z})\}\)</span>匹配一个边界框标签<span class="math inline">\(\{\boldsymbol{b}\}\)</span>.</p><p>FCOS and ImVoxelNet stick to the following scheme:</p><ul><li>For each location, only ground truth bounding boxes that <u>cover this location</u> are selected.</li><li>Then, the bounding boxes with at least one face <u>further from this location than a threshold</u> are ﬁltered out.</li><li>Finally, the <u>bounding box with the least volume</u> is assigned to this location.</li></ul><p>FCOS and ImVoxelNet使用如下方式:</p><ul><li>对每个位置, 覆盖到这个位置的边界框都会被选中.</li><li>去除那些有多于一个面到此位置的距离超过设定的阈值的边界框.</li><li>在剩下的边界框中容量最小的框会被作为此位置的标签.</li></ul><p>Such an assignment strategy is <u>suboptimal</u>, and its alterations are widely explored in 2D object detection.</p><p>这种匹配方式是次优的.</p><p>ImVoxelNet uses a modiﬁcation that requires <u>hand-tuning the face distance threshold for each feature level</u>.</p><p>ImVoxelNet需要手动设定每一个特征层级的距离阈值.</p><p>We propose a simpliﬁed solution designed for sparse data that <u>does not require tuning dataset-speciﬁc hyperparameters</u>.</p><ul><li>For each bounding box, we select <u>the last feature level</u> for which this <u>bounding box covers at least <span class="math inline">\(N_{\text{loc}}\)</span> locations</u>.</li><li>If the bounding box covers less than <span class="math inline">\(N_{\text{loc}}\)</span> locations at each feature level, we opt for the ﬁrst feature level.</li><li>We also ﬁlter locations via center sampling. In center sampling, only the points close to the center of the bounding box are considered positive matches.</li></ul><p>作者使用一个简化的不需要依赖数据集的超参数的方案来配对.</p><ul><li>对每个边界框, 我们选取此边界框框住多余<span class="math inline">\(N_{\text{loc}}\)</span>个位置的特征层级中的最后的层级.</li><li>如果所有层级中此边界框框住的位置数量都少于<span class="math inline">\(N_{\text{loc}}\)</span>, 那么选取第一个特征层级.</li><li>对于选定的层级中, 我们对位置进行中心采样, 只有靠近边界框中心的位置会被认为是匹配上的.</li></ul><p>After the location assignment, some ﬁnal locations <span class="math inline">\(\{(\hat{x}, \hat{y}, \hat{z})\}\)</span> are matched with ground truth bounding boxes <span class="math inline">\(\boldsymbol{b}_{\hat{x}, \hat{y}, \hat{z}}\)</span>.</p><p>位置匹配之后, 一些位置<span class="math inline">\(\{(\hat{x}, \hat{y}, \hat{z})\}\)</span>会与边界框标签<span class="math inline">\(\boldsymbol{b}_{\hat{x}, \hat{y}, \hat{z}}\)</span>配对.</p><p>Consequently, these locations become associated with ground truth labels <span class="math inline">\(p_{\hat{x}, \hat{y}, \hat{z}}\)</span> and 3D centerness values <span class="math inline">\(c_{\hat{x}, \hat{y}, \hat{z}}\)</span>.</p><p>这些位置也会与对应的标签<span class="math inline">\(p_{\hat{x}, \hat{y}, \hat{z}}\)</span>和中心度数值<span class="math inline">\(c_{\hat{x}, \hat{y}, \hat{z}}\)</span>匹配.</p><p>During inference, the <u>scores <span class="math inline">\(\hat{\boldsymbol{p}}\)</span> are multiplied by 3D centerness <span class="math inline">\(\hat{c}\)</span></u> just before applying NMS.</p><p>在预测时, 分类分数<span class="math inline">\(\hat{\boldsymbol{p}}\)</span>会乘上中心度<span class="math inline">\(\hat{c}\)</span>作为边界框置信度进行NMS.</p><h5 id="loss-function">Loss function</h5><p>The <u>overall loss function</u> is formulated as follows: <span class="math display">\[\begin{array}{r}L=\frac{1}{N_{\mathrm{pos}}} \sum_{\hat{x}, \hat{y}, \hat{z}}\left(L_{\mathrm{cls}}(\hat{\boldsymbol{p}}, p)+\mathbb{1}_{\left\{p_{\hat{x}, \hat{y}, \hat{z}} \neq 0\right\}} L_{\mathrm{reg}}(\hat{\boldsymbol{b}}, \boldsymbol{b})+\mathbb{1}_{\left\{p_{\hat{x}, \hat{y}, \hat{z}} \neq 0\right\}} L_{\mathrm{cntr}}(\hat{c}, c)\right)\end{array}\]</span> 以上是整体的损失函数.</p><p>Here, the <u>number of matched locations</u> <span class="math inline">\(N_{\text{pos}}\)</span> is <span class="math inline">\(\sum_{\hat{x}, \hat{y}, \hat{z}} \mathbb{1}_{\left\{p_{\hat{x}, \hat{y}, \hat{z}} \neq 0\right\}}\)</span>.</p><p>匹配到的位置点总数<span class="math inline">\(N_{\text{pos}} = \sum_{\hat{x}, \hat{y}, \hat{z}} \mathbb{1}_{\left\{p_{\hat{x}, \hat{y}, \hat{z}} \neq 0\right\}}\)</span>.</p><p>Classiﬁcation loss <span class="math inline">\(L_{\text{cls}}\)</span> is calculated as a focal loss, regression loss <span class="math inline">\(L_{\text{reg}}\)</span> is IoU, and centerness loss <span class="math inline">\(L_{\text{cntr}}\)</span> is binary cross-entropy.</p><p>类别损失<span class="math inline">\(L_{\text{cls}}\)</span>使用focal loss, 回归损失<span class="math inline">\(L_{\text{reg}}\)</span>使用IoU, 中心损失<span class="math inline">\(L_{\text{cntr}}\)</span>使用binary cross-entropy.</p><p>For each loss, <u>predicted values are denoted with a hat</u>.</p><p>所有预测值都有hat标记.</p><h4 id="bounding-box-parametrization部分">Bounding Box Parametrization部分</h4><p>The 3D object bounding boxes can be <u>axis-aligned (AABB)</u> or <u>oriented (OBB)</u>.</p><p>三维物体边界框分为坐标轴对齐的边界框AABB和有向边界框OBB.</p><p><u>An AABB can be described as <span class="math inline">\(\boldsymbol{b}^{\text{AABB}}=(x, y, z, w, l, h)\)</span></u>, while the deﬁnition of <u>an OBB includes a <em>heading angle</em> <span class="math inline">\(\theta\)</span>: <span class="math inline">\(\boldsymbol{b}^{\text{OBB}}=(x, y, z, w, l, h, \theta)\)</span></u>.</p><p>坐标轴对齐的边界框AABB定义为<span class="math inline">\(\boldsymbol{b}^{\text{AABB}}=(x, y, z, w, l, h)\)</span>, 有向边界框OBB定义为<span class="math inline">\(\boldsymbol{b}^{\text{OBB}}=(x, y, z, w, l, h, \theta)\)</span>, 其中<span class="math inline">\(\theta\)</span>为朝向角.</p><p>In both formulas, <u><span class="math inline">\(x, y, z\)</span> denote the coordinates of the center of a bounding box</u>, while <u><span class="math inline">\(w, l, h\)</span> are its width, length, and height</u>, respectively.</p><p>在两种边界框的定义中, <span class="math inline">\(x, y, z\)</span>表示边界框中心点的坐标, <span class="math inline">\(w, l, h\)</span>表示边界框的宽、长和高.</p><h5 id="aabb-parametrization">AABB parametrization</h5><p>Speciﬁcally, for <u>a ground truth AABB <span class="math inline">\((x, y, z, w, l, h)\)</span></u> and <u>a location <span class="math inline">\((\hat{x}, \hat{y}, \hat{z})\)</span></u>, <u><span class="math inline">\(\boldsymbol{\delta}\)</span> can be formulated as a 6-tuple</u>: <span class="math display">\[\begin{gathered}\delta_{1}=x+\frac{w}{2}-\hat{x}, \delta_{2}=\hat{x}-x+\frac{w}{2}, \delta_{3}=y+\frac{l}{2}-\hat{y} \\\delta_{4}=\hat{y}-y+\frac{l}{2}, \delta_{5}=z+\frac{h}{2}-\hat{z}, \delta_{6}=\hat{z}-z+\frac{h}{2}\end{gathered}\]</span></p><p>对于一个坐标轴对齐的边界框AABB<span class="math inline">\((x, y, z, w, l, h)\)</span>和一个位置<span class="math inline">\((\hat{x}, \hat{y}, \hat{z})\)</span>, <span class="math inline">\(\delta\)</span>可以参数化为以上的形式(其实就是位置点到六个面的距离).</p><p>The predicted AABB <span class="math inline">\(\hat{\boldsymbol{b}}\)</span> can be trivially obtained from <span class="math inline">\(\boldsymbol{\delta}\)</span>.</p><p>使用位置坐标及其对应的<span class="math inline">\(\boldsymbol{\delta}\)</span>即可反解出对应的坐标轴对齐的边界框AABB<span class="math inline">\((x, y, z, w, l, h)\)</span>. <span class="math display">\[\begin{gathered}w = \delta_{1} + \delta_{2}, l = \delta_{3} + \delta_{4}, h = \delta_{5} + \delta_{6} \\x = \frac{\delta_{1} - \delta_{2}}{2} + \hat{x}, y = \frac{\delta_{3} - \delta_{4}}{2} + \hat{y}, z = \frac{\delta_{5} - \delta_{6}}{2} + \hat{z}\end{gathered}\]</span></p><h5 id="heading-angle-estimation">Heading angle estimation</h5><p>All existing state-of-the-art 3D object detection methods from point clouds <u>address the heading angle estimation task as classiﬁcation followed by regression</u>.</p><p>所有已有的使用点云的三位检测SOTA算法都将朝向角估计视为回归之后的分类任务.</p><p>The <u>heading angle is classiﬁed into bins</u>; then, the <u>precise value of the heading angle is regressed within a bin</u>.</p><p>朝向角先被分类为bins中的某一个bin, 之后再回归朝向角和这个bin之间的残差.</p><p>For indoor scenes, there are typically <u><span class="math inline">\(12\)</span> bins that uniformly divide the range from <span class="math inline">\(0\)</span> to <span class="math inline">\(2\pi\)</span> into <span class="math inline">\(12\)</span> sectors</u>.</p><p>对于室内场景, 通常在<span class="math inline">\(0\)</span>到<span class="math inline">\(2\pi\)</span>之间均匀划分<span class="math inline">\(12\)</span>个部分.</p><p>For outdoor scenes, <u>the number of bins is usually set to two</u> as the objects on the road can <u>be oriented either parallel or perpendicular to the road</u>.</p><p>对于室外场景, 通常划分两个类别, 因为路上的物体一般只有与道路平行或垂直这两种情况.</p><p>When a heading angle bin is chosen, the <u>actual value of the heading angle should be estimated through regression</u>.</p><p>得到朝向角的bin之后, 真实数值需要使用回归进行估计.</p><p>VoteNet and other voting-based methods estimate the value of missing <span class="math inline">\(\theta\)</span> directly.</p><p>VoteNet和其他voting-based的方法直接估计朝向角<span class="math inline">\(\theta\)</span>的数值.</p><p>Outdoor methods explore more elaborate approaches, e.g. predicting the values of trigonometric functions.</p><p>室外方法探索了更加复杂的方式, 例如预测三角函数的数值.</p><p>For instance, SMOKE estimates the values of <span class="math inline">\(\sin{\theta}\)</span> and <span class="math inline">\(\cos{\theta}\)</span>, which allows recovering the heading angle.</p><p>例如, SMOKE估计了<span class="math inline">\(\sin{\theta}\)</span>和<span class="math inline">\(\cos{\theta}\)</span>的数值, 通过这两个数值我们可以恢复朝向角的角度.</p><p>Figure below depicts indoor objects where the <u>heading angle cannot be deﬁned unambiguously</u>.</p><p>下图展示了在室内的物体的朝向角无法被没有歧义地被定义.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202112311353526.png" /></p><p>Ground truth angle annotations are random for these objects, making heading angle bin classiﬁcation meaningless.</p><p>这些物体的朝向角的标注是随机的, 这使得朝向角的bin分类没有意义.</p><p><u>To avoid penalizing the correct predictions that do not coincide with annotations</u>, we <u>use rotated IoU as a loss function</u>, since its value is not affected by the choice of a heading angle among possible options.</p><p>为了避免惩罚与标注不相符的正确的预测角, 作者使用旋转后的IoU作为损失函数, 因为旋转后的IoU不会被朝向角的固定标注值影响.</p><p>Thus, we propose <u>OBB parametrization that allows considering the rotation ambiguity</u>.</p><p>因此, 作者提出了考虑了旋转歧义的有向边界框的参数化方法.</p><h5 id="proposed-mobius-obb-parametrization">Proposed Mobius OBB parametrization</h5><p>Assuming that <u>an OBB has the parameters <span class="math inline">\((x, y, z, w, l, h, \theta)\)</span></u>, let us <u>denote <span class="math inline">\(q=\frac{w}{l}\)</span></u>.</p><p>假设有向边界框有参数<span class="math inline">\((x, y, z, w, l, h, \theta)\)</span>, 记<span class="math inline">\(q=\frac{w}{l}\)</span>.</p><p>If <span class="math inline">\(x, y, z, w + l, h\)</span> are ﬁxed, it turns out that the <u>OBBs with <span class="math inline">\((q, \theta),\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right),(q, \theta+\pi),\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right)\)</span> deﬁne the same bounding box</u>.</p><p>如果<span class="math inline">\(x, y, z, w + l, h\)</span>都是固定的, <span class="math inline">\((q, \theta),\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right),(q, \theta+\pi),\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right)\)</span>所代表的有向边界框等价.</p><p>We notice that the set of <span class="math inline">\((q, \theta)\)</span>, where <span class="math inline">\(\theta \in (0, 2\pi]\)</span>, <span class="math inline">\(q \in (0, +\inf)\)</span> is <u>topologically equivalent to a Mobius strip</u> up to this equivalence relation.</p><p>作者注意到<span class="math inline">\((q, \theta)\)</span>参数集合在拓扑不变性层面和莫比乌斯环相似, 其中<span class="math inline">\(\theta \in (0, 2\pi]\)</span>, <span class="math inline">\(q \in (0, +\inf)\)</span>.</p><p>Hence, we can reformulate the task of estimating <span class="math inline">\((q, \theta)\)</span> as a task of <u>predicting a point on a Mobius strip</u>.</p><p>于是估计<span class="math inline">\((q, \theta)\)</span>的任务转化为了估计莫比乌斯环上的一个点.</p><p>A natural way to embed a Mobius strip being a two-dimensional manifold to Euclidean space is the following:</p><p>一个将莫比乌斯环, 这一个二维流形, 嵌入欧几里得空间的方式如下: <span class="math display">\[(q, \theta) \mapsto(\ln (q) \sin (2 \theta), \ln (q) \cos (2 \theta), \sin (4 \theta), \cos (4 \theta))\]</span> It is easy to verify that <u>4 points are mapped into a single point</u> in Euclidean space.</p><p>可以验证之前提到的四个等价参数在上述表示的情况下对应同一个点.</p><p>However, the experiments reveal that <u>predicting only <span class="math inline">\(\ln{(q)} \sin{(2\theta)}\)</span> and <span class="math inline">\(\ln{(q)} \cos{(2θ)}\)</span> improves results compared to predicting all four values</u>.</p><p>实验证明只预测<span class="math inline">\(\ln{(q)} \sin{(2\theta)}\)</span>和<span class="math inline">\(\ln{(q)} \cos{(2θ)}\)</span>比预测全部的四个值更能有效提升预测结果.</p><p>Thereby, we opt for <u>a pseudo embedding of a Mobius strip to <span class="math inline">\(\mathbb{R}^{2}\)</span></u>.</p><p>于是, 我们优化一个伪莫比乌斯二维嵌入表达式.</p><p>We call it pseudo since the entire center circle of a Mobius strip deﬁned by <span class="math inline">\(\ln{(q)} = 0\)</span> maps to <span class="math inline">\((0, 0)\)</span>.</p><p>这个是伪莫比乌斯二维嵌入表达式是因为, 当<span class="math inline">\(\ln{(q)} = 0\)</span>的时候, 莫比乌斯环退化为一个圆且前两个参数变为<span class="math inline">\((0, 0)\)</span>.</p><p>Accordingly, we <u>cannot distinguish points with <span class="math inline">\(\ln{(q)} = 0\)</span></u>.</p><p>因此, 我们无法处理<span class="math inline">\(\ln{(q)} = 0\)</span>的点.</p><p>However, <span class="math inline">\(\ln{(q)} = 0\)</span> implies strict equality of <span class="math inline">\(w\)</span> and <span class="math inline">\(l\)</span>, which is <u>rare in real-world scenarios</u>.</p><p>然而, <span class="math inline">\(\ln{(q)} = 0\)</span>表示<span class="math inline">\(w\)</span>和<span class="math inline">\(l\)</span>相等, 在真实世界中, 这是很稀少的情况.</p><p>Moreover, the choice of an angle has a <u>minor effect on the IoU</u> if <span class="math inline">\(w = l\)</span>; thereby, we <u>ignore this rare case</u> for the sake of detection accuracy and simplicity of the method.</p><p>而且<span class="math inline">\(w = l\)</span>对于使用IoU损失函数来监督角度的选择的影响很小. 因此, 作者忽略了<span class="math inline">\(w = l\)</span>这种罕见的情况为了检测的准确性和方法的简洁性.</p><p>Overall, we obtain <u>a novel OBB parametrization</u>:</p><p>于是, 作者获得了一个全新的有向边界框的参数化方式: <span class="math display">\[\delta_{7}=\ln \frac{w}{l} \sin (2 \theta), \delta_{8}=\ln \frac{w}{l} \cos (2 \theta)\]</span> In standard 3D bounding box parametrization (AABB), <u><span class="math inline">\(\hat{\boldsymbol{b}}\)</span> is trivially derived from <span class="math inline">\(\boldsymbol{\delta}\)</span></u>.</p><p>在坐标轴对齐的边界框AABB的参数化模型中, <span class="math inline">\(\hat{\boldsymbol{b}}\)</span>可以轻而易举地从<span class="math inline">\(\boldsymbol{\delta}\)</span>中派生出来.</p><p>In the proposed parametrization, <span class="math inline">\(w, l, \theta\)</span> are non-trivial and can be obtained via the following:</p><p>在提出的新的有向边界框的参数化模型中, <span class="math inline">\(w, l, \theta\)</span>需要通过以下方式获取: <span class="math display">\[w=\frac{s q}{1+q}, l=\frac{s}{1+q}, \theta=\frac{1}{2} \arctan \frac{\delta_{7}}{\delta_{8}}\]</span> where ratio <span class="math inline">\(q=e^{\sqrt{\delta_{7}^{2}+\delta_{8}^{2}}}\)</span> and size <span class="math inline">\(s=\delta_1+\delta_2+\delta_3+\delta_4\)</span>.</p><h5 id="莫比乌斯环参数解释">莫比乌斯环参数解释</h5><blockquote><p><em>参考资料: <a href="https://zhuanlan.zhihu.com/p/75237170">莫比乌斯带的参数方程是怎么来的？它又为什么没有方向呢？</a></em></p></blockquote><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202112311824156.png" /></p><p>莫比乌斯环参数: <span class="math inline">\((q, \theta) \mapsto (\ln{(q)}\sin{(2\theta)}, \ln{(q)}\cos{(2\theta)}, \sin{(4\theta)}, \cos{(4\theta)})\)</span>, <span class="math inline">\(q\in (0, +\inf)\)</span>, <span class="math inline">\(\theta \in (0, 2\pi]\)</span>.</p><p>后两个参数<span class="math inline">\((\sin{(4\theta)}, \cos{(4\theta)})\)</span>在<span class="math inline">\(xoy\)</span>平面确定圆<span class="math inline">\(C_1\)</span>, 圆心为原点<span class="math inline">\(o\)</span>, 半径<span class="math inline">\(r\)</span>为<span class="math inline">\(1\)</span>, 假设某一角度<span class="math inline">\(\theta\)</span>对应的点为<span class="math inline">\(P\)</span>, 射线<span class="math inline">\(\vec{ol}\)</span>是从原点<span class="math inline">\(o\)</span>引向点<span class="math inline">\(P\)</span>的射线, 如下图所示.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021151199.png" /></p><p>前两个参数<span class="math inline">\((\ln{(q)}\sin{(2\theta)}, \ln{(q)}\cos{(2\theta)})\)</span>在<span class="math inline">\(zol\)</span>平面确定圆<span class="math inline">\(C_2\)</span>, 平面<span class="math inline">\(zol\)</span>与平面<span class="math inline">\(xoy\)</span>垂直且交线为射线<span class="math inline">\(ol\)</span>所在直线, 圆<span class="math inline">\(C_2\)</span>的圆心为点<span class="math inline">\(P\)</span>(即<span class="math inline">\(xoy\)</span>平面上的<span class="math inline">\(P\)</span>点), 圆<span class="math inline">\(C_2\)</span>的半径<span class="math inline">\(r\)</span>为<span class="math inline">\(\ln{(q)}\)</span>, 角度<span class="math inline">\(\theta\)</span>对应的点为<span class="math inline">\(Q\)</span>, <span class="math inline">\(Q\)</span>点即为参数<span class="math inline">\(q\)</span>和<span class="math inline">\(\theta\)</span>确定的莫比乌斯环上的一点, 如下图所示.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021153898.png" /></p><p>由于<span class="math inline">\(q \in (0, +\inf)\)</span>, 为了能够表示完整的直径, 需要使用<span class="math inline">\(\ln{(q)}\)</span>将值域拓展到<span class="math inline">\((-\inf, +\inf)\)</span>以涵盖整个直径的范围而不是只有半径的范围(意思是<span class="math inline">\(\vec{PQ}\)</span>可以反向延伸), 同时<span class="math inline">\(\ln{(q)}\)</span>可以使得文中提出的四组参数<span class="math inline">\((q, \theta),\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right),(q, \theta+\pi),\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right)\)</span>在参数化之后等价.</p><p>由于莫比乌斯环的性质, 莫比乌斯环上的点在运动时, 在圆<span class="math inline">\(C_1\)</span>上的角速度为圆<span class="math inline">\(C_2\)</span>上的两倍, 因此两个圆的表达式中, <span class="math inline">\(\theta\)</span>前面的系数也应该保持<span class="math inline">\(2:1\)</span>的关系.</p><p>由于需要使得文中提出的四组参数<span class="math inline">\((q, \theta),\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right),(q, \theta+\pi),\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right)\)</span>在参数化之后等价, 所以不直接用<span class="math inline">\((2 \theta, \theta)\)</span>而是用<span class="math inline">\((4 \theta, 2 \theta)\)</span>.</p><h3 id="experiments部分">Experiments部分</h3><h4 id="datasets部分">Datasets部分</h4><p>We evaluate our method on three 3D object detection benchmarks: <u>ScanNet V2, SUN RGB-D, and S3DIS</u>.</p><p>作者在ScanNet V2、SUN RGB-D和S3DIS上测试了FCAF3D的算法性能.</p><p>For all datasets, we use <u>mean average precision (mAP) under IoU thresholds of 0.25 and 0.5</u> as a metric.</p><p>对于所有数据集, 作者使用<span class="math inline">\(mAP@0.25\)</span>和<span class="math inline">\(mAP@0.5\)</span>作为评价指标.</p><h5 id="scannet-v2">ScanNet V2</h5><p>The ScanNet dataset contains <u>1513 reconstructed 3D indoor scans</u> with <u>per-point instance and semantic labels of 18 object categories</u>.</p><p>ScanNet数据集包含1513个三维室内场景, 每个场景带有点级别的实例标注和18个物体类别的语义标注.</p><p>Given this annotation, we <u>calculate AABBs through a standard approach</u>.</p><p>ScanNet数据集使用的是AABB边界框.</p><p>The <u>training subset is comprised of 1201 scans</u>, while <u>the resting 312 scans are left for validation</u>.</p><p>ScanNet数据集的训练集包含1201个扫描场景, ScanNet数据集的验证集包含剩下的312个扫描场景.</p><h5 id="sun-rgb-d">SUN RGB-D</h5><p>SUN RGB-D is a monocular dataset for 3D scene understanding <u>containing more than 10,000 indoor RGB-D images</u>.</p><p>SUN RGB-D数据集包含超过10000个室内场景RGB-D图片.</p><p>The annotation consists of <u>per-point semantic labels and OBBs of 37 object categories</u>.</p><p>SUN RGB-D数据集标注包含点级别的语义标签和37类物体的OBB边界框.</p><p>We run experiments with objects of the <u>10 most common categories</u>.</p><p>作者在10类最常见的类别上进行了实验.</p><p>The <u>training and validation splits contain 5285 and 5050 point clouds</u>, respectively.</p><p>SUN RGB-D数据集的训练集包含5285个点云, SUN RGB-D数据集验证集包含5050个点云.</p><h5 id="s3dis">S3DIS</h5><p>Stanford Large-Scale 3D Indoor Spaces dataset <u>contains 3D scans of 6 buildings with 272 rooms</u>.</p><p>S3DIS数据集包含6栋建筑物的272个房间的三维场景扫描数据.</p><p>Each scan is annotated <u>with instance and semantic labels of seven structural elements (e.g. ﬂoor and ceiling) and ﬁve furniture categories</u>.</p><p>S3DIS数据集的每个扫描数据包含7类结构元素和5个家具类别的实例和物体标签.</p><p>We evaluate our method on <u>furniture categories only</u>.</p><p>作者在家具类别上进行了实验.</p><p>Similar to ScanNet, <u>AABBs are derived from 3D semantics</u>.</p><p>S3DIS数据集与ScanNet数据集类似, 使用AABB边界框.</p><p>We use the ofﬁcial split, where <u>68 rooms from Area 5 are intended for validation</u>, while the <u>remaining 204 rooms comprise the training subset</u>.</p><p>S3DIS数据集训练集包含区域5之外的204个房间, S3DIS数据集测试集包含区域5的68个房间.</p><h4 id="implementation-details部分">Implementation Details部分</h4><h5 id="hyperparameters">Hyperparameters</h5><p>First, <u>the size of output classiﬁcation layer equals the number of object categories</u>, which is <u>18, 10, and 5 for ScanNet, SUN RGB-D, and S3DIS</u>.</p><p>分类层的输出与各数据集的物体类别匹配, ScanNet V2是18, SUN RGB-D是10, S3DIS是5.</p><p>Second, <u>SUN RGB-D contains OBBs</u>, so we <u>predict additional targets <span class="math inline">\(\delta_7\)</span> and <span class="math inline">\(\delta_8\)</span> for this dataset</u>; note that the <u>loss function is not affected</u>.</p><p>只有SUN RGB-D数据集包含OBB边界框, 因此只在SUN RGB-D数据集中预测<span class="math inline">\(\delta_7\)</span>和<span class="math inline">\(\delta_8\)</span>, 损失函数不受影响.</p><p>Last, <u>ScanNet, SUN RGB-D, and S3DIS contain different numbers of scenes</u>, so we <u>repeat each scene 10, 3, and 13 times per epoch</u>, respectively.</p><p>ScanNet、SUN RGB-D和S3DIS数据集场景数量各不相同, 在每个epoch中, 这三个数据集中每个场景分别重复出现10、3和13次.</p><p>In initial point cloud voxelization, we <u>set the voxel size to 0.01m and the number of points <span class="math inline">\(N_{\text{pts}}\)</span> to 100,000</u>.</p><p>在点云体素化的过程中, 作者设置体素大小为0.01米, 设置点数量<span class="math inline">\(N_{\text{pts}}=100000\)</span>.</p><p>Respectively, <u><span class="math inline">\(N_{\text{vox}}\)</span> equals to 100,000.</u></p><p>对应的, <span class="math inline">\(N_{\text{vox}}\)</span>也设置为100000.</p><p><u>Both ATSS and FCOS set <span class="math inline">\(N_{\text{loc}}\)</span> to <span class="math inline">\(3^2\)</span></u> for 2D object detection.</p><p>在二维检测中, ATSS和FCOS算法都设置<span class="math inline">\(N_{\text{loc}}\)</span>为<span class="math inline">\(3^2\)</span>.</p><p>Accordingly, we select a feature level so <u>bounding box covers at least <span class="math inline">\(N_{\text{loc}} = 3^3\)</span> locations</u>.</p><p>同样地, 作者设置<span class="math inline">\(N_{\text{loc}}\)</span>为<span class="math inline">\(3^3\)</span>(因为是三维检测).</p><p>By center sampling, we <u>select 18 locations</u>, while the <u>NMS IoU threshold is 0.5</u>.</p><p>通过中心点采样, 作者选取18个位置, 同时设置NMS的IoU阈值为0.5.</p><h5 id="training">Training</h5><p>We implement our FCAF3D <u>using the MMdetection3D framework</u>.</p><p>作者使用MMdetection3D架构实现FCAF3D.</p><p>The overall training procedure follows the default scheme from MMdetection: <u>training takes 12 epochs with the learning rate decreasing on the 8th and the 11th epochs</u>.</p><p>训练过程遵循默认的MMdetection的设置: 训练12个epoch, 学习率在第8个和第11个epoch下降.</p><p>We employ the <u>Adam optimizer with an initial learning rate of 0.001 and weight decay of 0.0001</u>.</p><p>作者使用Adam优化器优化学习率, 初始学习率设置为0.001, 权重衰减设置为0.0001.</p><p>All models are trained on two NVidia V100 with a <u>batch size of 8</u>.</p><p>所有的模型都在两块NVidia V100上训练, 批大小是8.</p><p>Evaluation and performance tests are run on a single NVidia GTX1080Ti.</p><p>评估和表现测试都是在单张NVidia GTX1080Ti上完成的.</p><h5 id="evaluation">Evaluation</h5><p>Both training and evaluation procedures are randomized, as <u>the raw point clouds are randomly sampled to select <span class="math inline">\(N_{\text{pts}}\)</span> for the input</u>.</p><p>由于每次都会在原始点云上随机采样<span class="math inline">\(N_{\text{pts}}\)</span>个点, 训练和验证过程都是随机的.</p><p>Thus, we re-run all the experiments to <u>obtain statistically signiﬁcant results</u>.</p><p>因此, 作者反复进行实验来获取统计学上显著的结果.</p><p>We run <u>training 5 times and test each trained model 5 times independently</u>.</p><p>作者独立的训练了5个模型, 每个模型分别独立测试5次.</p><p>We report both the <u>best and average metrics across all <span class="math inline">\(5 \times 5\)</span> trials</u>.</p><p>作者报告了这<span class="math inline">\(5 \times 5\)</span>实验中评价指标的最优值和均值.</p><h3 id="results部分">Results部分</h3><h4 id="comparison-with-state-of-the-art-methods部分">Comparison with State-of-the-art Methods部分</h4><p>Results of FCAF3D and existing indoor 3D object detection methods that <u>accept point clouds</u>.</p><p>FCAF3D和现有的接受点云输入的室内三维物体检测方法的实验结果.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021131231.png" /></p><p>The reported metric value is the <u>best one across 25 trials</u>; the <u>average value is given in brackets</u>.</p><p>在FCAF3D的25组模型中, 括号外的是表现最好的模型的评价指标, 括号内是所有模型评价指标的均值.</p><h4 id="object-geometry-priors部分">Object Geometry Priors部分</h4><p>VoteNet and ImVoteNet have the <u>same head</u> and are trained with the <u>same losses</u>.</p><p>VoteNet和ImVoteNet有相同的head和相同的损失函数.</p><p>Among them, there are 4 prior losses: <u>size classiﬁcation loss, size regression loss, direction classiﬁcation loss, and direction regression loss</u>.</p><p>VoteNet和ImVoteNet的与先验信息有关的损失函数包含: 大小分类损失, 大小回归损失, 朝向分类损失和朝向回归损失.</p><p><u>Both classiﬁcation losses correspond to targets parametrized using priors</u> (per-category mean object sizes and a set of angle bins).</p><p>大小分类函数和朝向分类函数都与使用先验信息的参数有关.</p><p>Similar to FCAF3D, we <u>replace the aforementioned losses with a rotated IoU loss with Mobius parametrization</u>.</p><p>作者将VoteNet和ImVoteNet的上述损失函数更换为旋转IoU损失并且将边界框的参数化方法更换为莫比乌斯参数化模型.</p><p>To give a complete picture, we also <u>try a sin-cos parametrization used in the outdoor 3D object detection method SMOKE</u>.</p><p>为了更完全的比较, 作者也尝试使用了在室外三维物体检测算法SMOKE中使用的sin-cos参数化方法进行对比实验.</p><p>The rotated IoU loss allows <u>decreasing the number of trainable parameters and hyperparameters</u>, including geometry priors and loss weights.</p><p>旋转IoU损失可以有效减少训练的参数量和超参数的数量.</p><p>ImVoxelNet <u>does not use a classiﬁcation+regression scheme to estimate heading angle</u> but <u>predicts its value directly in a single step</u>.</p><p>ImVoxelNet不是采用分类加回归的方案预测朝向角, 其直接预测朝向角的数值.</p><p>Since the original ImVoxelNet uses the rotated IoU loss, we do not need to remove redundant losses, <u>only to change the parametrization</u>.</p><p>ImVoxelNet使用旋转IoU损失, 因此不用修改损失函数, 只用更改其边界框参数化方法.</p><p>Results of several existing 3D object detection methods that accept inputs of different modalities, with different OBB parametrization on the SUN RGB-D dataset.</p><p>使用不同损失函数和边界框参数的对比试验结果.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021147495.png" /></p><p>For FCAF3D, the reported metric value is the <u>best across 25 trials</u>; the <u>average value is given in brackets</u>.</p><p>在FCAF3D的25组模型中, 括号外的是表现最好的模型的评价指标, 括号内是所有模型评价指标的均值.</p><p>For other methods, we report results from the <u>original papers</u> and also the <u>results obtained through our experiments with MMdetection3D-based re-implementations (marked as Reimpl)</u>.</p><p>其他方法的数据来自其原始论文和作者使用MMdetection3D重新实现的代码进行实验的结果(标记为Reimpl).</p><h5 id="gsdn-anchors">GSDN anchors</h5><p>Next, we study GSDN anchors to prove that the <u>generalization ability of anchor-based models is limited</u>.</p><p>作者对GSDN进行实验来验证anchor-based算法的生成能力是有限的.</p><p><u>Without domain-speciﬁc guidance in the form of anchors, GSDN demonstrates a poor performance</u>; hence, we claim this method to be <u>inﬂexible and non-generalized</u>.</p><p>在没有anchor的情况下, GSDN表现很糟糕, 因此, 我们认为GSDN的延展性和生成能力很差.</p><p>Results of fully convolutional 3D object detection methods that accept point clouds on ScanNet.</p><p>GSDN和FCAF3D的对比实验结果.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021148945.png" /></p><h4 id="ablation-study部分">Ablation Study部分</h4><p>We run experiments with <u>varying voxel size, the number of points in a point cloud <span class="math inline">\(N_{\text{pts}}\)</span>, the number of locations selected by center sampling, and with and without centerness</u>.</p><p>作者使用不同的体素大小、<span class="math inline">\(N_{\text{pts}}\)</span>、中心采样的位置点数量和有无中心度进行消融实验.</p><p>Results of ablation studies on the voxel size, the number of points (which equals the number of voxels <span class="math inline">\(N_{\text{vox}}\)</span> in pruning), centerness, and center sampling in FCAF3D.</p><p>消融实验的实验结果.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021407746.png" /></p><p>The reported metric value is the <u>best across 25 trials</u>; the <u>average value is given in brackets</u>.</p><p>在FCAF3D的25组模型中, 括号外的是表现最好的模型的评价指标, 括号内是所有模型评价指标的均值.</p><h5 id="voxel-size">Voxel size</h5><p>Expectedly, <u>with an increasing voxel size, accuracy goes down</u>.</p><p>体素越大, 精度越低.</p><h5 id="number-of-points">Number of points</h5><p>Similar to 2D images, <u>subsampled point clouds are sometimes referred to as <em>low-resolution</em> ones</u>.</p><p>下采样的点云可以称之为低精度点云.</p><p>Accordingly, they <u>contain less information than their <em>high-resolution</em> versions</u>.</p><p>低精度点云相比于高精度点云包含更少的信息.</p><p>As can be expected, <u>the fewer the points, the lower is detection accuracy</u>.</p><p>点云数量越少, 精度越低.</p><h5 id="centerness">Centerness</h5><p>Using centerness <u>improves mAP for the ScanNet and SUN RGB-D datasets</u>.</p><p>使用中心度对于ScanNet和SUN RGB-D数据集有提升.</p><p>For S3DIS, the results are controversial: the better <span class="math inline">\(mAP@0.5\)</span> is balanced with a minor decrease of <span class="math inline">\(mAP@0.25\)</span>.</p><p>对于S3DIS数据集, 使用中心度在<span class="math inline">\(mAP@0.5\)</span>上有提升, 在<span class="math inline">\(mAP@0.25\)</span>上有略微下降.</p><p>Nevertheless, we analyze the results altogether, so we can <u>consider centerness a helpful feature with a small positive effect on the mAP</u>, almost reaching 1% of <span class="math inline">\(mAP@0.5\)</span> on ScanNet.</p><p>整体来说, 中心度是一个有用的特征能够对于提升mAP有较小的积极作用.</p><h5 id="center-sampling">Center sampling</h5><p>We select <span class="math inline">\(9\)</span> locations, as proposed in FCOS, the entire set of <span class="math inline">\(27\)</span> locations, as in ImVoxelNet, and <span class="math inline">\(18\)</span> being an average of these options.</p><p>作者尝试了9, 27和18这三种不同的选项.</p><p><u>The latter appeared to be the best choice that allows achieving higher mAP</u> on all the benchmarks.</p><p>将中心采样的位置数量设置为18时, 在mAP上的表现最好.</p><h4 id="inference-speed部分">Inference Speed部分</h4><p>Compared to standard convolutions, <u>sparse convolutions are time- and memory-efﬁcient</u>.</p><p>稀疏卷积在时间上和空间上更加高效.</p><p>FCAF3D uses the <u>same sparse convolutions and the same backbone</u> as GSDN.</p><p>FCAF3D使用与GSDN相同的卷积和backbone.</p><p>However, <u>the default FCAF3D is slower than GSDN</u>.</p><p>但是默认设置的FCAF3D比GSDN慢.</p><p>This is due to the smaller voxel size: we use 0.01m for a proper multi-level assignment while GSDN uses 0.05m.</p><p>这是因为FCAF3D的体素比GSDN的小.</p><p>To build the fastest method, we conduct experiments with HDResNet34:3 and HDResNet34:2 with only three and two feature levels, respectively.</p><p>为了加快FCAF3D的运行速度, 作者设计了使用两层或三层特征层级的版本.</p><p>With these modiﬁcations, FCAF3D is faster on inference than GSDN.</p><p>修改后的版本运行速度比GSDN快.</p><p>The comparison is shown graphically in follow figure (<span class="math inline">\(mAP@0.5\)</span> scores on ScanNet against scenes per second).</p><p>速度对比结果如下(横轴是每秒预测的场景数量, 纵轴是对应的<span class="math inline">\(mAP@0.5\)</span>分数):</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021429824.png" /></p><p>In performance tests, we opt for implementations <u>based on the MMdetection3D framework</u> to mitigate codebase differences.</p><p>为消除代码实现的不同带来的影响, 所有的速度对比都是基于MMdetection3D框架实现的.</p><p>The reported inference speed for all methods is measured on the same single GPU so they can be directly compared.</p><p>所有的预测都是在同一张单GPU上进行的.</p><h3 id="supplement部分">Supplement部分</h3><h4 id="additional-comments-on-mobius-parametrization部分">Additional Comments on Mobius Parametrization部分</h4><p>The OBB heading angle <span class="math inline">\(\theta\)</span> is typically deﬁned as <u>an angle between x-axis and a vector towards a center of one of OBB faces</u>.</p><p>有向边界框的朝向角一般是<span class="math inline">\(x\)</span>轴和一个面中心点对应的向量的夹角.</p><p><u>If a frontal face exists, then <span class="math inline">\(\theta\)</span> is deﬁned unambiguously</u>; however, this is not the case for some indoor objects.</p><p>如果正面是确定的, 那么<span class="math inline">\(\theta\)</span>不会产生歧义, 但是对大多数室内物体来说, 正面不是确定的.</p><p>If a frontal face cannot be chosen unequivocally, there are <u>four possible representations</u> for a single OBB.</p><p>如果正面无法被确定, 那么对于一个有向边界框将会有四种不同的表达方式.</p><p>The <u>heading angle describes a rotation</u> within the <span class="math inline">\(xy\)</span> plane around <span class="math inline">\(z\)</span>-axis w.r.t. the OBB center.</p><p>朝向角只描述了边界框绕<span class="math inline">\(z\)</span>轴旋转的角度.</p><p>Therefore, the <u>OBB center <span class="math inline">\((x, y, z)\)</span>, height <span class="math inline">\(h\)</span>, and the OBB size <span class="math inline">\(s=w+l\)</span> are the same</u> for all representations.</p><p>因此, 边界框中心<span class="math inline">\((x, y, z)\)</span>, 高<span class="math inline">\(h\)</span>和大小<span class="math inline">\(s=w+l\)</span>对于这四种表达来说是相同的.</p><p>Meanwhile, <u>the ratio <span class="math inline">\(q = \frac{w}{l}\)</span> of the frontal and lateral OBB faces and the heading angle <span class="math inline">\(\theta\)</span> do vary</u>.</p><p>但是比例<span class="math inline">\(q = \frac{w}{l}\)</span>和角度<span class="math inline">\(\theta\)</span>对这四种表达来说是不同的.</p><p>Speciﬁcally, there are <u>four options for the heading angle</u>: <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\theta+\frac{\pi}{2}\)</span>, <span class="math inline">\(\theta+\pi\)</span>, <span class="math inline">\(\theta+\frac{3\pi}{2}\)</span>.</p><p>有四种不同的角度: <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\theta+\frac{\pi}{2}\)</span>, <span class="math inline">\(\theta+\pi\)</span>, <span class="math inline">\(\theta+\frac{3\pi}{2}\)</span>.</p><p>Swapping frontal and lateral faces gives <u>two ratio options</u>: <span class="math inline">\(q\)</span> and <span class="math inline">\(\frac{1}{q}\)</span>.</p><p>有两种不同的比例: <span class="math inline">\(q\)</span> and <span class="math inline">\(\frac{1}{q}\)</span>.</p><p>Overall, there are <u>four different tuples <span class="math inline">\((q, \theta)\)</span> for the same OBB</u>: <span class="math inline">\((q, \theta)\)</span>, <span class="math inline">\(\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right)\)</span>, <span class="math inline">\((q, \theta+\pi)\)</span>, <span class="math inline">\(\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right)\)</span>.</p><p>于是这四种表示分别是: <span class="math inline">\((q, \theta)\)</span>, <span class="math inline">\(\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right)\)</span>, <span class="math inline">\((q, \theta+\pi)\)</span>, <span class="math inline">\(\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right)\)</span>.</p><p>Here, we prove that <u>four different representations of the same OBB map to the same point on a Mobius strip</u>.</p><p>可以证明这四种表示在莫比乌斯环上对应同一个点. <span class="math display">\[\begin{aligned}(q, \theta) \mapsto &amp; (\ln (q) \sin (2 \theta), \ln (q) \cos (2 \theta), \sin (4 \theta), \cos (4 \theta)) \\\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right) \mapsto &amp; \left(\ln \left(\frac{1}{q}\right) \sin (2 \theta+\pi), \ln \left(\frac{1}{q}\right) \cos (2 \theta+\pi), \sin (4 \theta+2 \pi), \cos (4 \theta+2 \pi)\right) \\&amp;=(\ln (q) \sin (2 \theta), \ln (q) \cos (2 \theta), \sin (4 \theta), \cos (4 \theta)) \\(q, \theta+\pi) \mapsto &amp; (\ln (q) \sin (2 \theta+2 \pi)), \ln (q) \cos (2 \theta+2 \pi), \sin (4 \theta+4 \pi), \cos (4 \theta+4 \pi)) \\&amp;=(\ln (q) \sin (2 \theta), \ln (q) \cos (2 \theta), \sin (4 \theta), \cos (4 \theta)) \\\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right) \mapsto &amp; \left(\ln \left(\frac{1}{q}\right) \sin (2 \theta+3 \pi)\right), \ln \left(\frac{1}{q}\right) \cos (2 \theta+3 \pi), \sin (4 \theta+6 \pi), \cos (4 \theta+6 \pi) \\&amp;=(\ln (q) \sin (2 \theta), \ln (q) \cos (2 \theta), \sin (4 \theta), \cos (4 \theta))\end{aligned}\]</span></p><h4 id="per-category-results部分">Per-category results部分</h4><h4 id="visualization部分">Visualization部分</h4><h3 id="精读总结">精读总结</h3><blockquote><p>精读目标及效果自测：所读段落是否详细掌握，回答三个的终极问题(你是谁 - 论文提出/采用什么方法、细节是什么；从哪里来 - 论文要解决什么问题/任务、其启发点或借鉴之处在哪；到哪里去 - 论文方法达到什么效果)。</p></blockquote><p>论文采用稀疏卷积结合类似FCOS的设计, 构建了一个多特征层级的三维室内物体检测器.</p><p>论文使用旋转IoU损失函数实现anchor-free并提升模型的生成能力.</p><p>论文还提出了基于莫比乌斯表达的新型三维边界框参数化模型以解决三维框朝向角歧义的问题.</p><p>论文指出这种新型参数化方法应用到任何一种现有的三维物体检测方法上都有助于提升检测精度.</p><p>论文设计的方案在现有的室内检测数据集上达到了SOTA并且保持较快的预测速度.</p><h2 id="总结">总结</h2><blockquote><p>总结：总览全文，归纳总结，总结文中创新点，关键点，启发点（论文idea的来源或者说是论文的motivation）等重要信息。</p></blockquote><p>论文的创新点、关键点:</p><ul><li>使用多特征层级的方案进行三维目标检测;</li><li>设计新的边界框参数化方法优化预测效果;</li><li>使用旋转IoU损失函数实现anchor-free;</li><li>使用稀疏卷积和剪枝控制模型参数加快预测速度;</li><li>在目前主要的室内三维检测数据集上达到了SOTA.</li></ul><p>论文的启发点:</p><ul><li>FCOS的网络架构;</li><li>GSDN的稀疏卷积;</li><li>ImVoxelNet的旋转IoU损失函数.</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
