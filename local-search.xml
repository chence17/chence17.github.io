<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Text2Mesh: Text-Driven Neural Stylization for Meshes阅读笔记</title>
    <link href="/2022/01/06/Text2Mesh-Text-Driven-Neural-Stylization-for-Meshes%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/01/06/Text2Mesh-Text-Driven-Neural-Stylization-for-Meshes%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="text2mesh-text-driven-neural-stylization-for-meshes阅读笔记">Text2Mesh: Text-Driven Neural Stylization for Meshes阅读笔记</h1><blockquote><p>读论文三步曲：泛读，精读，总结。</p></blockquote><h2 id="泛读">泛读</h2><blockquote><p>泛读：快速浏览，把握概要。重点读标题、摘要、结论、所有小标题。</p></blockquote><h3 id="title部分">Title部分</h3><p>Text2Mesh: <u>Text-Driven</u> Neural <em>Stylization for Meshes</em></p><ul><li>任务: Stylization for Meshes [三维Mesh的风格变换]</li><li>特点: Text-Driven [文本驱动的]</li></ul><h3 id="abstract部分">Abstract部分</h3><p>Our framework, <u>Text2Mesh</u>, <u>stylizes a 3D mesh</u> by <u>predicting color and local geometric details</u> which <u>conform to a target text prompt</u>.</p><p>作者提出的架构Text2Mesh可以通过预测符合目标文本描述的颜色和局部几何细节来变换一个三维mesh的风格.</p><p>We consider <u>a disentangled representation of a 3D object</u> using <u>a ﬁxed mesh input (content)</u> coupled with <u>a learned neural network</u>, which we term <u>neural style field network</u>.</p><p>作者将三维物体的表示方法进行了解耦, 一个三维物体由固定的点云输入(内容)和一个与之相关联的神经网络, 作者称这个神经网络为NSF(Neural Style Field)网络.</p><p>In order to modify style, we obtain <u>a similarity score between a text prompt (describing style) and a stylized mesh</u> by harnessing the representational power of <a href="https://openai.com/blog/clip/">CLIP</a>.</p><p>作者利用<a href="https://openai.com/blog/clip/">CLIP</a>来获取文本描述和mesh风格之间的相似度分数.</p><p>Text2Mesh requires <u>neither a pre-trained generative model nor a specialized 3D mesh dataset</u>.</p><p>Text2Mesh既不需要一个预先训练好的生成模型, 也不需要一个特殊的三维mesh数据集.</p><p>It can handle <u>low-quality meshes</u> (non-manifold, boundaries, etc.) with arbitrary genus, and <u><em>does not require UV parameterization</em></u>.</p><p>Text2Mesh可以处理低质量的任意类型的mesh并且不需要任何UV参数.</p><p>Our code and results are available in our <u>project webpage</u>: <a href="https://threedle.github.io/text2mesh/">https://threedle.github.io/text2mesh/</a>.</p><p>Text2Mesh的项目主页是<a href="https://threedle.github.io/text2mesh/">https://threedle.github.io/text2mesh/</a>.</p><h4 id="clip"><a href="https://openai.com/blog/clip/">CLIP</a></h4><blockquote><p>参考资料: <a href="https://mileistone.github.io/work/2021/01/14/thought-on-connecting-text-and-images/">对Connecting Text and Images的理解</a>、<a href="https://blog.csdn.net/Only_Wolfy/article/details/112675777">CLIP: Connecting Text and Images 介绍</a>、<a href="https://github.com/exacity/deeplearningbook-chinese">Deep Learning 中文翻译</a></p></blockquote><p>动机:</p><ul><li>CV领域数据集标注成本高昂;</li><li>CV模型一般只能胜任一个任务, 迁移到新任务上非常困难;</li><li>CV模型泛化能力较差.</li></ul><p>解决方案:</p><ul><li>互联网上较容易搜集到大量成对的文本和图像, 对于任何一个图像文本对而言, 文本可以认为是图像的标签, 从而解决数据集的问题.</li><li>互联网上存在的这些成对的文本和图像, 数量大且差异大, 当我们在这样的数据集上训练一个表达能力足够强的模型时, 这个模型就能具备较强的泛化能力, 可以较容易迁移到其他新任务上.</li></ul><p>特点:</p><ul><li>zero-shot做得好, 在不同的数据集上表现还可以, 可以自定义任务, 而且效率很高.</li><li>高效, 虽然GPT3做zero-shot也很好, 但是CLIP吃的资源少, 计算量少, 训练效率高.</li><li>灵活和通用, 直接从自然语言中学习广泛的视觉概念, CLIP明显比现有的ImageNet模型更灵活和通用.</li></ul><p>CLIP的zero-shot:</p><ul><li>通过CLIP训练出来一个模型之后，满足以下条件的新任务都可以直接zero-shot进行识别.<ul><li>能够用文字描述清楚这个新分类任务中每个类别;</li><li>这个描述对应的概念在CLIP的训练集中出现过.</li></ul></li><li>CLIP把分类转换为了跨模态检索, 模型足够强的情况下, 检索会比分类扩展性强. CLIP的zero-shot其实就是把分类问题转化为了检索问题.</li><li>CLIP能够zero-shot, 而且效果不错的原因如下.<ul><li>训练集够大, zero-shot任务的图像分布在训练集中有类似的, zero-shot任务的concept在训练集中有相近的;</li><li>将分类问题转换为检索问题.</li></ul></li></ul><p>zero-shot和one-shot:</p><ul><li>只有一个标注样本的迁移任务被称为one-shot学习;</li><li>没有标注样本的迁移任务被称为zero-shot学习.</li></ul><h4 id="uv-parameterization">UV parameterization</h4><blockquote><p>参考资料: <a href="https://en.wikipedia.org/wiki/UV_mapping">UV mapping</a></p></blockquote><p>UV参数是指在进行三维纹理贴图的时候使用UV贴图的时候需要用到的参数.</p><p>UV贴图的过程是将二维的纹理特征转化为UV图, 之后将UV图转换到三维空间上.</p><p>常见的UV贴图是将二维纹理特征转换到三维球的空间上, 之后使用三维纹理球将纹理添加到三维物体上.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201051717992.png" /></p><p>对于三维纹理球上的任何一点<span class="math inline">\(P\)</span>, 假设向量<span class="math inline">\(\vec{d}\)</span>是球心到点<span class="math inline">\(P\)</span>对应的向量, 假设球的两极与<span class="math inline">\(y\)</span>轴对齐, UV图坐标范围为<span class="math inline">\([0, 1]\)</span>, 那么换算关系如下: <span class="math display">\[\begin{aligned}&amp;u=0.5+\frac{\arctan 2\left(d_{x}, d_{z}\right)}{2 \pi} \\&amp;v=0.5-\frac{\arcsin \left(d_{y}\right)}{\pi}\end{aligned}\]</span></p><h3 id="conclusion部分">Conclusion部分</h3><p>It can <u>predict structured textures</u> (e.g. bricks), <u>without a directional field or mesh parameterization</u>.</p><p>Text2Mesh在不需要方向域或mesh参数的情况下可以预测结构化的纹理特征(例如砖块).</p><p>Traditionally, the direction of texture patterns over 3D surfaces has been <u>guided by 3D shape analysis techniques</u>.</p><p>传统上三维物体表面的纹理形式的方向需要使用三维形状分析技术.</p><p>In this work, the texture direction is <u>driven by 2D rendered images</u>, which <u>capture the semantics of how textures appear in the real world</u>.</p><p>Text2Mesh中纹理的方向则是由二维渲染图片决定的, 二维渲染图片可以捕捉到纹理如何出现在真实世界中的这一语义信息.</p><p>Our system is capable of <u>generating out-of-domain stylized outputs</u>.</p><p>Text2Mesh能够生成out-of-domain的不同风格的输出.</p><p>Our framework uses a pre-trained CLIP model, which <u>has been shown to contain bias</u>.</p><p>Text2Mesh使用了一个预先训练的CLIP模型, 这个模型包含了一定的偏置.</p><p>We postulate that our proposed method can be used to <u>visualize, understand, and interpret such model biases in a more direct and transparent way</u>.</p><p>Text2Mesh可以以一种直接而透明的方式可视化和理解这一偏差并与之交互.</p><p>As future work, our framework could be used to <u>manipulate 3D content</u> as well.</p><p>未来, 作者希望Text2Mesh可以利用三维信息.</p><p>Instead of modifying a given input mesh, one could learn to <u>generate meshes from scratch</u> driven by a text prompt.</p><p>作者希望能够直接从零开始生成符合文本描述的mesh, 而不是修改输入的mesh.</p><p>Moreover, our NSF (Neural Style Field) is <u>tailored to a single 3D mesh</u>.</p><p>Text2Mesh是为生成单个mesh量身定制的.</p><p>It may be possible to <u>train a network to stylize a collection of meshes towards a target style</u> in a feed-forward manner.</p><p>未来可以尝试训练一个能够更改一系列mesh到指定风格的前馈神经网络.</p><h3 id="小标题分析">小标题分析</h3><ul><li>Introduction <em>[简介]</em></li><li>Related Work <em>[相关工作]</em></li><li>Method <em>[方法]</em><ul><li>Neural Style Field Network <em><u>[NSF网络]</u></em></li><li>Text-based correspondence <em><u>[基于文本的相关性]</u></em></li><li>Viewpoints and Augmentations <em><u>[视角和增强]</u></em></li></ul></li><li>Experiments <em>[实验]</em><ul><li>Neural Stylization and Controls <em>[风格转换与控制]</em></li><li>Text2Mesh Priors <em><u>[Text2Mesh先验]</u></em></li><li>Stylization Fidelity <em><u>[风格准确性]</u></em></li><li>Beyond Textual Stylization <em>[文本指定的风格之外]</em></li><li>Incorporating Symmetries <em><u>[吸收对称性]</u></em></li><li>Limitations <em><u>[局限性]</u></em></li></ul></li><li>Conclusion <em>[结论]</em></li><li>Supplement <em>[附加材料]</em><ul><li>Additional Results <em>[更多的结果]</em></li><li>High Resolution Stylization <em>[高分辨率的风格转换]</em></li><li>Choice of anchor view <em><u>[锚点视角的选择]</u></em></li><li>Training and Implementation Details <em>[训练和实现细节]</em><ul><li>Network Architecture <em><u>[网络架构]</u></em></li><li>Training <em>[训练]</em></li></ul></li><li>Baseline Comparison and User Study <em>[基线比较和用户调研]</em></li><li>Societal Impact <em>[对社会的冲击]</em></li></ul></li></ul><h3 id="泛读总结">泛读总结</h3><blockquote><p>泛读目标及效果自测：1.论文要解决什么问题? 2.论文采用了什么方法? 3.论文达到什么效果?</p></blockquote><p>论文要解决什么问题? - 基于文本的三维Mesh的风格变换</p><p>论文采用了什么方法? - 将三维物体解耦成固定的mesh和NSF(Neural Style Field)网络, 通过NSF网络来控制和修改mesh的风格. 利用<a href="https://openai.com/blog/clip/">CLIP</a>来获取文本描述和mesh风格之间的相似度分数, 用以监督网络的训练.</p><p>论文达到什么效果? - Text2Mesh可以较为准确的将文本指定的风格应用到输入的mesh上.</p><h2 id="精读">精读</h2><blockquote><p>精读：通过泛读找到需要精读重点部分和快速略过的部分，选出精华，仔细阅读。</p></blockquote><h3 id="introduction部分">Introduction部分</h3><p>We consider <u><em>content</em> as the global structure prescribed by a 3D mesh</u>, which <u>defines the overall shape surface and topology</u>.</p><p>作者考虑选择三维mesh作为全局结构的content, 三维mesh定义了全局的形状表面和拓扑结构.</p><p>We consider <u><em>style</em> as the object’s particular appearance or affect</u>, as <u>determined by its color and ﬁne-grained (local) geometric details</u>.</p><p>作者认为风格是物体特殊的外在形式和效果, 风格由颜色和好的局部细节决定.</p><p>We propose <u>expressing the desired style through natural language (a text prompt)</u>, similar to how a commissioned artist is provided a verbal or textual description of the desired work.</p><p>作者使用自然语言作为选择风格的方式.</p><p><u>A natural cue for modifying the appearance of 3D shapes is through 2D projections</u>, as they correspond with how humans and machines perceive 3D geometry.</p><p>修改三维物体外表的一个自然的想法是通过其二维投影, 因为这和人类和机器观测三维几何结构的方式相关.</p><p>We use a neural network to <u>synthesize color and local geometric details over the 3D input shape</u>, which we refer to as a <em>neural style field</em> (NSF).</p><p>作者使用神经网络来在全局三维物体上合成颜色和局部几何细节, 这些颜色和局部几何细节被称之为神经元风格域(NSF).</p><p>The weights the NSF network are optimized such that <u>the resulting 3D stylized mesh adheres to the style described by text</u>.</p><p>NSF网络的优化目标是让输出的三维mesh的风格与输入的文本相匹配.</p><p>In particular, our neural optimization is guided by <u>multiple 2D (CLIP-embedded) views of the stylized mesh matching our target text</u>.</p><p>NSF网络的优化方式是借助CLIP网络让输出三维mesh的多视角的二维图片的风格与输入文本相匹配.</p><p>Text2Mesh produces <u>color and geometric details</u> over a variety of source meshes, driven by a target text prompt.</p><p>Text2Mesh可以在输入的三维mesh基础上合成符合输入文本描述的颜色和几何细节.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061101377.png" /></p><p>Our method produces <u>different colors and local deformations for the same 3D mesh</u> content to match the speciﬁed text.</p><p>作者提出的方法能够对相同的三维mesh合成不同的颜色和局部变形来满足不同的文本描述.</p><p>Moreover, Text2Mesh produces structured textures that are aligned with salient features, <u>without needing to estimate sharp 3D curves or a mesh parameterization</u>.</p><p>Text2Mesh可以合成与突出特征对齐的结构化的纹理且不需要估计尖锐的三维曲线或mesh参数.</p><p>Given a source mesh (gray), our method produces stylized meshes (<u>containing color and local geometric displacements</u>) which conform to various target texts.</p><p>输入一个mesh, Text2Mesh可以合成颜色和局部的几何变换来满足不同的文本描述.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061103977.png" /></p><p>Our method also demonstrates global understanding; e.g. in following figure human body parts are stylized in accordance with their semantic role.</p><p>Text2Mesh具有全局性的理解能力, 例如在下图中, 人体的各个部位分别与语义描述的对象的各个部位相对应.</p><p>Given the same input bare mesh, our neural style ﬁeld network produces <u>deformations for outerwear of various types</u> (capturing ﬁne details such as creases in clothing and complementary accessories), and <u>distinct features such as muscle and hair</u>.</p><p>输入相同的mesh, Text2Mesh可以为不同的类型的衣物(捕捉到好的细节, 例如衣服的褶皱和装饰物)和显著的特征(例如毛发和肌肉)生成相对应的形变.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061105095.png" /></p><p>We use the weights of the NSF network to encode a stylization (e.g. color and displacements) over the <em>explicit</em> mesh surface.</p><p>作者使用NSF网络来编码在显示的mesh表面的风格样式.</p><p>Meshes faithfully portray 3D shapes and can accurately represent sharp, extrinsic features using a high level of detail.</p><p>三维mesh可以很好的描绘三维形状并且可以利用高层级的细节准确的再现形状和外在特征.</p><p>Our neural style ﬁeld is <u><em>complementary</em> to the mesh content</u>, and appends colors and small displacements to the input mesh.</p><p>NSF是对mesh的一种补充, NSF包含有为mesh加入颜色和微小形变的信息.</p><p>Speciﬁcally, our neural style ﬁeld network <u>maps points on the mesh surface to style attributes</u> (i.e., RGB colors and displacements).</p><p>NSF可以将mesh表面的点与其风格属性进行匹配(例如颜色和偏移).</p><p>We guide the NSF network by <u>rendering the stylized 3D mesh from multiple 2D views and measuring the similarity of those views against the target text</u>, using CLIP’s embedding space.</p><p>作者使用风格化后的三维mesh的多视角图片在CLIP上计算的与目标文本的相似度分数来监督NSF网络的训练.</p><p>However, a straightforward optimization of the 3D stylized mesh which maximizes the CLIP similarity score <u>converges to a degenerate (i.e. noisy) solution</u>.</p><p>直接优化使得基于CLIP的相似度分数最大会让模型收敛到一个退化的结果.</p><p>Speciﬁcally, we observe that the joint text-image embedding space <u>contains an abundance of <em>false positives</em></u>, where a valid target text and a degenerate image (i.e. noise, artifacts) result in a high similarity score.</p><p>作者观察到文本-图像空间包含很多假阳性案例, 意思是一个有效的文本与一张退化的图片(例如有噪音和瑕疵)可以得到一个高的相似度分数.</p><p>Therefore, <u>employing CLIP for stylization requires careful regularization</u>.</p><p>使用CLIP来做风格转换需要很小心的正则化.</p><p>We <u>leverage multiple <em>priors</em> to effectively guide our NSF network</u>.</p><p>作者使用多重先验信息来指导NSF网络的训练.</p><p>The <u>3D mesh input acts as a <em>geometric prior</em></u> that imposes global shape structure, as well as local details that indicate the appropriate position for stylization.</p><p>输入的三维mesh作为包含全局形状结构信息和局部细节的几何先验.</p><p>The <u>weights of the NSF network act as a <em>neural prior</em></u> (i.e. regularization technique), which tends to favor smooth solutions.</p><p>NSF网络的权重作为神经元先验(例如正则化技术)来获取光滑的结果.</p><p>In order to produce accurate styles which contain high-frequency content with high ﬁdelity, we <u>use a frequency-based positional encoding</u>.</p><p>作者使用基于频率的位置编码来产生正确的包含高可信度的高频内容的风格.</p><p>We garner a strong signal about the quality of the neural style ﬁeld by <u>rendering the stylized mesh from multiple 2D views and then applying 2D augmentations</u>.</p><p>作者通过获取三维风格化mesh的多视角图片并将其进行图像增强来获取一个评估NSF生成质量的信号.</p><p>This results in a system which can <u>effectively avoid degenerate solutions</u>, while still <u>maintaining high-ﬁdelity results</u>.</p><p>这能够让系统避免陷入退化的结果同时能够保证高可信度的结果.</p><p>The focus of our work is <u>text-driven stylization</u>, since text is easily modiﬁable and can effectively express complex concepts related to style.</p><p>作者的目标是设计一个文本驱动的风格化网络, 因为文本可以被容易地编辑并且可以表达复杂风格概念.</p><p>Beyond text, our framework extends to <u>additional target modalities, such as images, 3D meshes, or even cross-modal combinations</u>.</p><p>除了文本, 作者提出的架构可以拓展到其他模态, 比如图片、三维mesh甚至跨模态的结合体.</p><p>In summary, we present <u>a technique for the semantic manipulation of style for 3D meshes</u>, harnessing the representational power of CLIP.</p><p>作者提出了一个基于CLIP使用语义信息修改三维mesh风格的技术.</p><p>Our system <u>combines the advantages of <em>explicit</em> mesh surfaces and the generality of neural ﬁelds to facilitate intuitive control for stylizing 3D shapes</u>.</p><p>作者提出的系统融合了显式表达的mesh表面和NSF对于三维形状风格的生成能力的优势.</p><p>A notable advantage of our framework is <u>its ability to handle low-quality meshes (e.g., non-manifold) with arbitrary genus</u>.</p><p>作者提出的系统的另一大优点是能够处理任意类别的低质量的mesh.</p><p>We show that <u>Text2Mesh can stylize a variety of 3D shapes with many different target styles</u>.</p><p>作者展示了Text2Mesh可以将大量不同的三维mesh进行不同的风格化.</p><h3 id="related-work部分">Related Work部分</h3><h4 id="text-driven-manipulation">Text-Driven Manipulation</h4><p>The above techniques (StyleCLIP, StyleGAN, VQGAN-CLIP, etc.) leverage <u>a pre-trained generative network or a dataset</u> to avoid the degenerate solutions common when using CLIP for synthesis.</p><p>这些技术(StyleCLIP、StyleGAN、VQGAN-CLIP等等)都利用预训练模型或者数据集来避免CLIP在合成上带来的退化结果.</p><p>The ﬁrst to leverage CLIP for synthesis <u>without the need for a pre-trained network or dataset</u> is CLIPDraw.</p><p>CLIPDraw是第一个没有使用预训练模型或数据集来避免这一问题的算法.</p><p>Concurrent work uses CLIP to <u>optimize over parameters of the SMPL human body model to create digital creatures</u>.</p><p>最近的一些工作使用CLIP来优化SMPL人体参数来创建数字生物.</p><p>Prior to CLIP, <u>text-driven control for deforming 3D shapes was explored using specialized 3D datasets</u>.</p><p>在CLIP之前, 文本驱动的三维形状控制一般是建立在特殊化的数据集上.</p><h4 id="geometric-style-transfer-in-3d">Geometric Style Transfer in 3D</h4><p>Some approaches <u>analyze 3D shapes and identify similarly shaped geometric elements and parts which differ in style</u>.</p><p>一些方法通过分析三维物体的一致性和差异性来进行几何风格迁移.</p><p>Others transfer geometric style based on <u>content/style separation</u>.</p><p>另一些方法则是基于内容/风格分离的思路.</p><p>Other approaches are <u>speciﬁc to categories</u> of furniture, 3D collages, LEGO, and portraits.</p><p>另一些方法则是基于特定的类别.</p><p>The above methods <u>rely on 3D datasets</u>, while other techniques <u>use a single mesh exemplar for synthesizing geometric textures or producing mesh reﬁnements</u>.</p><p>上述的方法都需要三维数据集, 其他的技术也需要使用一个mesh样例来合成几何纹理和mesh修复.</p><p>Shapes can be edited to contain <u>cubic stylization</u>, or <u>stripe patterns</u>.</p><p>形状可以被修改来包含立方体风格形式或者条纹样式.</p><p>Unlike these methods, we consider <u>a wide range of styles</u>, guided by an intuitive and compact (text) speciﬁcation.</p><p>与这些方法不同, 作者提出的方法能够使用符合直觉的精炼的文字描述来指定生成大范围的不同的风格.</p><h4 id="texture-transfer-in-3d">Texture Transfer in 3D</h4><p>Aspects of a 3D mesh style can be controlled by <u>texturing a surface through mesh parameterization</u>.</p><p>三维mesh的风格可以被mesh参数定义的表面纹理所控制.</p><p>However, most parameterization approaches <u>place strict requirements on the quality of the input mesh</u> (e.g., a manifold, non-intersecting, and low/zero genus), which do not hold for most meshes in the wild.</p><p>但是这种方式对mesh的质量要求很高, 大多数mesh达不到这种要求.</p><p>We avoid parameterization altogether and opt to modify appearance using a neural ﬁeld which <u>provides a style value (i.e., an RGB value and a displacement) for every vertex on the mesh</u>.</p><p>作者提出的方法避免了这一问题, 作者提出的方法直接使用NFS为每个mesh顶点生成对应的风格数值(颜色和偏移)从而达到修改mesh外表的目的.</p><p>Recent work explored a neural representation of texture, here we consider <u>both color and local geometry changes</u> for the manipulation of style.</p><p>近期的一些工作探索了纹理的神经元表示, 作者同时考虑与风格对应的颜色和几何变化.</p><h4 id="neural-priors-and-neural-fields">Neural Priors and Neural Fields</h4><p>Our framework <u>leverages the inductive bias of neural networks to act as a prior which guides Text2Mesh away from degenerate solutions present in the CLIP embedding space</u>.</p><p>作者提出的架构利用神经网络的偏置作为先验信息来防止Text2Mesh收敛到一个退化的结果.</p><p>Speciﬁcally, our stylization network acts as a neural prior, which <u>leverages positional encoding to synthesize ﬁne-grained stylization details</u>.</p><p>作者提出的网络可以利用位置编码来合成好的风格化的细节.</p><p>NeRF and follow ups have demonstrated success on 3D scene modeling.</p><p>NeRF及其后继者在三维场景建模领域获得了巨大的成功.</p><p>They leverage a neural ﬁeld to represent 3D objects using network weights.</p><p>他们利用神经元域使用网络权重来表示三维物体.</p><p>However, <u>neural ﬁelds commonly entangle geometry and appearance, which limits separable control of content and style</u>.</p><p>神经元域通常会融合几何和外表特征, 这限制了分别控制这两种特征的内容和风格的能力.</p><p>Moreover, <u>they struggle to accurately portray sharp features, are slow to render, and are difﬁcult to edit</u>.</p><p>神经元域还会生成很锐利的特征, 这些特征渲染速度很慢并且很难编辑.</p><p>Instead, our method <u>uses a disentangled representation of a 3D object using an explicit mesh representation of shape and a neural style ﬁeld which controls appearance</u>.</p><p>作者提出的方法使用解耦的表达, 将物体分为显式的mesh形状和基于NSF的外表.</p><p>This formulation <u>avoids parametrization</u>, and can be used to <u>easily manipulate appearance and generate high resolution outputs</u>.</p><p>这种方式避免了参数化并且可以轻松的控制外表和生成高分辨率的结果.</p><h3 id="method部分">Method部分</h3><p>Text2Mesh <u>modiﬁes an input mesh to conform to the target text</u> by predicting color and geometric details.</p><p>Text2Mesh通过修改输入mesh的几何特征和颜色来使之满足目标文本的需求.</p><p><u>The weights of the neural style network are optimized by rendering multiple 2D images and applying 2D augmentations</u>, which are given a similarity score to the target from the CLIP-based semantic loss.</p><p>通过渲染多幅二维图像并使用图像增强技术来优化NSF网络的权重, 这是通过基于CLIP的语义损失函数实现的.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061421943.png" /></p><p>As an overview, <u>the 3D object <em>content</em> is deﬁned by an input mesh <span class="math inline">\(M\)</span> with vertices <span class="math inline">\(V \in \mathbb{R}^{n \times 3}\)</span> and faces <span class="math inline">\(F \in\{1, \ldots, n\}^{m \times 3}\)</span>, and is ﬁxed throughout training</u>.</p><p>三维物体内容是由输入mesh定义的, 其顶点<span class="math inline">\(V \in \mathbb{R}^{n \times 3}\)</span>, 面片<span class="math inline">\(F \in\{1, \ldots, n\}^{m \times 3}\)</span>, 这在整个训练过程中都是固定的.</p><p><u>The object’s style (color and local geometry) is modiﬁed to conform to a target text prompt <span class="math inline">\(t\)</span></u>, resulting in a stylized mesh <span class="math inline">\(M^S\)</span>.</p><p>生成的物体风格需要符合目标文本<span class="math inline">\(t\)</span>的描述, 记格式化的mesh为<span class="math inline">\(M^S\)</span>.</p><p>The NSF learns to <u>map points on the mesh surface <span class="math inline">\(p \in V\)</span> to an RGB color and displacement along the normal direction</u>.</p><p>NSF需要学习mesh的表面点<span class="math inline">\(p \in V\)</span>与最终风格的颜色和沿着法线方向的偏移量之间的映射.</p><p>We <u>render <span class="math inline">\(M^S\)</span> from multiple views and apply 2D augmentations</u> that are embedded using CLIP.</p><p>作者使用多视角渲染<span class="math inline">\(M^S\)</span>并使用增强技术增强图片, 之后将其嵌入到CLIP空间中.</p><p>The <u>CLIP similarity between the rendered and augmented images and the target text is used as a signal to update the neural network weights</u>.</p><p>将渲染图片和增强图片与目标文本之间的CLIP相似度分数用作更新神经网络权重的信号.</p><h4 id="neural-style-field-network部分">Neural Style Field Network部分</h4><p>Our NSF network <u>produces a style attribute for every vertex which results in a <em>style ﬁeld</em> deﬁned over the entire shape surface</u>.</p><p>NSF网络为风格域上的每个表面点分别计算风格属性.</p><p>Our style ﬁeld is represented as an MLP, which <u>maps a point <span class="math inline">\(p \in V\)</span> on the mesh surface <span class="math inline">\(M\)</span> to a color and displacement along the surface normal <span class="math inline">\((c_p, d_p) \in (\mathbb{R}^{3}, \mathbb{R})\)</span></u>.</p><p>风格域被一个多重感知器描述, 将mesh的面上一点<span class="math inline">\(p \in V\)</span>与颜色和沿着表面法线方向的偏移<span class="math inline">\((c_p, d_p) \in (\mathbb{R}^{3}, \mathbb{R})\)</span>建立映射关系.</p><p>In practice, we <u>treat the given vertices of <span class="math inline">\(M\)</span> as query points into this ﬁeld</u>, and <u>use a differentiable renderer to visualize the style over the given triangulation</u>.</p><p>作者将<span class="math inline">\(M\)</span>中的表面点视作在风格域中进行查询的点, 使用一个差分渲染器来可视化输出的风格.</p><p><u>Increasing the number of triangles in <span class="math inline">\(M\)</span></u> for the purposes of learning a neural ﬁeld with ﬁner granularity is trivial.</p><p>可以尝试增加<span class="math inline">\(M\)</span>中三角面片的数量来获取更好的表现.</p><p>Even using a standard GPU (11GB of VRAM) our method handles meshes with up to 180K triangles.</p><p>使用一个常规的GPU, 作者的方法能够处理180K个三角面片.</p><p>Since our NSF uses low-dimensional coordinates as input to an MLP, <u>this exhibits a spectral bias toward smooth solutions</u>.</p><p>由于NSF使用低维度的坐标作为多重感知器的输入, 因此会有倾向于光滑结果的频谱偏置.</p><p>To synthesize high-frequency details, we <u>apply a positional encoding using fourier feature mappings</u>, which enables MLPs to overcome the spectral bias and learn to interpolate high-frequency functions.</p><p>为了能够合成高频信息, 作者使用了傅里叶特征的位置编码, 这帮助多重感知器克服频谱偏置并学习到高频信息.</p><p><u>For every point <span class="math inline">\(p\)</span> its positional encoding <span class="math inline">\(\gamma(p)\)</span> is given by:</u></p><p>对于任意一点<span class="math inline">\(p\)</span>, 其位置编码<span class="math inline">\(\gamma(p)\)</span>定义如下: <span class="math display">\[\gamma(p)=[\cos (2 \pi \mathbf{B} p), \sin (2 \pi \mathbf{B} p)]^{\mathrm{T}}\]</span></p><p>where <u><span class="math inline">\(B \in \mathbb{R}^{n \times 3}\)</span> is a random Gaussian matrix where each entry is randomly drawn from <span class="math inline">\(\mathcal{N}\left(0, \sigma^{2}\right)\)</span></u>.</p><p>其中<span class="math inline">\(B \in \mathbb{R}^{n \times 3}\)</span>是一个高斯随机矩阵, 其分布由<span class="math inline">\(\mathcal{N}\left(0, \sigma^{2}\right)\)</span>确定.</p><p><u>The value of <span class="math inline">\(\sigma\)</span> is chosen as a hyperparameter which controls the frequency of the learned style function.</u></p><p><span class="math inline">\(\sigma\)</span>是一个控制风格频率的超参数.</p><p>First, we <u>normalize the coordinates <span class="math inline">\(p \in V\)</span> to lie inside a unit bounding box</u>.</p><p>首先, 作者将<span class="math inline">\(p \in V\)</span>归一化到一个单位大小的正方体内.</p><p>Then, the <u>per-vertex positional encoding features <span class="math inline">\(\gamma(p)\)</span> are passed as input to an MLP <span class="math inline">\(N_s\)</span>, which then branches out to MLPs <span class="math inline">\(N_d\)</span> and <span class="math inline">\(N_c\)</span></u>.</p><p>之后, 每个顶点的位置编码<span class="math inline">\(\gamma(p)\)</span>会被传到多重感知器<span class="math inline">\(N_s\)</span>里, 然后会分出<span class="math inline">\(N_d\)</span>和<span class="math inline">\(N_c\)</span>两个多重感知器.</p><p>Speciﬁcally, <u>the output of <span class="math inline">\(N_c\)</span> is a color <span class="math inline">\(c_p \in [0, 1]^3\)</span></u>, and <u>the output of <span class="math inline">\(N_d\)</span> is a displacement along the vertex normal <span class="math inline">\(d_p\)</span></u>.</p><p><span class="math inline">\(N_c\)</span>的输出是颜色<span class="math inline">\(c_p \in [0, 1]^3\)</span>, <span class="math inline">\(N_d\)</span>的输出是沿着顶点的法线方向的位移量<span class="math inline">\(d_p\)</span>.</p><p>To <u>prevent content-altering displacements</u>, we constrain <span class="math inline">\(d_p\)</span> to be in the range <span class="math inline">\((−0.1, 0.1)\)</span>.</p><p>为了防止越界和形状变化过大, 作者限制<span class="math inline">\(d_p\)</span>的范围为<span class="math inline">\((−0.1, 0.1)\)</span>.</p><p>To obtain our stylized mesh prediction <span class="math inline">\(M^S\)</span>, <u>every point <span class="math inline">\(p\)</span> is displaced by <span class="math inline">\(d_p \cdot \vec{n}_p\)</span> and colored by <span class="math inline">\(c_p\)</span></u>.</p><p>为了获取最终的<span class="math inline">\(M^S\)</span>, 所有的点<span class="math inline">\(p\)</span>都需要位移<span class="math inline">\(d_p \cdot \vec{n}_p\)</span>并更新颜色<span class="math inline">\(c_p\)</span>.</p><p>Vertex colors propagate over the entire mesh surface <u>using an interpolation-based differentiable renderer</u>.</p><p>顶点颜色使用基于插值的差分渲染的方式传播到整个表面.</p><p>During training we also consider <u>the displacement-only mesh <span class="math inline">\(M_{\text{displ}}^S\)</span>, which is the same as <span class="math inline">\(M^S\)</span> without the predicted vertex colors (replaced by gray)</u>.</p><p>在训练期间, 作者也考虑仅有偏移量的<span class="math inline">\(M_{\text{displ}}^S\)</span>, 其与<span class="math inline">\(M^S\)</span>的几何结构一模一样只是没有预测的颜色(全部被灰色替代).</p><p>Without the use of <span class="math inline">\(M_{\text{displ}}^S\)</span> in our ﬁnal loss formulation, <u>the learned geometric style is noisier</u>.</p><p>如果不在损失函数中使用<span class="math inline">\(M_{\text{displ}}^S\)</span>会导致生成结果的几何结构包含许多噪音.</p><h4 id="text-based-correspondence部分">Text-based correspondence部分</h4><p>Our neural optimization is guided by the <u>multi-modal embedding space provided by a pre-trained CLIP model</u>.</p><p>作者提出的网络的优化是基于将多模态表示使用预训练的CLIP模型嵌入到CLIP空间的方式进行的.</p><p>Given the stylized mesh <span class="math inline">\(M^S\)</span> and the displaced mesh <span class="math inline">\(M_{\text{displ}}^S\)</span>, we <u>sample <span class="math inline">\(n_\theta\)</span> views around a pre-deﬁned anchor view and render them using a differentiable renderer</u>.</p><p>对于格式化后的mesh输出<span class="math inline">\(M^S\)</span>及其对应的几何结构<span class="math inline">\(M_{\text{displ}}^S\)</span>, 作者基于预先定义好的锚点视角随机采样<span class="math inline">\(n_\theta\)</span>个视角, 根据这些视角使用差分渲染器获取渲染的图片.</p><p>For each view, <span class="math inline">\(\theta\)</span>, we <u>render two 2D projections of the surface, <span class="math inline">\(I_{\theta}^{\text{full}}\)</span> for <span class="math inline">\(M^S\)</span> and <span class="math inline">\(I_{\theta}^{\text{displ}}\)</span> for <span class="math inline">\(M_{\text{displ}}^S\)</span></u>.</p><p>对每个视角<span class="math inline">\(\theta\)</span>, 作者获取两张渲染图, <span class="math inline">\(M^S\)</span>对应的<span class="math inline">\(I_{\theta}^{\text{full}}\)</span>和<span class="math inline">\(M_{\text{displ}}^S\)</span>对应的<span class="math inline">\(I_{\theta}^{\text{displ}}\)</span>.</p><p>Next, we draw <u>a 2D augmentation <span class="math inline">\(\psi_{\text{global}} \in \Psi_{\text{global}}\)</span> and <span class="math inline">\(\psi_{\text{local}} \in \Psi_{\text{local}}\)</span></u>.</p><p>之后, 作者使用两个图像增强方法<span class="math inline">\(\psi_{\text{global}} \in \Psi_{\text{global}}\)</span>和<span class="math inline">\(\psi_{\text{local}} \in \Psi_{\text{local}}\)</span>.</p><p>We <u>apply <span class="math inline">\(\psi_{\text{global}}\)</span>, <span class="math inline">\(\psi_{\text{local}}\)</span> to the full view and <span class="math inline">\(\psi_{\text{local}}\)</span> to the uncolored view, and embed them into CLIP space</u>.</p><p>作者将<span class="math inline">\(\psi_{\text{global}}\)</span>和<span class="math inline">\(\psi_{\text{local}}\)</span>应用到<span class="math inline">\(I_{\theta}^{\text{full}}\)</span>, 将<span class="math inline">\(\psi_{\text{local}}\)</span>应用到<span class="math inline">\(I_{\theta}^{\text{displ}}\)</span>, 并将增强后的结果嵌入到CLIP空间.</p><p>Finally, we <u>average the embeddings across all views</u>:</p><p>最后, 作者计算所有视角嵌入CLIP空间的平均值. <span class="math display">\[\begin{aligned}\hat{S}^{\text {full }} &amp;=\frac{1}{n_{\theta}} \sum_{\theta} E\left(\psi_{\text {global }}\left(I_{\theta}^{\text {full }}\right)\right) \in \mathbb{R}^{512} \\\hat{S}^{\text {local }} &amp;=\frac{1}{n_{\theta}} \sum_{\theta} E\left(\psi_{\text {local }}\left(I_{\theta}^{\text {full }}\right)\right) \in \mathbb{R}^{512} \\\hat{S}^{\text {displ }} &amp;=\frac{1}{n_{\theta}} \sum_{\theta} E\left(\psi_{\text {local }}\left(I_{\theta}^{\text {displ }}\right)\right) \in \mathbb{R}^{512}\end{aligned}\]</span></p><p>That is, we <u>consider an augmented representation of our input mesh as the average of its encoding from multiple augmented views</u>.</p><p>作者将输入mesh的一个增强的表现形式看作其多个增强后的视角编码的平均值.</p><p>The <u>target <span class="math inline">\(t\)</span> is similarly embedded through CLIP by <span class="math inline">\(\phi_{\text{target}} = E (t) \in \mathbb{R}^{512}\)</span></u>.</p><p>目标<span class="math inline">\(t\)</span>也类似的嵌入CLIP空间<span class="math inline">\(\phi_{\text{target}} = E (t) \in \mathbb{R}^{512}\)</span>.</p><p>Our loss is then:</p><p>损失函数如下: <span class="math display">\[\mathcal{L}_{\mathrm{sim}}=\sum_{\hat{S}} \operatorname{sim}\left(\hat{S}, \phi_{\mathrm{target}}\right)\]</span> where <span class="math inline">\(\hat{S} \in\left\{\hat{S}^{\text {full }}, \hat{S}^{\text {displ }}, \hat{S}^{\text {local }}\right\}\)</span> and <span class="math inline">\(\operatorname{sim}(a, b)=\frac{a \cdot b}{|a| \cdot|b|}\)</span> is the <u>cosine similarity between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span></u>.</p><p>其中<span class="math inline">\(\hat{S} \in\left\{\hat{S}^{\text {full }}, \hat{S}^{\text {displ }}, \hat{S}^{\text {local }}\right\}\)</span>, 并且<span class="math inline">\(\operatorname{sim}(a, b)=\frac{a \cdot b}{|a| \cdot|b|}\)</span>是<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>之间的余弦相似度.</p><p>We <u>repeat the above with new sampled augmentations <span class="math inline">\(n_{\text{aug}}\)</span> times for each iteration</u>.</p><p>每次迭代, 作者重复上述的采样增强过程<span class="math inline">\(n_{\text{aug}}\)</span>次.</p><p>We note that the terms <u>using <span class="math inline">\(\hat{S}^{\text{full}}\)</span> and <span class="math inline">\(\hat{S}^{\text{local}}\)</span> update <span class="math inline">\(N_s\)</span>, <span class="math inline">\(N_c\)</span> and <span class="math inline">\(N_d\)</span> while the term using <span class="math inline">\(\hat{S}^{\text{displ}}\)</span> only updates <span class="math inline">\(N_s\)</span> and <span class="math inline">\(N_d\)</span></u>.</p><p>作者使用<span class="math inline">\(\hat{S}^{\text{full}}\)</span>和<span class="math inline">\(\hat{S}^{\text{local}}\)</span>更新<span class="math inline">\(N_s\)</span>、<span class="math inline">\(N_c\)</span>和<span class="math inline">\(N_d\)</span>, 使用<span class="math inline">\(\hat{S}^{\text{displ}}\)</span>只更新<span class="math inline">\(N_s\)</span>和<span class="math inline">\(N_d\)</span>.</p><p>The <u>separation into a geometry-only loss and geometry-and-color loss is an effective tool for encouraging meaningful changes in geometry</u>.</p><p>分别计算纯几何损失函数和几何及颜色损失函数是一个有效促进有意义的几何变化的方式.</p><h4 id="viewpoints-and-augmentations部分">Viewpoints and Augmentations部分</h4><p>Given an input 3D mesh and target text, we ﬁrst ﬁnd an <u>anchor view</u>.</p><p>对于给定的三维mesh和目标文本, 先要找到锚定视角.</p><p>We render the 3D mesh at <u>uniform intervals around a sphere</u> and <u>obtain the CLIP similarity for each view and target text</u>.</p><p>首先按照均匀间隔绕着一个球用不同的视角渲染三维mesh, 然后将渲染的图片与目标文本进行匹配计算CLIP相似度分数.</p><p>We <u>select the view with the highest (i.e. best) CLIP similarity as the anchor view</u>.</p><p>选择分数最高的视角作为锚点视角.</p><p>Often there are <u>multiple high-scoring views around the object</u>, and using any of them as the anchor will produce an effective and meaningful stylization.</p><p>通常有很多个高分视角, 从中任意选取一个即可.</p><p>We <u>render multiple views of the object from randomly sampled views using a Gaussian distribution centered around the anchor view (with <span class="math inline">\(\sigma=\pi / 4\)</span>)</u>.</p><p>在获取生成mesh的渲染图片时, 以锚点为均值, <span class="math inline">\(\pi / 4\)</span>为方差, 计算高斯分布, 随机采样多个视角进行渲染.</p><p>We <u>average over the CLIP-embedded views prior</u> to feeding them into our loss, which encourages the network to leverage view consistency.</p><p>将这些视角的图片嵌入到CLIP空间中并求取平均值, 然后放入损失函数中, 这样有助于网络利用视角的一致性.</p><p>For all our experiments, <u><span class="math inline">\(n_{\theta}=5\)</span> (number of sampled views)</u>.</p><p>对于所有的实验, 采样的视角数设置为<span class="math inline">\(n_{\theta}=5\)</span>.</p><p>The 2D augmentations generated using <span class="math inline">\(\psi_{\text{global}}\)</span> and <span class="math inline">\(\psi_{\text{local}}\)</span> are critical for our method to <u>avoid degenerate solutions</u>.</p><p>由<span class="math inline">\(\psi_{\text{global}}\)</span>和<span class="math inline">\(\psi_{\text{local}}\)</span>定义的二维增强方法对于防止退化的情况至关重要.</p><p><span class="math inline">\(\psi_{\text{global}}\)</span> involves a random perspective transformation and <span class="math inline">\(\psi_{\text{local}}\)</span> generates both a random perspective and a random crop that is <span class="math inline">\(10\%\)</span> of the original image.</p><p><span class="math inline">\(\psi_{\text{global}}\)</span>包括一个随机的透视变换. <span class="math inline">\(\psi_{\text{local}}\)</span>包括一个随机的透视变换和一个随机裁剪原图的<span class="math inline">\(10\%\)</span>的子图.</p><p>Cropping allows the network to <u>focus on localized regions</u> when making ﬁne grained adjustments to the surface geometry and color.</p><p>裁剪有助于帮助网络获取局部的细节用于优化表面的几何结构和颜色.</p><h3 id="experiments部分">Experiments部分</h3><p>We consider a variety of sources including: <u>COSEG, Thingi10K, Shapenet, Turbo Squid, and ModelNet</u>.</p><p>作者考虑了一批不同来源的数据: COSEG、Thingi10K、Shapenet、Turbo Squid和ModelNet.</p><p>Our method requires <u>no particular quality constraints or preprocessing of inputs</u>, and the breadth of shapes we stylize in this paper and in our project webpage illustrates its ability to handle low-quality meshes.</p><p>作者提出的方法对于输入mesh没有任何限制, 因此即使是低质量的mesh也可以被处理.</p><p>Our method takes <u>less than 25 minutes to train on a single GPU</u>, and <u>high quality results usually appear in less than 10 minutes</u>.</p><p>作者的方法只需要不到25分钟即可在单张GPU上训练好, 预测高质量的结果也只需要不到10分钟.</p><h4 id="neural-stylization-and-controls部分">Neural Stylization and Controls部分</h4><h5 id="fine-grained-controls">Fine Grained Controls</h5><p>Our network leverages a <u>positional encoding where the range of frequencies can be directly controlled by the standard deviation <span class="math inline">\(\sigma\)</span> of the <span class="math inline">\(\mathbf{B}\)</span> matrix</u>.</p><p>作者使用了位置编码, 因此纹理频率可以直接由<span class="math inline">\(\mathbf{B}\)</span>矩阵的标准差<span class="math inline">\(\sigma\)</span>控制.</p><p>In following figure, we show the results of <u>three different frequency values</u> when stylizing a source mesh of a torus towards the target text 'stained glass donut'.</p><p>下图展示了不同频率值对于相同的输入的影响, 文本输入是"彩色玻璃甜甜圈".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062111336.png" /></p><p>Increasing the frequency value <u>increases the frequency of style details</u> on the mesh and <u>produces sharper and more frequent displacements along the normal direction</u>.</p><p>增大频率值增加了纹理的出现频率也让mesh表面更加锐利, 沿着法线方向的偏移出现地也越频繁.</p><p>We further demonstrate our method’s ability to successfully <u>synthesize styles of varying levels of speciﬁcity</u>.</p><p>作者提出的方法还能够生成不同层级地风格.</p><p><u>Increasing the target text prompt granularity for a source mesh of a lamp and iron.</u> Top row targets: (a). 'Lamp', (b).'Luxo lamp', (c).'Blue steel luxo lamp', (d).'Blue steel luxo lamp with corrugated metal'. Bottom row targets: (a).'Clothes iron', (b).'Clothes iron made of crochet', (c).'Golden clothes iron made of crochet', (d).'Shiny golden clothes iron made of crochet'.</p><p>增加灯和熨斗的目标文本的层级。第一行: (a)."灯", (b)."Luxo灯", (c)."蓝色钢制Luxo灯", (d)."带波纹金属的蓝色钢制Luxo灯". 最后一行：(a)."衣服熨斗", (b)."用钩针制成的熨斗", (c)."用钩针制成的金熨斗", (d)."用钩针制成的闪亮的金色衣服的熨斗".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062113223.png" /></p><p>Though the primary mode of style control is through the <u>text prompt</u>, we explore the way the network adapts to <u>the geometry of the source shape</u>.</p><p>虽然样式控制主要是通过文本进行的, 但输入的mesh形状也有影响.</p><p>In following figure, the target text prompt is ﬁxed to 'cactus'.</p><p>在下图中, 文本描述被限定为"仙人掌".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062115512.png" /></p><p>We consider different input source spheres with <u>increasing protrusion frequency</u>.</p><p>突起频率的增加的不同的球形mesh进行输入.</p><p>Observe that both the frequency and structure of the generated style changes to <u>align with the pre-existing structure of the input surface</u>.</p><p>这些不同球形mesh的输出各不相同, 且输出的突起频率与输入类似.</p><p>This shows that our method has the ability to <u>preserve the content of the input mesh without compromising the quality of the stylization</u>.</p><p>这表明作者的方法能够保留输入mesh的结构, 而不会影响样式化的质量.</p><p>Meshes with corresponding connectivity can be used to <u>morph between two surfaces</u>.</p><p>具有相应连接性的网格可用于在两个曲面之间变形.</p><p>Thus, <u>our ability to modify style while preserving the input mesh enables morphing</u>.</p><p>因此, 作者提出的方法能够在保留输入mesh结构的同时修改样式, 从而可以实现变形.</p><p>To morph between meshes, we apply <u>linear interpolation between the style value</u> (RGB and displacement) of every point on the mesh, for each instance of the stylized mesh.</p><p>在两个不同的风格的mesh之间使用线性插值的方式(对每个点的颜色和偏移量进行插值)获得两个不同风格之间的渐变效果.</p><p>Morphing between two different stylizations (geometry and color). Left:'wooden chair', right:'colorful crochet chair'.</p><p>两种不同样式(几何结构和颜色)之间的变形.左: "木椅", 右: "彩色钩针椅".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062118366.png" /></p><h4 id="text2mesh-priors部分">Text2Mesh Priors部分</h4><p>Our method incorporates a number of priors that allow us to perform stylization <u>without a pre-trained GAN</u>.</p><p>作者提出的方法使用了一系列先验, 因此不需要使用预训练GAN.</p><p><u>Ablation on the priors used in our method (full) for a candle mesh and target 'Candle made of bark'</u>: w/o our style ﬁeld network (−net), w/o 2D augmentations (−aug), w/o positional encoding (−FFN), w/o crop augmentations for <span class="math inline">\(\psi_{\text{local}}\)</span> (−crop), w/o the geometry-only component of <span class="math inline">\(L_{\text{sim}}\)</span> (−displ), and learning over a 2D plane in 3D space (−3D). We show the CLIP score (<span class="math inline">\(\text{sim}(\hat{S}^{\text{full}}, \phi_{\text{target}})\)</span>).</p><p>先验信息的消融实验结果, 输入时一个蜡烛的mesh和风格文本"树皮做的蜡烛": 没有NSF网络(-net)、没有二维图像增强(-aug)、没有位置编码(-FFN)、没有裁剪的<span class="math inline">\(\psi_{\text{local}}\)</span>(-crop)、没有几何结构损失函数<span class="math inline">\(L_{\text{sim}}\)</span>(-displ)和只学习二维平面(-3D). 通过<span class="math inline">\(\text{sim}(\hat{S}^{\text{full}}, \phi_{\text{target}})\)</span>计算的CLIP分数在图中最下面展示.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062043618.png" /></p><p>Removing the style ﬁeld network (−net), and instead directly optimizing the vertex colors and displacements, results in <u>noisy and arbitrary displacements over the surface</u>.</p><p>移除NSF网络会导致表面随机且充满噪音的出现偏移.</p><p><u>Random 2D augmentations are necessary</u> to generate meaningful CLIP-guided drawings.</p><p>随机的二维增强对于利用CLIP来说时十分重要的.</p><p>We observe the same phenomena in our method, whereby removing 2D augmentations results in <u>a stylization completely unrelated to the target text prompt</u>.</p><p>在没有使用图像增强的情况下, 风格化的结果与目标文本描述相差甚远.</p><p>Without fourier feature encoding (−FFN), the generated style <u>loses all ﬁne-grained details</u>.</p><p>不使用位置编码会导致所有的细节缺失.</p><p>With the cropping augmentation removed (−crop), the output is similarly <u>unable to synthesize the ﬁne-grained style details</u> that deﬁne the target.</p><p>不进行局部裁剪会导致无法生成满足目标文本描述的细节.</p><p>Removing the geometry-only component of <span class="math inline">\(L_{\text{sim}}\)</span> (−displ) <u>hinders geometric reﬁnement</u>, and the network instead compensates by simulating geometry through shading.</p><p>不使用几何损失函数导致几何结构的优化出现问题, 网络倾向于使用阴影来模拟几何结构.</p><p>Without a geometric prior (−3D) there is no source mesh to impose global structure, thus, <u>the 2D plane in 3D space is treated as an image canvas</u>.</p><p>在没有三维输入的时候, 三维空间的一个二维平面被视为一张图像的画布.</p><p>Our method obtains the <u>highest score</u> across different ablations.</p><p>在消融实验中, 作者提出的方法获得了最高的分数.</p><p>Ideally, there is <u>a correlation between visual quality and CLIP scores</u>.</p><p>在理想情况下, 视觉效果和CLIP分数存在一定的相关性.</p><p>However, <u>-3D manages to achieve a high CLIP similarity</u>, despite its zero regard for global content semantics.</p><p>然而, 在没有三维输入的情况下, 仍然能够获得很高的CLIP相似度分数.</p><p>This shows an example of <u>how CLIP may naively prefer degenerate solutions</u>, while our geometric prior steers our method away from these solutions.</p><p>这从侧面展示了CLIP倾向于退化的结果, 但是作者提出的先验有效避免了这个.</p><h5 id="interplay-of-geometry-and-color">Interplay of Geometry and Color</h5><p>Our method utilizes the <u>interplay between geometry and color for effective stylization</u>.</p><p>作者提出的方法有效利用了颜色和几何特征.</p><p>Interplay between geometry and color for stylization. <em>Full</em> - our method, <em>Color</em> - only color changes, and <em>Geometry</em> - only geometric changes. We also display the CLIP similarity.</p><p>下图展示了几何结构和颜色在风格转换中的作用. <em>Full</em>代表全部使用, <em>Color</em>代表只使用颜色变化, <em>Geometry</em>代表只使用几何变化. CLIP相似度分数也在最下面展示.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062050060.png" /></p><p>Learning to predict only geometric manipulations produces inferior geometry compared to learning geometry and color together, as the network <u>attempts to simulate shading by generating displacements for self shadowing</u>.</p><p>只是用几何特征相比于使用颜色和几何特征表现更差, 因为在只使用几何特征的情况下, 网络倾向于生成几何纹理来模拟阴影.</p><p>Similarly learning to predict only color results in the network <u>attempting to hallucinate geometric detail through shading</u>, leading to a ﬂat and unrealistic texture that nonetheless is capable of achieving a relatively high CLIP score when projected to 2D.</p><p>只是用颜色特征也表现不好, 网络会错误的将阴影认为时几何特征, 从而导致几何结构的缺失. 因为在光滑的表面进行投影能够获取更高的CLIP分数.</p><h4 id="stylization-fidelity部分">Stylization Fidelity部分</h4><p>Our method performs the task of <u>general text-driven stylization of meshes</u>.</p><p>作者提出的方法是基于文本的mesh风格化.</p><p>Given that no approaches exist for this task, we evaluate our method’s performance by <u>extending VQGAN-CLIP</u>.</p><p>由于没有其他的现存算法针对这个任务, 作者拓展了VQGAN-CLIP算法作为基准.</p><p>This baseline <u>synthesizes color inside a binary 2D mask projected from the 3D source shape</u> (without 3D deformations) guided by CLIP.</p><p>这个基准通过CLIP指导合成从三维物体投影的二维二值模板内部的颜色.</p><p>Further, the baseline is <u>initialized with a rendered view of the 3D source</u>.</p><p>这个基准需要使用三维物体的一个视角的渲染图片来初始化.</p><p>We conduct a user study to <u>evaluate the perceived quality of the generated outputs, the degree to which they preserve the source content, and how well they match the target style</u>.</p><p>作者进行了一次用户调研来评估生成mesh的质量、生成mesh在多大程度上保持了源物体的结构以及生成的mesh在多大程度上满足了格式化的要求.</p><p>We had <u>57 users evaluate 8 random source meshes and style text prompt combinations</u>.</p><p>一共有57个用户在8组不同的结果上进行测试.</p><p>For each combination, we <u>display the target text and the stylized output in pairs</u>.</p><p>作者在调研的时候成对地放置风格文本和输出地mesh.</p><p>The users are then asked to assign a score (1-5) to three factors:</p><ul><li>(Q1) "How natural is the output depiction of {content} + {style}?"</li><li>(Q2) "How well does the output match the original {content}?"</li><li>(Q3) "How well does the output match the target {style}?"</li></ul><p>用户使用分数1到5回答下列问题:</p><ul><li>输出在多大程度上自然地描述了{输入点云}+{风格文本}?</li><li>输出在多大程度上与原始{输入点云}保持一致?</li><li>输出在多大程度上与{风格文本}地描述保持一致?</li></ul><p>We report the mean opinion <u>scores with standard deviations</u> in parentheses for each factor averaged across all style outputs for our method and the baseline in following table.</p><p>下表显示了用户调研地结果, 作者地方法和基线地方法在每个问题上地分数通过均值(标准差)的形式表示.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062019507.png" /></p><p>We include <u>three control questions where the images and target text do not match</u>, and obtain a mean control score of <span class="math inline">\(1.16\)</span>.</p><p>作者设置了控制问题(图像和目标文本完全不匹配的例子), 这些控制问题的得分是<span class="math inline">\(1.16\)</span>.</p><p>Our method <u><em>outperforms the VQGAN baseline across all questions</em></u>, with a difference of <span class="math inline">\(1.07\)</span>, <span class="math inline">\(0.44\)</span>, and <span class="math inline">\(1.32\)</span> for Q1-Q3, respectively.</p><p>作者提出的方法在Q1-Q3都超过了基准方法, 超过的分数依次是<span class="math inline">\(1.07\)</span>、<span class="math inline">\(0.44\)</span>和<span class="math inline">\(1.32\)</span>.</p><p>Though VQGAN is somewhat <u>effective at representing the natural content</u> in our prompts, perhaps due to the <u>implicit content signal it receives from the mask</u>, it struggles to synthesize these representations with style in a meaningful way.</p><p>VQGAN在表现自然内容方面是有效的, 但是由于其从模板接受隐式内容信号, 导致其无法有效的合成相关的表达.</p><h4 id="beyond-textual-stylization部分">Beyond Textual Stylization部分</h4><p>Beyond text-based stylization, our method can be used to <u>stylize a mesh toward different target modalities</u> such as a 2D image or even a 3D object.</p><p>除了使用文本进行格式化, 作者提出的框架还能够使用不同的模态进行格式化, 例如图像甚至三维物体.</p><p>For a target 2D image <span class="math inline">\(I_t\)</span>, <u><span class="math inline">\(\phi_{\text{target}}\)</span> represents the image-based CLIP embedding of <span class="math inline">\(I_t\)</span></u>.</p><p>对于图像<span class="math inline">\(I_t\)</span>, <span class="math inline">\(\phi_{\text{target}}\)</span>代表将图像<span class="math inline">\(I_t\)</span>嵌入CLIP空间.</p><p>Stylization <u>driven by an image target</u>.</p><p>基于图像的格式化.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062012132.png" /></p><p>For a target mesh <span class="math inline">\(T\)</span>, <u><span class="math inline">\(\phi_{\text{target}}\)</span> is the average embedding, in CLIP space, of the 2D renderings of <span class="math inline">\(T\)</span></u>, where the views are the same as those sampled for the source mesh.</p><p>对于<span class="math inline">\(T\)</span>代表的mesh, <span class="math inline">\(\phi_{\text{target}}\)</span>代表了将<span class="math inline">\(T\)</span>按照与输入mesh一致的视角进行二维渲染得到的图片嵌入CLIP空间的均值.</p><p>Beyond different modalities, we can <u>combine targets across different modalities by simply summing <span class="math inline">\(\mathcal{L}_{\text {sim}}\)</span> over each target</u>.</p><p>除了不同的模态, 作者认为还可以融合多个模态, 通过简单的叠加每个模态的<span class="math inline">\(\mathcal{L}_{\text {sim}}\)</span>.</p><p>Neural stylization <u>driven by mesh targets</u>. (a) &amp; (c) are styled using Targets 1 &amp; 2, respectively. (b) &amp; (d) are styled with text in addition to the mesh targets: (b) 'a cactus that looks like a cow', (d) 'a mouse that looks like a duck'.</p><p>基于mesh的格式化. (a)和(c)分别使用Target 1和Target 2进行格式化. (b) &amp; (d)在分别使用Target 1和Target 2进行格式化的同时还使用文本进行格式化: (b)"像奶牛一样的仙人掌", (d)"像鸭子一样的老鼠".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201062013015.png" /></p><h4 id="incorporating-symmetries部分">Incorporating Symmetries部分</h4><p>We can make use of prior knowledge of the <u>input shape symmetry</u> to <u>enforce style consistency across the axis of symmetry</u>.</p><p>作者提出的框架可以利用输入形状的对称性作为先验来强制保证对称轴对称部分的一致性.</p><p>Such symmetries can be introduced into our model by <u>modifying the input to our positional encoding</u>.</p><p>这种对称性可以通过修改位置编码的方式实现.</p><p>For instance, <u>given a point <span class="math inline">\(p = (x, y, z)\)</span> and a shape with bilateral symmetry across the <span class="math inline">\(X-Y\)</span> plane</u>, one can <u>apply a function prior to the the positional encoding such that <span class="math inline">\(\gamma (x, y, \abs{z})\)</span></u>.</p><p>例如, 对于沿着<span class="math inline">\(X-Y\)</span>平面对称的物体的任意一点<span class="math inline">\(p = (x, y, z)\)</span>, 可以修改位置编码为<span class="math inline">\(\gamma (x, y, \abs{z})\)</span>.</p><p>Effect of the <u>symmetry prior on a UFO mesh input</u> with text prompt: 'colorful UFO'.</p><p>下图是对一个UFO的mesh有无使用对称性先验的生成结果, 输入的文本是"colorful UFO".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061951971.png" /></p><p>This prior is <u>effective even when the triangulation is not perfectly symmetrical</u>, since the function is applied in Euclidean space.</p><p>由于这种对称先验使用的函数是在欧几里得空间中进行操作, 这个先验对于那些不是严格对称的物体仍然有效.</p><p>A full investigation into <u>incorporating additional symmetries within positional encoding</u> is an interesting direction for future work.</p><p>将对称性融入位置编码的更完善的研究是未来的一个有意思的研究方向.</p><h4 id="limitations部分">Limitations部分</h4><p>Our method implicitly <u>assumes there exists a synergy between the input 3D geometry and the target style prompt</u>.</p><p>作者提出的方法隐式的假设输入的三维物体和目标风格之间存在一定的联系.</p><p>If the target style is <u>unrelated to the 3D mesh content</u>, the stylization may <u>ignore the 3D content</u>. Results are improved when including the content in the target text prompt.</p><p>当目标风格与三维物体无关的时候, 作者提出的方法会忽略三维物体的形状. 当风格中包含三维物体的描述生成的效果会更好.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061907420.png" /></p><p>Therefore, <u>in order to preserve the original content</u> when editing towards a mismatched target prompt, we <u>simply include the object category in the text prompt</u> (e.g., stained glass dragon) which <u>adds a content preservation constraint into the target</u>.</p><p>因此, 为了保护原有的三维形状, 作者简单的在风格文本中加入三维物体的描述, 这有效的保护了三维物体的形状.</p><h3 id="supplement部分">Supplement部分</h3><h4 id="additional-results部分">Additional Results部分</h4><p>Please refer to <u>our project webpage</u> additional results.</p><p>作者的项目网站上有更多的生成结果.</p><h4 id="high-resolution-stylization部分">High Resolution Stylization部分</h4><p>Our method is effective even on coarse inputs, and one can always <u>increase the resolution of a mesh <span class="math inline">\(M\)</span> to learn a neural ﬁeld with ﬁner granularity</u>.</p><p>作者提出的方法在粗糙的输入有很好的表现, 作者认为还可以使用插入顶点的方式增加<span class="math inline">\(M\)</span>对应的三维mesh的分辨率.</p><p>In following figure, we <u>upsample the mesh by inserting a degree-3 vertex in the barycenter of each triangle face of the mesh</u>.</p><p>在如下的图片中, 作者上采样输入的mesh, 通过在三角面片的中心位置插入入度为3的顶点来提升分辨率.</p><p>Style results over a coarse torus (left) and the same mesh with each triangle barycenter inserted as an additional vertex (right). Prompt: 'a donut made of cactus'.</p><p>左边是粗糙的mesh输入, 右边是左边的mesh经过上采样之后的高分辨率mesh输入. 输入的文本是"仙人掌制作的甜甜圈".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061747482.png" /></p><p>The network is <u>able to synthesize a ﬁner style by leveraging these additional vertices</u>.</p><p>网络可以利用这些新插入的点合成更好的结果.</p><p>作者使用的上采样过程如下图所示.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061750755.png" /></p><h4 id="choice-of-anchor-view部分">Choice of anchor view部分</h4><p>As mentioned in the main text, we select <u>the view with the highest (i.e. best) CLIP similarity to the content as the anchor</u>.</p><p>作者选取CLIP相似度最高的视角作为锚点视角.</p><p>There are often <u>many possible views that can be chosen as the anchor</u> that will allow a high-quality stylization.</p><p>通常可以作为锚点的视角不止一个.</p><p>找锚点视角的过程: 首先获取三维mesh每个点的法线方向渲染的二维图片, 然后将每张图片嵌入到CLIP空间中, 之后将这个三维mesh物体的文本名称嵌入到CLIP空间中, 比较这两个嵌入形式可以得到一个相似度分数, 选取相似度分数最高的几个视角之一作为锚点视角即可.</p><p>The CLIP score exhibits a strong positive correlation with views that are semantically meaningful, and thus can be used for automatic anchor view selection, as described in the main paper.</p><p>CLIP相似度分数通常与视角图片的语义信息是否有意义正相关.</p><p>This metric is <u>limited in expressiveness</u>, however, as <u>demonstrated by the constrained range</u> that the scores fall within for all the views around the mesh.</p><p>这种方式在表达能力上有所限制, 因为锚点视角只能看到物体的一个范围, 在所有锚点视角上相似度分数较高, 如果考虑其他视角的话, 相似度分数会下降.</p><p><span class="math inline">\(n_{\theta}\)</span>, <u>the number of sampled views</u>, is set to <span class="math inline">\(5\)</span>.</p><p>采样的视角数量<span class="math inline">\(n_{\theta}\)</span>被设置为<span class="math inline">\(5\)</span>.</p><p>We show in following figure that <u>increasing the number of views beyond 5 does little to change the quality of the output stylization</u>. Prompt: 'A horse made of cactus'.</p><p>下图表明增加采样的视角数量对最终生成的结果影响不大. 输入的文本是"仙人掌制作的马".</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061739346.png" /></p><h4 id="training-and-implementation-details部分">Training and Implementation Details部分</h4><h5 id="network-architecture部分">Network Architecture部分</h5><p>We <u>map a vertex <span class="math inline">\(p \in \mathbb{R}^{3}\)</span> to a 256-dimensional Fourier feature</u>.</p><p>作者将一个顶点<span class="math inline">\(p \in \mathbb{R}^{3}\)</span>映射到一个256-dimensional的傅里叶特征.</p><p><u>Typically <span class="math inline">\(5.0\)</span> is used as the standard deviation for the entries of the Gaussian matrix <span class="math inline">\(\mathbf{B}\)</span></u>, although this can be set to the preference of the user.</p><p>作者设置高斯矩阵<span class="math inline">\(\mathbf{B}\)</span>中的标准差为<span class="math inline">\(5.0\)</span>, 也可以设置为其他值.</p><p>The <u>shared MLP layers <span class="math inline">\(N_s\)</span> consist of 4 256-dimensional linear layers with ReLU activation</u>.</p><p>MLP层<span class="math inline">\(N_s\)</span>包含4个256维的带有ReLU激活函数的线性层.</p><p>The branched layers, <span class="math inline">\(N_d\)</span> and <span class="math inline">\(N_c\)</span>, each consist of two 256-dimensional linear layers with ReLU activation.</p><p><span class="math inline">\(N_d\)</span>和<span class="math inline">\(N_c\)</span>这两个分支层各包含2个256维的带有ReLU激活函数的线性层.</p><p>After the ﬁnal linear layer, a tanh activation is applied to each branch.</p><p>在最后一层, tanh激活函数会应用在每个分支.</p><p>The <u>weights of the ﬁnal linear layer of each branch are initialized to zer</u>o so that the original content mesh is unaltered at initialization.</p><p>所有的权重初始化都为0以保证初始时原始的mesh没有被改变.</p><p>We <u>divide the output of <span class="math inline">\(N_c\)</span> by <span class="math inline">\(2\)</span> and add it to <span class="math inline">\([0.5, 0.5, 0.5]\)</span></u>.</p><p>作者将<span class="math inline">\(N_c\)</span>经过tanh激活层的输出除<span class="math inline">\(2\)</span>并加上<span class="math inline">\([0.5, 0.5, 0.5]\)</span>.</p><p>This <u>enforces the final color prediction <span class="math inline">\(c_p\)</span> to be in range <span class="math inline">\((0.0, 1.0)\)</span></u>.</p><p>这个操作保证最终的颜色预测值<span class="math inline">\(c_p\)</span>在范围<span class="math inline">\((0.0, 1.0)\)</span>内.</p><p>We ﬁnd that <u>initializing the mesh color to <span class="math inline">\([0.5, 0.5, 0.5]\)</span> (grey) and adding the network output as a residual helps prevent undesirable solutions in the early iterations of training</u>.</p><p>作者法线初始化mesh的颜色为<span class="math inline">\([0.5, 0.5, 0.5]\)</span>并且以残差的形式连接网络输出和输入的mesh可以防止在早期训练中出现不想要的情况.</p><p>For the branch <span class="math inline">\(N_d\)</span>, we <u>multiply the ﬁnal tanh layer by <span class="math inline">\(0.1\)</span> to get displacements in the range <span class="math inline">\((−0.1, 0.1)\)</span></u>.</p><p>对于<span class="math inline">\(N_d\)</span>层的输出, 作者在tanh激活层之后将其输出乘上<span class="math inline">\(0.1\)</span>来保证偏移距离在<span class="math inline">\((−0.1, 0.1)\)</span>内.</p><h5 id="training部分">Training部分</h5><p>We use the <u>Adam optimizer with an initial learning rate of <span class="math inline">\(5e−4\)</span></u>, and <u>decay the learning rate by a factor of <span class="math inline">\(0.9\)</span> every <span class="math inline">\(100\)</span> iterations</u>.</p><p>作者使用Adam优化器, 初始学习率是<span class="math inline">\(5e−4\)</span>, 每<span class="math inline">\(100\)</span>次迭代学习率乘上系数<span class="math inline">\(0.9\)</span>.</p><p>We train for <span class="math inline">\(1500\)</span> iterations on a single Nvidia GeForce RTX2080Ti GPU, which <u>takes around <span class="math inline">\(25\)</span> minutes to complete</u>.</p><p>作者在单张Nvidia GeForce RTX2080Ti GPU上迭代训练<span class="math inline">\(1500\)</span>次, 大约需要<span class="math inline">\(25\)</span>分钟完成训练.</p><p>For augmentations <span class="math inline">\(\Psi_{\text{global}}\)</span>, we use <u>a random perspective transformation</u>.</p><p>对于图像增强<span class="math inline">\(\Psi_{\text{global}}\)</span>, 作者使用一个随机透视变换.</p><p>For <span class="math inline">\(\Psi_{\text{local}}\)</span>, we randomly crop the image to <span class="math inline">\(10\%\)</span> of its original size and then apply a random perspective transformation.</p><p>对于图像增强<span class="math inline">\(\Psi_{\text{local}}\)</span>, 作者随机裁剪图像到原始尺寸的<span class="math inline">\(10\%\)</span>然后使用一个随机透视变换.</p><p>Before encoding images with CLIP, we normalize per-channel by mean <span class="math inline">\((0.48145466, 0.4578275, 0.40821073)\)</span> and standard deviation <span class="math inline">\((0.26862954, 0.26130258, 0.27577711)\)</span>.</p><p>在使用CLIP编码图片之前, 作者正则化图像的三个通道, 使之均值为<span class="math inline">\((0.48145466, 0.4578275, 0.40821073)\)</span>, 标准差为<span class="math inline">\((0.26862954, 0.26130258, 0.27577711)\)</span>. [这是一组常用的正则化参数.]</p><h4 id="baseline-comparison-and-user-study部分">Baseline Comparison and User Study部分</h4><h4 id="societal-impact部分">Societal Impact部分</h4><p>Our framework utilizes a pre-trained CLIP embedding space, <u>which has been shown to contain bias</u>.</p><p>作者提出的架构利用了预训练的CLIP嵌入空间, 而CLIP嵌入空间已经被证明存在偏置.</p><p>Since our system is capable of synthesizing a style driven by a target text prompt, <u>it enables visualizing such biases in a direct and transparent way</u>.</p><p>由于作者提出的系统能够合成符合文本描述的风格特征, 此系统可以直接透明的将这个偏置可视化出来.</p><p>For example, <u>the nurse style in following figure is biased towards adding female features to the input male shape</u>. Given a human male input, and target prompt: ‘a nurse’, we observe a gender bias in CLIP to favor female shapes.</p><p>例如, 给定一个男性mesh, 限定文字输入为护士, 我们可以观察到输出的mesh在男性mesh上增加了许多女性特征, 这表示CLIP存在偏差, CLIP在护士这一类别上更偏好女性.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201061653413.png" /></p><p>An important future work may leverage our proposed system in <u>helping create a datasheet for CLIP in addition to future image-text embedding models</u>.</p><p>作者设计的系统也可以帮助未来准备使用CLIP的图像文本嵌入模型制作数据集.</p><h3 id="精读总结">精读总结</h3><blockquote><p>精读目标及效果自测：所读段落是否详细掌握，回答三个的终极问题(你是谁 - 论文提出/采用什么方法、细节是什么；从哪里来 - 论文要解决什么问题/任务、其启发点或借鉴之处在哪；到哪里去 - 论文方法达到什么效果)。</p></blockquote><p>论文构建了一个基于文本的三维物体风格转换器.</p><p>论文使用残差的方式解耦三维物体和风格参数.</p><p>论文将风格参数划分为表面顶点的颜色和沿法线方向的位置偏移, 分别进行回归计算.</p><p>论文采用了多种先验信息(位置编码、图像增强、几何结构等等)构建损失函数, 优化网络权重, 避免获得退化结果.</p><p>论文使用CLIP进行多模态融合, 实现了多模态定义的风格变换.</p><p>论文设计的风格变换架构能够生成质量更好的不同风格的三维物体.</p><h2 id="总结">总结</h2><blockquote><p>总结：总览全文，归纳总结，总结文中创新点，关键点，启发点（论文idea的来源或者说是论文的motivation）等重要信息。</p></blockquote><p>论文的创新点、关键点:</p><ul><li>用残差的方式解耦三维物体和风格参数;</li><li>将风格参数划分为表面顶点的颜色和沿法线方向的位置偏移, 分别进行回归计算;</li><li>采用了多种先验信息(位置编码、图像增强、几何结构等等)构建损失函数, 优化网络权重, 避免获得退化结果;</li><li>使用CLIP进行多模态融合, 实现了多模态定义的风格变换;</li><li>设计的风格变换架构能够生成质量更好的不同风格的三维物体.</li></ul><p>论文的启发点:</p><ul><li>CLIP的多模态融合;</li><li>残差解耦三维物体和风格参数;</li><li>多重先验信息防止网络退化.</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection阅读笔记</title>
    <link href="/2022/01/02/FCAF3D-Fully-Convolutional-Anchor-Free-3D-Object-Detection%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/01/02/FCAF3D-Fully-Convolutional-Anchor-Free-3D-Object-Detection%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="fcaf3d-fully-convolutional-anchor-free-3d-object-detection阅读笔记">FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection阅读笔记</h1><blockquote><p>读论文三步曲：泛读，精读，总结。</p></blockquote><h2 id="泛读">泛读</h2><blockquote><p>泛读：快速浏览，把握概要。重点读标题、摘要、结论、所有小标题。</p></blockquote><h3 id="title部分">Title部分</h3><p>FCAF3D: <u>Fully Convolutional</u> <strong>Anchor-Free</strong> <em>3D Object Detection</em></p><ul><li>任务: 3D Object Detection</li><li>方法: Fully Convolution</li><li>特点: Anchor-Free</li></ul><h4 id="什么是anchor">什么是Anchor?</h4><blockquote><p><em>参考资料: <a href="https://zhuanlan.zhihu.com/p/55824651">目标检测中的Anchor</a></em></p></blockquote><p>在目标检测中, Anchor指锚点, Anchor Box指锚框.</p><p>目标检测需要解决<strong>在哪里有什么</strong>的问题, 具体来说, 检测目标的类别、数量、位置、尺度都是不确定的.</p><p>传统非深度学习方法和早期深度学习方法都要结合<strong>金字塔多尺度</strong>和<strong>遍历滑窗</strong>的方式, 逐尺度逐位置判断<strong>这个尺度的这个位置处有没有认识的目标</strong>, 非常耗时.</p><p>近期顶尖(SOTA)的目标检测方法几乎都用了anchor技术. 首先预设一组不同尺度不同位置的固定参考框, 覆盖几乎所有位置和尺度, 每个参考框负责检测与其交并比大于阈值(训练预设值，常用0.5或0.7)的目标，anchor技术将问题转换为<strong>这个固定参考框中有没有认识的目标、目标框偏离参考框多远</strong>, 不再需要多尺度遍历滑窗, 真正实现了又好又快。</p><p>使用Anchor技术的算法称为Anchor-based算法, 不使用Anchor技术的算法称为Anchor-free的算法.</p><p>Anchor-based的算法一般需要先在训练集上统计一组不同尺寸检测框的集合, 这个集合代表目标框主要分布的尺度, 在推理生成的特征图上使用这一组框滑动判断框内有无目标得到候选框, 最后聚合所有的候选框得到最终的检测框结果.</p><h3 id="abstract部分">Abstract部分</h3><p>FCAF3D - a ﬁrst-in-class fully convolutional anchor-free <u>indoor</u> 3D object detection method.</p><p>细化使用场景为室内.</p><p>It is a simple yet effective method that uses a <u>voxel representation</u> of a point cloud and processes voxels with <u>sparse convolutions</u>.</p><p>使用点云的体素化表示方法并使用稀疏卷积对体素进行操作.</p><p>Existing 3D object detection methods make <u>prior assumptions on the geometry of objects</u>, and we argue that it <u>limits their generalization ability</u>.</p><p>作者认为目前的三维检测方法对物体的几何尺寸有预先假设, 这限制了模型的生成能力.</p><p>To get rid of any prior assumptions, we propose <u>a novel parametrization of oriented bounding boxes</u>.</p><p>为了不依赖任何先验假设, 作者提出了一种有向边界框的参数化方法.</p><p>The proposed method achieves state-of-the-art 3D object detection results in terms of <span class="math inline">\(mAP@0.5\)</span> on ScanNet V2 (+4.5), SUN RGB-D (+3.5), and S3DIS (+20.5) datasets.</p><p>作者提出的方法在三大室内数据集(ScanNet V2、SUN RGB-D、S3DIS)上均达到了SOTA.</p><p>The code and models are available at <a href="https://github.com/samsunglabs/fcaf3d">https://github.com/samsunglabs/fcaf3d</a>.</p><p>代码在<a href="https://github.com/samsunglabs/fcaf3d">GitHub</a>.</p><h3 id="conclusion部分">Conclusion部分</h3><p>The proposed method signiﬁcantly outperforms the previous state-of-the-art on the challenging indoor SUN RGB-D, ScanNet, and S3DIS benchmarks in terms of both <u>mAP</u> and <u>inference speed</u>.</p><p>作者提出的方法在mAP和预测速度这两个指标上都超过之前的算法.</p><p>We have also proposed a novel oriented bounding box parametrization and shown that it improves detection accuracy for <u>several existing 3D object detection methods</u>.</p><p>作者提出的新的向边界框的参数化方法能够提升已有算法的准确性.</p><p>Moreover, the proposed parametrization allows avoiding any prior assumptions about objects, thus <u>reducing the number of hyperparameters</u>.</p><p>作者提出的新的有向边界框的参数化方法能够减少超参数的数量, 因为其避免使用物体框的先验信息.</p><h3 id="小标题分析">小标题分析</h3><ul><li>Introduction <em>[简介]</em></li><li>Related Work <em>[相关工作]</em></li><li>Proposed Method <em>[提出的方法]</em><ul><li>Sparse Neural Network <em>[<u>FCAF3D结构</u>]</em></li><li>Bounding Box Parametrization <em>[<u>新的有向边界框的参数化方法</u>]</em></li></ul></li><li>Experiments <em>[实验]</em><ul><li>Datasets <em>[数据集]</em></li><li>Implementation Details <em>[实现细节]</em></li></ul></li><li>Results <em>[实验结果]</em><ul><li>Comparison with State-of-the-art Methods <em>[与其他SOTA的比较]</em></li><li>Object Geometry Priors <em>[<u>物体结构先验</u>]</em></li><li>Ablation Study <em>[<u>消融实验</u>]</em></li><li>Inference Speed <em>[预测速度]</em></li></ul></li><li>Conclusion <em>[结论]</em></li><li>Supplement <em>[附加材料]</em><ul><li>Additional Comments on Mobius Parametrization <em>[<u>新的有向边界框的参数化方法的附加说明</u>]</em></li><li>Per-category results <em>[每个类别的实验结果]</em></li><li>Visualization <em>[可视化结果]</em></li></ul></li></ul><h3 id="泛读总结">泛读总结</h3><blockquote><p>泛读目标及效果自测：1.论文要解决什么问题? 2.论文采用了什么方法? 3.论文达到什么效果?</p></blockquote><p>论文要解决什么问题? - 室内场景的三维检测任务.</p><p>论文采用了什么方法? - 使用Anchor-Free的全卷积方法, 输入为三维点云经过体素化后的体素数据, 使用稀疏卷积对体素进行操作. 同时, 作者认为物体的几何尺寸的先验限制了模型的生成能力. 为了不依赖任何先验假设, 作者提出了一种有向边界框的参数化方法.</p><p>论文达到什么效果? - 作者提出的新的参数化方法不仅能够提升已有检测方法的准确性, 而且由于避免使用形状先验, 这种新的参数化方法减少了需要的超参数的数量. FCAF3D在mAP和预测速度这两个指标上、在三大室内数据集(ScanNet V2、SUN RGB-D、S3DIS)上均达到了SOTA.</p><h2 id="精读">精读</h2><blockquote><p>精读：通过泛读找到需要精读重点部分和快速略过的部分，选出精华，仔细阅读。</p></blockquote><h3 id="introduction部分">Introduction部分</h3><p>3D methods are challenged by <u>irregular unstructured 3D data of arbitrary volume</u>.</p><p>三维物体检测的难点在于三维数据是不规则的、非结构化的、容量可变的.</p><p>All convolutional methods for 3D object detection have <u>scalability issues</u>: large-scale scenes either require an impractical amount of computational resources or take too much time to process.</p><p>三维物体检测方法一般都面临拓展性的问题, 对于大规模场景需要耗费很高的计算成本或花费很长时间.</p><p>Other methods opt for voxel data representation and employ sparse convolutions; however, these methods solve scalability problems at the cost of detection accuracy.</p><p>有一些使用稀疏卷积的方法优化处理体素数据, 通过牺牲检测准确度的方式解决拓展性问题.</p><p>Besides being scalable and accurate, an ideal 3D object detection method should also be able to handle <u>diverse objects of arbitrary shapes and sizes</u> without additional hacks and hand-tuned hyperparameters.</p><p>理想的三维检测器应该能够预测各种大小和形状的多种多样的物体.</p><p>Moreover, we introduce a novel oriented bounding box (OBB) parametrization inspired by a <u>Mobius strip</u> that reduces the number of hyperparameters.</p><p>作者提出的新的有向边界框的参数化方法是受到莫比乌斯带的启发.</p><p>Overall, our contribution is three-fold: 1. To our knowledge, we propose a ﬁrst-in-class fully convolutional anchor-free 3D object detection method (FCAF3D) for indoor scenes. 2. We present a novel OBB parametrization and prove it to boost accuracy of several existing 3D object detection methods on SUN RGB-D. 3. Our method signiﬁcantly outperforms the previous state-of-the-art on challenging large-scale indoor ScanNet, SUN RGB-D, and S3DIS datasets in terms of mAP while being faster on inference.</p><p>作者的贡献:</p><ol type="1"><li>提出了FCAF3D, 一个anchor-free的全卷积的室内三维检测算法.</li><li>提出了一种新的有向边界框的参数化方法并证明了这种方法能够显著提升已有的三维检测算法的表现.</li><li>FCAF3D不仅mAP大幅超过之前的SOTA还比之前的算法更快在预测的时候.</li></ol><h3 id="related-work部分">Related Work部分</h3><p>Indoor and outdoor methods have been developing almost <u>independently</u>, <u>applying domain-speciﬁc solutions to address data issues</u>.</p><p>室内和室外三维检测方法基本上是分开独立发展的, 因为这样可以利用domain-speciﬁc的特点解决数据带来的问题.</p><p>Currently, three approaches dominate the ﬁeld of 3D object detection - <u>voting-based</u>, <u>transformer-based</u>, and <u>3D convolutional</u>.</p><p>目前, 室内三维检测主要有三种方式: <u>voting-based</u>、<u>transformer-based</u>和<u>3D convolutional</u>.</p><h4 id="voting-based-methods">Voting-based methods</h4><p>Voting-based方法: VoteNet, BRNet, MLCVNet, H3DNet, VENet.</p><p>All the voting-based methods <u>inherited from VoteNet</u> are limited by design.</p><p>所有的voting-based方法都继承自VoteNet.</p><p>First, their performance depends on <u>the amount of input data</u>; thus, they tend to slow down if given larger scenes and demonstrate <u>poor scalability</u>.</p><p>Voting-based方法的表现依赖于输入数据的数量, 因此在处理大量数据的时候会很慢, 这产生了拓展性的问题.</p><p>Moreover, many voting-based methods <u>implement voting and grouping strategies as complex custom layers</u> making it <u>difﬁcult to reproduce or debug these methods or port them to mobile devices</u>.</p><p>Voting-based方法一般将投票和分组策略实现成复杂的定制的层, 这使得再现和调试变得很困难并且难以将这些方法移植到移动设备上.</p><h4 id="transformer-based-methods">Transformer-based methods</h4><p>Transformer-based方法: GroupFree, 3DETR.</p><p>However, similar to early voting-based methods, more advanced transformer-based methods still experience <u>scalability issues</u>.</p><p>Transformer-based方法也面临拓展性的问题.</p><p>Differently, our method is fully-convolutional thus being <u>faster and signiﬁcantly easier</u> to implement compared to both voting-based and tranformer-based methods.</p><p>FCAF3D是全卷积的方法, 与voting-based方法和transformer-based方法相比, 运行更快且更易实现.</p><h4 id="d-convolutional-methods">3D convolutional methods</h4><p>3D convolutional方法: GSDN.</p><p>Difﬁculties with handling cubically growing sparse 3D data can be overcome by <u>using voxel representation</u>.</p><p>使用体素表达可以解决稀疏的三维数据的立方增长的问题.</p><p>However, <u>dense volumetric features still consume much memory</u>, and <u>3D convolutions are computationally expensive</u>.</p><p>但是稠密的体素特征仍然消耗许多内存, 三维卷积的计算量也很大.</p><p>Overall, processing large scenes requires a lot of resources and <u>cannot be done within a single pass</u>.</p><p>处理大场景的数据需要大量的资源并且无法一次性完成.</p><p>At the same time, our method is anchor-free while taking all advantages of sparse 3D convolutions.</p><p>FCAF3D是一种anchor-free的方法并且有效利用了稀疏三维卷积.</p><h4 id="rgb-based-anchor-free-object-detection">RGB-based anchor-free object detection</h4><p>二维检测中的anchor-free算法: FCOS.</p><p>FCOS3D和ImVoxelNet借鉴了FCOS的思想.</p><p>We adapt the ideas from aforementioned anchor-free methods to address the sparse irregular data.</p><p>作者借鉴了上述的三种方法来解决稀疏的不规则的数据带来的问题.</p><h3 id="proposed-method部分">Proposed Method部分</h3><p>Following the standard 3D detection problem statement, FCAF3D <u>accepts <span class="math inline">\(N_{\text{pts}}\)</span> RGB-colored points</u> and <u>outputs a set of 3D object bounding boxes</u>.</p><p>FCOS3D输入为<span class="math inline">\(N_{\text{pts}}\)</span>个包含RGB颜色信息的点, 输出为三维物体边界框构成的集合.</p><p>The architecture of FCAF3D consists of three parts: a backbone, a neck, and a head.</p><p>FCOS3D架构包含三部分: backbone、neck和head.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202112302104766.png" /></p><p>All convolutions and transposed convolutions are <u>three-dimensional</u> and <u>sparse</u>.</p><p>所有的卷积和转置卷积都是三维稀疏卷积和三维稀疏转置卷积.</p><h4 id="sparse-neural-network部分">Sparse Neural Network部分</h4><h5 id="backbone">Backbone</h5><p>The backbone in FCAF3D is <u>a sparse modiﬁcation of ResNet</u> where all 2D convolutions are replaced with sparse 3D convolutions.</p><p>FCAF3D的backbone是ResNet的一个修改版本, 所有的二维卷积都换成了三维稀疏卷积.</p><p>For brevity, we refer to the family of <u>sparse high-dimensional versions</u> of ResNet as to <u>HDResNet</u>.</p><p>作者称稀疏高维版本的ResNet为HDResNet.</p><h5 id="neck">Neck</h5><p>The neck is <u>a simpliﬁed decoder</u> from GSDN.</p><p>FCAF3D的neck是一个简化后的GSDN的解码器.</p><p>Features on each level are processed with <u>one sparse transposed 3D convolution</u> and <u>one sparse 3D convolution</u>.</p><p>各个层级的特征会被一个稀疏转置三维卷积操作和一个稀疏三位卷积操作处理.</p><p>Each <u>transposed sparse 3D convolution</u> with a <u>kernel size of <span class="math inline">\(2\)</span></u> can increase the number of non-zero values <u>by <span class="math inline">\(2^3\)</span> times</u>.</p><p>每个稀疏转置三维卷积的核大小是<span class="math inline">\(2\)</span>, 处理后的非零元素值的数目会变为原来的<span class="math inline">\(2^3\)</span>倍.</p><p>To <u>prevent the rapid growth of required memory</u>, GSDN introduces the <u>pruning layer</u> that <u>ﬁlters all elements of input with a probability mask</u>.</p><p>为避免内存的急速增长, GSDN引入剪枝层, 接用概率模板来过滤输入元素.</p><p>In GSDN, feature level-wise probabilities are calculated with <u>an additional convolutional scoring layer</u>.</p><p>GSDN使用额外的卷积层来获取不同特征层级的概率模板.</p><p>This layer is trained with a special loss that <u>encourages consistency between the predicted sparsity and anchors</u>.</p><p>GSDN使用一个特别的损失函数来保证预测稀疏程度和锚点的一致性.</p><p>Speciﬁcally, voxel sparsity is set to be positive <u>if any of the subsequent anchors associated to the current voxel is positive</u>.</p><p>体素稀疏度会被置为1, 如果此体素中任何一个子部分与锚点相关联.</p><p>However, using this loss may be <u>suboptimal</u>, as <u>distant voxels</u> of an object might get assigned with a <u>low probability</u>.</p><p>这种获取概率模板的方法是次优的, 因为隔物体较远的体素可能会得到一个很低的概率.</p><p>For simplicity, we <u>remove the scoring layer</u> with the corresponding loss and <u>use probabilities from the classiﬁcation layer in the head instead</u>.</p><p>为了简化, 作者去除了这个计算概率模板的卷积层以及针对这个卷积层设计的损失函数, 作者使用head中的分类层来替代这个卷积层的功能.</p><p>We <u>do not tune the probability threshold</u> but <u>keep at most <span class="math inline">\(N_{\text{vox}}\)</span> voxels to control the sparsity level</u>, where <span class="math inline">\(N_{\text{vox}}\)</span> equals the number of input points <span class="math inline">\(N_{\text{pts}}\)</span>.</p><p>作者不使用概率模板, 而是直接根据概率从高到低排序选择前<span class="math inline">\(N_{\text{vox}}\)</span>个体素, <span class="math inline">\(N_{\text{vox}}\)</span>与输入点的个数<span class="math inline">\(N_{\text{pts}}\)</span>相同.</p><p>We claim this to be a simple yet elegant way to prevent sparsity growth since reusing the same hyperparameter makes the process more transparent and consistent.</p><p>作者认为这种简单的方法能够保持稀疏性, 重复利用超参数让整个处理过程更加透明和一致.</p><h5 id="head">Head</h5><p>The anchor-free head of FCAF3D consists of <u>three parallel sparse convolutional layers with weights shared across feature levels</u>.</p><p>FCAF3D的head部分是anchor-free的, 由三个平行的卷积层组成, 不同的特征层级使用的head共享同一个权重.</p><p>For each location <span class="math inline">\((\hat{x}, \hat{y}, \hat{z})\)</span>, these sparse convolutional layers output <u>classiﬁcation probabilities <span class="math inline">\(\hat{\boldsymbol{p}}\)</span></u>, <u>bounding box regression parameters <span class="math inline">\(\boldsymbol{\delta}\)</span></u>, and <u>centerness <span class="math inline">\(\hat{c}\)</span></u>, respectively.</p><p>对每个位置<span class="math inline">\((\hat{x}, \hat{y}, \hat{z})\)</span>, 这三个平行的卷积层分别输出分类概率<span class="math inline">\(\hat{\boldsymbol{p}}\)</span>、边界框回归参数<span class="math inline">\(\boldsymbol{\delta}\)</span>和中心度(感受野中心与目标物中心的靠近程度)<span class="math inline">\(\hat{c}\)</span>.</p><p>This design is similar to the simple and light-weight head of <u>FCOS</u> but adapted to 3D data.</p><p>FCAF3D的head部分与FCOS的head设计类似.</p><h5 id="multi-level-location-assignment">Multi-level location assignment</h5><p>During training, FCAF3D outputs <u>locations <span class="math inline">\(\{(\hat{x}, \hat{y}, \hat{z})\}\)</span></u> for different feature levels, which should be <u>assigned to ground truth boxes <span class="math inline">\(\{\boldsymbol{b}\}\)</span></u> so the loss can be calculated.</p><p>为了计算损失函数, 我们需要给FCAF3D在不同特征层级的位置<span class="math inline">\(\{(\hat{x}, \hat{y}, \hat{z})\}\)</span>匹配一个边界框标签<span class="math inline">\(\{\boldsymbol{b}\}\)</span>.</p><p>FCOS and ImVoxelNet stick to the following scheme:</p><ul><li>For each location, only ground truth bounding boxes that <u>cover this location</u> are selected.</li><li>Then, the bounding boxes with at least one face <u>further from this location than a threshold</u> are ﬁltered out.</li><li>Finally, the <u>bounding box with the least volume</u> is assigned to this location.</li></ul><p>FCOS and ImVoxelNet使用如下方式:</p><ul><li>对每个位置, 覆盖到这个位置的边界框都会被选中.</li><li>去除那些有多于一个面到此位置的距离超过设定的阈值的边界框.</li><li>在剩下的边界框中容量最小的框会被作为此位置的标签.</li></ul><p>Such an assignment strategy is <u>suboptimal</u>, and its alterations are widely explored in 2D object detection.</p><p>这种匹配方式是次优的.</p><p>ImVoxelNet uses a modiﬁcation that requires <u>hand-tuning the face distance threshold for each feature level</u>.</p><p>ImVoxelNet需要手动设定每一个特征层级的距离阈值.</p><p>We propose a simpliﬁed solution designed for sparse data that <u>does not require tuning dataset-speciﬁc hyperparameters</u>.</p><ul><li>For each bounding box, we select <u>the last feature level</u> for which this <u>bounding box covers at least <span class="math inline">\(N_{\text{loc}}\)</span> locations</u>.</li><li>If the bounding box covers less than <span class="math inline">\(N_{\text{loc}}\)</span> locations at each feature level, we opt for the ﬁrst feature level.</li><li>We also ﬁlter locations via center sampling. In center sampling, only the points close to the center of the bounding box are considered positive matches.</li></ul><p>作者使用一个简化的不需要依赖数据集的超参数的方案来配对.</p><ul><li>对每个边界框, 我们选取此边界框框住多余<span class="math inline">\(N_{\text{loc}}\)</span>个位置的特征层级中的最后的层级.</li><li>如果所有层级中此边界框框住的位置数量都少于<span class="math inline">\(N_{\text{loc}}\)</span>, 那么选取第一个特征层级.</li><li>对于选定的层级中, 我们对位置进行中心采样, 只有靠近边界框中心的位置会被认为是匹配上的.</li></ul><p>After the location assignment, some ﬁnal locations <span class="math inline">\(\{(\hat{x}, \hat{y}, \hat{z})\}\)</span> are matched with ground truth bounding boxes <span class="math inline">\(\boldsymbol{b}_{\hat{x}, \hat{y}, \hat{z}}\)</span>.</p><p>位置匹配之后, 一些位置<span class="math inline">\(\{(\hat{x}, \hat{y}, \hat{z})\}\)</span>会与边界框标签<span class="math inline">\(\boldsymbol{b}_{\hat{x}, \hat{y}, \hat{z}}\)</span>配对.</p><p>Consequently, these locations become associated with ground truth labels <span class="math inline">\(p_{\hat{x}, \hat{y}, \hat{z}}\)</span> and 3D centerness values <span class="math inline">\(c_{\hat{x}, \hat{y}, \hat{z}}\)</span>.</p><p>这些位置也会与对应的标签<span class="math inline">\(p_{\hat{x}, \hat{y}, \hat{z}}\)</span>和中心度数值<span class="math inline">\(c_{\hat{x}, \hat{y}, \hat{z}}\)</span>匹配.</p><p>During inference, the <u>scores <span class="math inline">\(\hat{\boldsymbol{p}}\)</span> are multiplied by 3D centerness <span class="math inline">\(\hat{c}\)</span></u> just before applying NMS.</p><p>在预测时, 分类分数<span class="math inline">\(\hat{\boldsymbol{p}}\)</span>会乘上中心度<span class="math inline">\(\hat{c}\)</span>作为边界框置信度进行NMS.</p><h5 id="loss-function">Loss function</h5><p>The <u>overall loss function</u> is formulated as follows: <span class="math display">\[\begin{array}{r}L=\frac{1}{N_{\mathrm{pos}}} \sum_{\hat{x}, \hat{y}, \hat{z}}\left(L_{\mathrm{cls}}(\hat{\boldsymbol{p}}, p)+\mathbb{1}_{\left\{p_{\hat{x}, \hat{y}, \hat{z}} \neq 0\right\}} L_{\mathrm{reg}}(\hat{\boldsymbol{b}}, \boldsymbol{b})+\mathbb{1}_{\left\{p_{\hat{x}, \hat{y}, \hat{z}} \neq 0\right\}} L_{\mathrm{cntr}}(\hat{c}, c)\right)\end{array}\]</span> 以上是整体的损失函数.</p><p>Here, the <u>number of matched locations</u> <span class="math inline">\(N_{\text{pos}}\)</span> is <span class="math inline">\(\sum_{\hat{x}, \hat{y}, \hat{z}} \mathbb{1}_{\left\{p_{\hat{x}, \hat{y}, \hat{z}} \neq 0\right\}}\)</span>.</p><p>匹配到的位置点总数<span class="math inline">\(N_{\text{pos}} = \sum_{\hat{x}, \hat{y}, \hat{z}} \mathbb{1}_{\left\{p_{\hat{x}, \hat{y}, \hat{z}} \neq 0\right\}}\)</span>.</p><p>Classiﬁcation loss <span class="math inline">\(L_{\text{cls}}\)</span> is calculated as a focal loss, regression loss <span class="math inline">\(L_{\text{reg}}\)</span> is IoU, and centerness loss <span class="math inline">\(L_{\text{cntr}}\)</span> is binary cross-entropy.</p><p>类别损失<span class="math inline">\(L_{\text{cls}}\)</span>使用focal loss, 回归损失<span class="math inline">\(L_{\text{reg}}\)</span>使用IoU, 中心损失<span class="math inline">\(L_{\text{cntr}}\)</span>使用binary cross-entropy.</p><p>For each loss, <u>predicted values are denoted with a hat</u>.</p><p>所有预测值都有hat标记.</p><h4 id="bounding-box-parametrization部分">Bounding Box Parametrization部分</h4><p>The 3D object bounding boxes can be <u>axis-aligned (AABB)</u> or <u>oriented (OBB)</u>.</p><p>三维物体边界框分为坐标轴对齐的边界框AABB和有向边界框OBB.</p><p><u>An AABB can be described as <span class="math inline">\(\boldsymbol{b}^{\text{AABB}}=(x, y, z, w, l, h)\)</span></u>, while the deﬁnition of <u>an OBB includes a <em>heading angle</em> <span class="math inline">\(\theta\)</span>: <span class="math inline">\(\boldsymbol{b}^{\text{OBB}}=(x, y, z, w, l, h, \theta)\)</span></u>.</p><p>坐标轴对齐的边界框AABB定义为<span class="math inline">\(\boldsymbol{b}^{\text{AABB}}=(x, y, z, w, l, h)\)</span>, 有向边界框OBB定义为<span class="math inline">\(\boldsymbol{b}^{\text{OBB}}=(x, y, z, w, l, h, \theta)\)</span>, 其中<span class="math inline">\(\theta\)</span>为朝向角.</p><p>In both formulas, <u><span class="math inline">\(x, y, z\)</span> denote the coordinates of the center of a bounding box</u>, while <u><span class="math inline">\(w, l, h\)</span> are its width, length, and height</u>, respectively.</p><p>在两种边界框的定义中, <span class="math inline">\(x, y, z\)</span>表示边界框中心点的坐标, <span class="math inline">\(w, l, h\)</span>表示边界框的宽、长和高.</p><h5 id="aabb-parametrization">AABB parametrization</h5><p>Speciﬁcally, for <u>a ground truth AABB <span class="math inline">\((x, y, z, w, l, h)\)</span></u> and <u>a location <span class="math inline">\((\hat{x}, \hat{y}, \hat{z})\)</span></u>, <u><span class="math inline">\(\boldsymbol{\delta}\)</span> can be formulated as a 6-tuple</u>: <span class="math display">\[\begin{gathered}\delta_{1}=x+\frac{w}{2}-\hat{x}, \delta_{2}=\hat{x}-x+\frac{w}{2}, \delta_{3}=y+\frac{l}{2}-\hat{y} \\\delta_{4}=\hat{y}-y+\frac{l}{2}, \delta_{5}=z+\frac{h}{2}-\hat{z}, \delta_{6}=\hat{z}-z+\frac{h}{2}\end{gathered}\]</span></p><p>对于一个坐标轴对齐的边界框AABB<span class="math inline">\((x, y, z, w, l, h)\)</span>和一个位置<span class="math inline">\((\hat{x}, \hat{y}, \hat{z})\)</span>, <span class="math inline">\(\delta\)</span>可以参数化为以上的形式(其实就是位置点到六个面的距离).</p><p>The predicted AABB <span class="math inline">\(\hat{\boldsymbol{b}}\)</span> can be trivially obtained from <span class="math inline">\(\boldsymbol{\delta}\)</span>.</p><p>使用位置坐标及其对应的<span class="math inline">\(\boldsymbol{\delta}\)</span>即可反解出对应的坐标轴对齐的边界框AABB<span class="math inline">\((x, y, z, w, l, h)\)</span>. <span class="math display">\[\begin{gathered}w = \delta_{1} + \delta_{2}, l = \delta_{3} + \delta_{4}, h = \delta_{5} + \delta_{6} \\x = \frac{\delta_{1} - \delta_{2}}{2} + \hat{x}, y = \frac{\delta_{3} - \delta_{4}}{2} + \hat{y}, z = \frac{\delta_{5} - \delta_{6}}{2} + \hat{z}\end{gathered}\]</span></p><h5 id="heading-angle-estimation">Heading angle estimation</h5><p>All existing state-of-the-art 3D object detection methods from point clouds <u>address the heading angle estimation task as classiﬁcation followed by regression</u>.</p><p>所有已有的使用点云的三位检测SOTA算法都将朝向角估计视为回归之后的分类任务.</p><p>The <u>heading angle is classiﬁed into bins</u>; then, the <u>precise value of the heading angle is regressed within a bin</u>.</p><p>朝向角先被分类为bins中的某一个bin, 之后再回归朝向角和这个bin之间的残差.</p><p>For indoor scenes, there are typically <u><span class="math inline">\(12\)</span> bins that uniformly divide the range from <span class="math inline">\(0\)</span> to <span class="math inline">\(2\pi\)</span> into <span class="math inline">\(12\)</span> sectors</u>.</p><p>对于室内场景, 通常在<span class="math inline">\(0\)</span>到<span class="math inline">\(2\pi\)</span>之间均匀划分<span class="math inline">\(12\)</span>个部分.</p><p>For outdoor scenes, <u>the number of bins is usually set to two</u> as the objects on the road can <u>be oriented either parallel or perpendicular to the road</u>.</p><p>对于室外场景, 通常划分两个类别, 因为路上的物体一般只有与道路平行或垂直这两种情况.</p><p>When a heading angle bin is chosen, the <u>actual value of the heading angle should be estimated through regression</u>.</p><p>得到朝向角的bin之后, 真实数值需要使用回归进行估计.</p><p>VoteNet and other voting-based methods estimate the value of missing <span class="math inline">\(\theta\)</span> directly.</p><p>VoteNet和其他voting-based的方法直接估计朝向角<span class="math inline">\(\theta\)</span>的数值.</p><p>Outdoor methods explore more elaborate approaches, e.g. predicting the values of trigonometric functions.</p><p>室外方法探索了更加复杂的方式, 例如预测三角函数的数值.</p><p>For instance, SMOKE estimates the values of <span class="math inline">\(\sin{\theta}\)</span> and <span class="math inline">\(\cos{\theta}\)</span>, which allows recovering the heading angle.</p><p>例如, SMOKE估计了<span class="math inline">\(\sin{\theta}\)</span>和<span class="math inline">\(\cos{\theta}\)</span>的数值, 通过这两个数值我们可以恢复朝向角的角度.</p><p>Figure below depicts indoor objects where the <u>heading angle cannot be deﬁned unambiguously</u>.</p><p>下图展示了在室内的物体的朝向角无法被没有歧义地被定义.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202112311353526.png" /></p><p>Ground truth angle annotations are random for these objects, making heading angle bin classiﬁcation meaningless.</p><p>这些物体的朝向角的标注是随机的, 这使得朝向角的bin分类没有意义.</p><p><u>To avoid penalizing the correct predictions that do not coincide with annotations</u>, we <u>use rotated IoU as a loss function</u>, since its value is not affected by the choice of a heading angle among possible options.</p><p>为了避免惩罚与标注不相符的正确的预测角, 作者使用旋转后的IoU作为损失函数, 因为旋转后的IoU不会被朝向角的固定标注值影响.</p><p>Thus, we propose <u>OBB parametrization that allows considering the rotation ambiguity</u>.</p><p>因此, 作者提出了考虑了旋转歧义的有向边界框的参数化方法.</p><h5 id="proposed-mobius-obb-parametrization">Proposed Mobius OBB parametrization</h5><p>Assuming that <u>an OBB has the parameters <span class="math inline">\((x, y, z, w, l, h, \theta)\)</span></u>, let us <u>denote <span class="math inline">\(q=\frac{w}{l}\)</span></u>.</p><p>假设有向边界框有参数<span class="math inline">\((x, y, z, w, l, h, \theta)\)</span>, 记<span class="math inline">\(q=\frac{w}{l}\)</span>.</p><p>If <span class="math inline">\(x, y, z, w + l, h\)</span> are ﬁxed, it turns out that the <u>OBBs with <span class="math inline">\((q, \theta),\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right),(q, \theta+\pi),\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right)\)</span> deﬁne the same bounding box</u>.</p><p>如果<span class="math inline">\(x, y, z, w + l, h\)</span>都是固定的, <span class="math inline">\((q, \theta),\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right),(q, \theta+\pi),\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right)\)</span>所代表的有向边界框等价.</p><p>We notice that the set of <span class="math inline">\((q, \theta)\)</span>, where <span class="math inline">\(\theta \in (0, 2\pi]\)</span>, <span class="math inline">\(q \in (0, +\inf)\)</span> is <u>topologically equivalent to a Mobius strip</u> up to this equivalence relation.</p><p>作者注意到<span class="math inline">\((q, \theta)\)</span>参数集合在拓扑不变性层面和莫比乌斯环相似, 其中<span class="math inline">\(\theta \in (0, 2\pi]\)</span>, <span class="math inline">\(q \in (0, +\inf)\)</span>.</p><p>Hence, we can reformulate the task of estimating <span class="math inline">\((q, \theta)\)</span> as a task of <u>predicting a point on a Mobius strip</u>.</p><p>于是估计<span class="math inline">\((q, \theta)\)</span>的任务转化为了估计莫比乌斯环上的一个点.</p><p>A natural way to embed a Mobius strip being a two-dimensional manifold to Euclidean space is the following:</p><p>一个将莫比乌斯环, 这一个二维流形, 嵌入欧几里得空间的方式如下: <span class="math display">\[(q, \theta) \mapsto(\ln (q) \sin (2 \theta), \ln (q) \cos (2 \theta), \sin (4 \theta), \cos (4 \theta))\]</span> It is easy to verify that <u>4 points are mapped into a single point</u> in Euclidean space.</p><p>可以验证之前提到的四个等价参数在上述表示的情况下对应同一个点.</p><p>However, the experiments reveal that <u>predicting only <span class="math inline">\(\ln{(q)} \sin{(2\theta)}\)</span> and <span class="math inline">\(\ln{(q)} \cos{(2θ)}\)</span> improves results compared to predicting all four values</u>.</p><p>实验证明只预测<span class="math inline">\(\ln{(q)} \sin{(2\theta)}\)</span>和<span class="math inline">\(\ln{(q)} \cos{(2θ)}\)</span>比预测全部的四个值更能有效提升预测结果.</p><p>Thereby, we opt for <u>a pseudo embedding of a Mobius strip to <span class="math inline">\(\mathbb{R}^{2}\)</span></u>.</p><p>于是, 我们优化一个伪莫比乌斯二维嵌入表达式.</p><p>We call it pseudo since the entire center circle of a Mobius strip deﬁned by <span class="math inline">\(\ln{(q)} = 0\)</span> maps to <span class="math inline">\((0, 0)\)</span>.</p><p>这个是伪莫比乌斯二维嵌入表达式是因为, 当<span class="math inline">\(\ln{(q)} = 0\)</span>的时候, 莫比乌斯环退化为一个圆且前两个参数变为<span class="math inline">\((0, 0)\)</span>.</p><p>Accordingly, we <u>cannot distinguish points with <span class="math inline">\(\ln{(q)} = 0\)</span></u>.</p><p>因此, 我们无法处理<span class="math inline">\(\ln{(q)} = 0\)</span>的点.</p><p>However, <span class="math inline">\(\ln{(q)} = 0\)</span> implies strict equality of <span class="math inline">\(w\)</span> and <span class="math inline">\(l\)</span>, which is <u>rare in real-world scenarios</u>.</p><p>然而, <span class="math inline">\(\ln{(q)} = 0\)</span>表示<span class="math inline">\(w\)</span>和<span class="math inline">\(l\)</span>相等, 在真实世界中, 这是很稀少的情况.</p><p>Moreover, the choice of an angle has a <u>minor effect on the IoU</u> if <span class="math inline">\(w = l\)</span>; thereby, we <u>ignore this rare case</u> for the sake of detection accuracy and simplicity of the method.</p><p>而且<span class="math inline">\(w = l\)</span>对于使用IoU损失函数来监督角度的选择的影响很小. 因此, 作者忽略了<span class="math inline">\(w = l\)</span>这种罕见的情况为了检测的准确性和方法的简洁性.</p><p>Overall, we obtain <u>a novel OBB parametrization</u>:</p><p>于是, 作者获得了一个全新的有向边界框的参数化方式: <span class="math display">\[\delta_{7}=\ln \frac{w}{l} \sin (2 \theta), \delta_{8}=\ln \frac{w}{l} \cos (2 \theta)\]</span> In standard 3D bounding box parametrization (AABB), <u><span class="math inline">\(\hat{\boldsymbol{b}}\)</span> is trivially derived from <span class="math inline">\(\boldsymbol{\delta}\)</span></u>.</p><p>在坐标轴对齐的边界框AABB的参数化模型中, <span class="math inline">\(\hat{\boldsymbol{b}}\)</span>可以轻而易举地从<span class="math inline">\(\boldsymbol{\delta}\)</span>中派生出来.</p><p>In the proposed parametrization, <span class="math inline">\(w, l, \theta\)</span> are non-trivial and can be obtained via the following:</p><p>在提出的新的有向边界框的参数化模型中, <span class="math inline">\(w, l, \theta\)</span>需要通过以下方式获取: <span class="math display">\[w=\frac{s q}{1+q}, l=\frac{s}{1+q}, \theta=\frac{1}{2} \arctan \frac{\delta_{7}}{\delta_{8}}\]</span> where ratio <span class="math inline">\(q=e^{\sqrt{\delta_{7}^{2}+\delta_{8}^{2}}}\)</span> and size <span class="math inline">\(s=\delta_1+\delta_2+\delta_3+\delta_4\)</span>.</p><h5 id="莫比乌斯环参数解释">莫比乌斯环参数解释</h5><blockquote><p><em>参考资料: <a href="https://zhuanlan.zhihu.com/p/75237170">莫比乌斯带的参数方程是怎么来的？它又为什么没有方向呢？</a></em></p></blockquote><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202112311824156.png" /></p><p>莫比乌斯环参数: <span class="math inline">\((q, \theta) \mapsto (\ln{(q)}\sin{(2\theta)}, \ln{(q)}\cos{(2\theta)}, \sin{(4\theta)}, \cos{(4\theta)})\)</span>, <span class="math inline">\(q\in (0, +\inf)\)</span>, <span class="math inline">\(\theta \in (0, 2\pi]\)</span>.</p><p>后两个参数<span class="math inline">\((\sin{(4\theta)}, \cos{(4\theta)})\)</span>在<span class="math inline">\(xoy\)</span>平面确定圆<span class="math inline">\(C_1\)</span>, 圆心为原点<span class="math inline">\(o\)</span>, 半径<span class="math inline">\(r\)</span>为<span class="math inline">\(1\)</span>, 假设某一角度<span class="math inline">\(\theta\)</span>对应的点为<span class="math inline">\(P\)</span>, 射线<span class="math inline">\(\vec{ol}\)</span>是从原点<span class="math inline">\(o\)</span>引向点<span class="math inline">\(P\)</span>的射线, 如下图所示.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021151199.png" /></p><p>前两个参数<span class="math inline">\((\ln{(q)}\sin{(2\theta)}, \ln{(q)}\cos{(2\theta)})\)</span>在<span class="math inline">\(zol\)</span>平面确定圆<span class="math inline">\(C_2\)</span>, 平面<span class="math inline">\(zol\)</span>与平面<span class="math inline">\(xoy\)</span>垂直且交线为射线<span class="math inline">\(ol\)</span>所在直线, 圆<span class="math inline">\(C_2\)</span>的圆心为点<span class="math inline">\(P\)</span>(即<span class="math inline">\(xoy\)</span>平面上的<span class="math inline">\(P\)</span>点), 圆<span class="math inline">\(C_2\)</span>的半径<span class="math inline">\(r\)</span>为<span class="math inline">\(\ln{(q)}\)</span>, 角度<span class="math inline">\(\theta\)</span>对应的点为<span class="math inline">\(Q\)</span>, <span class="math inline">\(Q\)</span>点即为参数<span class="math inline">\(q\)</span>和<span class="math inline">\(\theta\)</span>确定的莫比乌斯环上的一点, 如下图所示.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021153898.png" /></p><p>由于<span class="math inline">\(q \in (0, +\inf)\)</span>, 为了能够表示完整的直径, 需要使用<span class="math inline">\(\ln{(q)}\)</span>将值域拓展到<span class="math inline">\((-\inf, +\inf)\)</span>以涵盖整个直径的范围而不是只有半径的范围(意思是<span class="math inline">\(\vec{PQ}\)</span>可以反向延伸), 同时<span class="math inline">\(\ln{(q)}\)</span>可以使得文中提出的四组参数<span class="math inline">\((q, \theta),\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right),(q, \theta+\pi),\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right)\)</span>在参数化之后等价.</p><p>由于莫比乌斯环的性质, 莫比乌斯环上的点在运动时, 在圆<span class="math inline">\(C_1\)</span>上的角速度为圆<span class="math inline">\(C_2\)</span>上的两倍, 因此两个圆的表达式中, <span class="math inline">\(\theta\)</span>前面的系数也应该保持<span class="math inline">\(2:1\)</span>的关系.</p><p>由于需要使得文中提出的四组参数<span class="math inline">\((q, \theta),\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right),(q, \theta+\pi),\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right)\)</span>在参数化之后等价, 所以不直接用<span class="math inline">\((2 \theta, \theta)\)</span>而是用<span class="math inline">\((4 \theta, 2 \theta)\)</span>.</p><h3 id="experiments部分">Experiments部分</h3><h4 id="datasets部分">Datasets部分</h4><p>We evaluate our method on three 3D object detection benchmarks: <u>ScanNet V2, SUN RGB-D, and S3DIS</u>.</p><p>作者在ScanNet V2、SUN RGB-D和S3DIS上测试了FCAF3D的算法性能.</p><p>For all datasets, we use <u>mean average precision (mAP) under IoU thresholds of 0.25 and 0.5</u> as a metric.</p><p>对于所有数据集, 作者使用<span class="math inline">\(mAP@0.25\)</span>和<span class="math inline">\(mAP@0.5\)</span>作为评价指标.</p><h5 id="scannet-v2">ScanNet V2</h5><p>The ScanNet dataset contains <u>1513 reconstructed 3D indoor scans</u> with <u>per-point instance and semantic labels of 18 object categories</u>.</p><p>ScanNet数据集包含1513个三维室内场景, 每个场景带有点级别的实例标注和18个物体类别的语义标注.</p><p>Given this annotation, we <u>calculate AABBs through a standard approach</u>.</p><p>ScanNet数据集使用的是AABB边界框.</p><p>The <u>training subset is comprised of 1201 scans</u>, while <u>the resting 312 scans are left for validation</u>.</p><p>ScanNet数据集的训练集包含1201个扫描场景, ScanNet数据集的验证集包含剩下的312个扫描场景.</p><h5 id="sun-rgb-d">SUN RGB-D</h5><p>SUN RGB-D is a monocular dataset for 3D scene understanding <u>containing more than 10,000 indoor RGB-D images</u>.</p><p>SUN RGB-D数据集包含超过10000个室内场景RGB-D图片.</p><p>The annotation consists of <u>per-point semantic labels and OBBs of 37 object categories</u>.</p><p>SUN RGB-D数据集标注包含点级别的语义标签和37类物体的OBB边界框.</p><p>We run experiments with objects of the <u>10 most common categories</u>.</p><p>作者在10类最常见的类别上进行了实验.</p><p>The <u>training and validation splits contain 5285 and 5050 point clouds</u>, respectively.</p><p>SUN RGB-D数据集的训练集包含5285个点云, SUN RGB-D数据集验证集包含5050个点云.</p><h5 id="s3dis">S3DIS</h5><p>Stanford Large-Scale 3D Indoor Spaces dataset <u>contains 3D scans of 6 buildings with 272 rooms</u>.</p><p>S3DIS数据集包含6栋建筑物的272个房间的三维场景扫描数据.</p><p>Each scan is annotated <u>with instance and semantic labels of seven structural elements (e.g. ﬂoor and ceiling) and ﬁve furniture categories</u>.</p><p>S3DIS数据集的每个扫描数据包含7类结构元素和5个家具类别的实例和物体标签.</p><p>We evaluate our method on <u>furniture categories only</u>.</p><p>作者在家具类别上进行了实验.</p><p>Similar to ScanNet, <u>AABBs are derived from 3D semantics</u>.</p><p>S3DIS数据集与ScanNet数据集类似, 使用AABB边界框.</p><p>We use the ofﬁcial split, where <u>68 rooms from Area 5 are intended for validation</u>, while the <u>remaining 204 rooms comprise the training subset</u>.</p><p>S3DIS数据集训练集包含区域5之外的204个房间, S3DIS数据集测试集包含区域5的68个房间.</p><h4 id="implementation-details部分">Implementation Details部分</h4><h5 id="hyperparameters">Hyperparameters</h5><p>First, <u>the size of output classiﬁcation layer equals the number of object categories</u>, which is <u>18, 10, and 5 for ScanNet, SUN RGB-D, and S3DIS</u>.</p><p>分类层的输出与各数据集的物体类别匹配, ScanNet V2是18, SUN RGB-D是10, S3DIS是5.</p><p>Second, <u>SUN RGB-D contains OBBs</u>, so we <u>predict additional targets <span class="math inline">\(\delta_7\)</span> and <span class="math inline">\(\delta_8\)</span> for this dataset</u>; note that the <u>loss function is not affected</u>.</p><p>只有SUN RGB-D数据集包含OBB边界框, 因此只在SUN RGB-D数据集中预测<span class="math inline">\(\delta_7\)</span>和<span class="math inline">\(\delta_8\)</span>, 损失函数不受影响.</p><p>Last, <u>ScanNet, SUN RGB-D, and S3DIS contain different numbers of scenes</u>, so we <u>repeat each scene 10, 3, and 13 times per epoch</u>, respectively.</p><p>ScanNet、SUN RGB-D和S3DIS数据集场景数量各不相同, 在每个epoch中, 这三个数据集中每个场景分别重复出现10、3和13次.</p><p>In initial point cloud voxelization, we <u>set the voxel size to 0.01m and the number of points <span class="math inline">\(N_{\text{pts}}\)</span> to 100,000</u>.</p><p>在点云体素化的过程中, 作者设置体素大小为0.01米, 设置点数量<span class="math inline">\(N_{\text{pts}}=100000\)</span>.</p><p>Respectively, <u><span class="math inline">\(N_{\text{vox}}\)</span> equals to 100,000.</u></p><p>对应的, <span class="math inline">\(N_{\text{vox}}\)</span>也设置为100000.</p><p><u>Both ATSS and FCOS set <span class="math inline">\(N_{\text{loc}}\)</span> to <span class="math inline">\(3^2\)</span></u> for 2D object detection.</p><p>在二维检测中, ATSS和FCOS算法都设置<span class="math inline">\(N_{\text{loc}}\)</span>为<span class="math inline">\(3^2\)</span>.</p><p>Accordingly, we select a feature level so <u>bounding box covers at least <span class="math inline">\(N_{\text{loc}} = 3^3\)</span> locations</u>.</p><p>同样地, 作者设置<span class="math inline">\(N_{\text{loc}}\)</span>为<span class="math inline">\(3^3\)</span>(因为是三维检测).</p><p>By center sampling, we <u>select 18 locations</u>, while the <u>NMS IoU threshold is 0.5</u>.</p><p>通过中心点采样, 作者选取18个位置, 同时设置NMS的IoU阈值为0.5.</p><h5 id="training">Training</h5><p>We implement our FCAF3D <u>using the MMdetection3D framework</u>.</p><p>作者使用MMdetection3D架构实现FCAF3D.</p><p>The overall training procedure follows the default scheme from MMdetection: <u>training takes 12 epochs with the learning rate decreasing on the 8th and the 11th epochs</u>.</p><p>训练过程遵循默认的MMdetection的设置: 训练12个epoch, 学习率在第8个和第11个epoch下降.</p><p>We employ the <u>Adam optimizer with an initial learning rate of 0.001 and weight decay of 0.0001</u>.</p><p>作者使用Adam优化器优化学习率, 初始学习率设置为0.001, 权重衰减设置为0.0001.</p><p>All models are trained on two NVidia V100 with a <u>batch size of 8</u>.</p><p>所有的模型都在两块NVidia V100上训练, 批大小是8.</p><p>Evaluation and performance tests are run on a single NVidia GTX1080Ti.</p><p>评估和表现测试都是在单张NVidia GTX1080Ti上完成的.</p><h5 id="evaluation">Evaluation</h5><p>Both training and evaluation procedures are randomized, as <u>the raw point clouds are randomly sampled to select <span class="math inline">\(N_{\text{pts}}\)</span> for the input</u>.</p><p>由于每次都会在原始点云上随机采样<span class="math inline">\(N_{\text{pts}}\)</span>个点, 训练和验证过程都是随机的.</p><p>Thus, we re-run all the experiments to <u>obtain statistically signiﬁcant results</u>.</p><p>因此, 作者反复进行实验来获取统计学上显著的结果.</p><p>We run <u>training 5 times and test each trained model 5 times independently</u>.</p><p>作者独立的训练了5个模型, 每个模型分别独立测试5次.</p><p>We report both the <u>best and average metrics across all <span class="math inline">\(5 \times 5\)</span> trials</u>.</p><p>作者报告了这<span class="math inline">\(5 \times 5\)</span>实验中评价指标的最优值和均值.</p><h3 id="results部分">Results部分</h3><h4 id="comparison-with-state-of-the-art-methods部分">Comparison with State-of-the-art Methods部分</h4><p>Results of FCAF3D and existing indoor 3D object detection methods that <u>accept point clouds</u>.</p><p>FCAF3D和现有的接受点云输入的室内三维物体检测方法的实验结果.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021131231.png" /></p><p>The reported metric value is the <u>best one across 25 trials</u>; the <u>average value is given in brackets</u>.</p><p>在FCAF3D的25组模型中, 括号外的是表现最好的模型的评价指标, 括号内是所有模型评价指标的均值.</p><h4 id="object-geometry-priors部分">Object Geometry Priors部分</h4><p>VoteNet and ImVoteNet have the <u>same head</u> and are trained with the <u>same losses</u>.</p><p>VoteNet和ImVoteNet有相同的head和相同的损失函数.</p><p>Among them, there are 4 prior losses: <u>size classiﬁcation loss, size regression loss, direction classiﬁcation loss, and direction regression loss</u>.</p><p>VoteNet和ImVoteNet的与先验信息有关的损失函数包含: 大小分类损失, 大小回归损失, 朝向分类损失和朝向回归损失.</p><p><u>Both classiﬁcation losses correspond to targets parametrized using priors</u> (per-category mean object sizes and a set of angle bins).</p><p>大小分类函数和朝向分类函数都与使用先验信息的参数有关.</p><p>Similar to FCAF3D, we <u>replace the aforementioned losses with a rotated IoU loss with Mobius parametrization</u>.</p><p>作者将VoteNet和ImVoteNet的上述损失函数更换为旋转IoU损失并且将边界框的参数化方法更换为莫比乌斯参数化模型.</p><p>To give a complete picture, we also <u>try a sin-cos parametrization used in the outdoor 3D object detection method SMOKE</u>.</p><p>为了更完全的比较, 作者也尝试使用了在室外三维物体检测算法SMOKE中使用的sin-cos参数化方法进行对比实验.</p><p>The rotated IoU loss allows <u>decreasing the number of trainable parameters and hyperparameters</u>, including geometry priors and loss weights.</p><p>旋转IoU损失可以有效减少训练的参数量和超参数的数量.</p><p>ImVoxelNet <u>does not use a classiﬁcation+regression scheme to estimate heading angle</u> but <u>predicts its value directly in a single step</u>.</p><p>ImVoxelNet不是采用分类加回归的方案预测朝向角, 其直接预测朝向角的数值.</p><p>Since the original ImVoxelNet uses the rotated IoU loss, we do not need to remove redundant losses, <u>only to change the parametrization</u>.</p><p>ImVoxelNet使用旋转IoU损失, 因此不用修改损失函数, 只用更改其边界框参数化方法.</p><p>Results of several existing 3D object detection methods that accept inputs of different modalities, with different OBB parametrization on the SUN RGB-D dataset.</p><p>使用不同损失函数和边界框参数的对比试验结果.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021147495.png" /></p><p>For FCAF3D, the reported metric value is the <u>best across 25 trials</u>; the <u>average value is given in brackets</u>.</p><p>在FCAF3D的25组模型中, 括号外的是表现最好的模型的评价指标, 括号内是所有模型评价指标的均值.</p><p>For other methods, we report results from the <u>original papers</u> and also the <u>results obtained through our experiments with MMdetection3D-based re-implementations (marked as Reimpl)</u>.</p><p>其他方法的数据来自其原始论文和作者使用MMdetection3D重新实现的代码进行实验的结果(标记为Reimpl).</p><h5 id="gsdn-anchors">GSDN anchors</h5><p>Next, we study GSDN anchors to prove that the <u>generalization ability of anchor-based models is limited</u>.</p><p>作者对GSDN进行实验来验证anchor-based算法的生成能力是有限的.</p><p><u>Without domain-speciﬁc guidance in the form of anchors, GSDN demonstrates a poor performance</u>; hence, we claim this method to be <u>inﬂexible and non-generalized</u>.</p><p>在没有anchor的情况下, GSDN表现很糟糕, 因此, 我们认为GSDN的延展性和生成能力很差.</p><p>Results of fully convolutional 3D object detection methods that accept point clouds on ScanNet.</p><p>GSDN和FCAF3D的对比实验结果.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021148945.png" /></p><h4 id="ablation-study部分">Ablation Study部分</h4><p>We run experiments with <u>varying voxel size, the number of points in a point cloud <span class="math inline">\(N_{\text{pts}}\)</span>, the number of locations selected by center sampling, and with and without centerness</u>.</p><p>作者使用不同的体素大小、<span class="math inline">\(N_{\text{pts}}\)</span>、中心采样的位置点数量和有无中心度进行消融实验.</p><p>Results of ablation studies on the voxel size, the number of points (which equals the number of voxels <span class="math inline">\(N_{\text{vox}}\)</span> in pruning), centerness, and center sampling in FCAF3D.</p><p>消融实验的实验结果.</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021407746.png" /></p><p>The reported metric value is the <u>best across 25 trials</u>; the <u>average value is given in brackets</u>.</p><p>在FCAF3D的25组模型中, 括号外的是表现最好的模型的评价指标, 括号内是所有模型评价指标的均值.</p><h5 id="voxel-size">Voxel size</h5><p>Expectedly, <u>with an increasing voxel size, accuracy goes down</u>.</p><p>体素越大, 精度越低.</p><h5 id="number-of-points">Number of points</h5><p>Similar to 2D images, <u>subsampled point clouds are sometimes referred to as <em>low-resolution</em> ones</u>.</p><p>下采样的点云可以称之为低精度点云.</p><p>Accordingly, they <u>contain less information than their <em>high-resolution</em> versions</u>.</p><p>低精度点云相比于高精度点云包含更少的信息.</p><p>As can be expected, <u>the fewer the points, the lower is detection accuracy</u>.</p><p>点云数量越少, 精度越低.</p><h5 id="centerness">Centerness</h5><p>Using centerness <u>improves mAP for the ScanNet and SUN RGB-D datasets</u>.</p><p>使用中心度对于ScanNet和SUN RGB-D数据集有提升.</p><p>For S3DIS, the results are controversial: the better <span class="math inline">\(mAP@0.5\)</span> is balanced with a minor decrease of <span class="math inline">\(mAP@0.25\)</span>.</p><p>对于S3DIS数据集, 使用中心度在<span class="math inline">\(mAP@0.5\)</span>上有提升, 在<span class="math inline">\(mAP@0.25\)</span>上有略微下降.</p><p>Nevertheless, we analyze the results altogether, so we can <u>consider centerness a helpful feature with a small positive effect on the mAP</u>, almost reaching 1% of <span class="math inline">\(mAP@0.5\)</span> on ScanNet.</p><p>整体来说, 中心度是一个有用的特征能够对于提升mAP有较小的积极作用.</p><h5 id="center-sampling">Center sampling</h5><p>We select <span class="math inline">\(9\)</span> locations, as proposed in FCOS, the entire set of <span class="math inline">\(27\)</span> locations, as in ImVoxelNet, and <span class="math inline">\(18\)</span> being an average of these options.</p><p>作者尝试了9, 27和18这三种不同的选项.</p><p><u>The latter appeared to be the best choice that allows achieving higher mAP</u> on all the benchmarks.</p><p>将中心采样的位置数量设置为18时, 在mAP上的表现最好.</p><h4 id="inference-speed部分">Inference Speed部分</h4><p>Compared to standard convolutions, <u>sparse convolutions are time- and memory-efﬁcient</u>.</p><p>稀疏卷积在时间上和空间上更加高效.</p><p>FCAF3D uses the <u>same sparse convolutions and the same backbone</u> as GSDN.</p><p>FCAF3D使用与GSDN相同的卷积和backbone.</p><p>However, <u>the default FCAF3D is slower than GSDN</u>.</p><p>但是默认设置的FCAF3D比GSDN慢.</p><p>This is due to the smaller voxel size: we use 0.01m for a proper multi-level assignment while GSDN uses 0.05m.</p><p>这是因为FCAF3D的体素比GSDN的小.</p><p>To build the fastest method, we conduct experiments with HDResNet34:3 and HDResNet34:2 with only three and two feature levels, respectively.</p><p>为了加快FCAF3D的运行速度, 作者设计了使用两层或三层特征层级的版本.</p><p>With these modiﬁcations, FCAF3D is faster on inference than GSDN.</p><p>修改后的版本运行速度比GSDN快.</p><p>The comparison is shown graphically in follow figure (<span class="math inline">\(mAP@0.5\)</span> scores on ScanNet against scenes per second).</p><p>速度对比结果如下(横轴是每秒预测的场景数量, 纵轴是对应的<span class="math inline">\(mAP@0.5\)</span>分数):</p><p><img src="https://raw.githubusercontent.com/chence17/picgo/master/202201021429824.png" /></p><p>In performance tests, we opt for implementations <u>based on the MMdetection3D framework</u> to mitigate codebase differences.</p><p>为消除代码实现的不同带来的影响, 所有的速度对比都是基于MMdetection3D框架实现的.</p><p>The reported inference speed for all methods is measured on the same single GPU so they can be directly compared.</p><p>所有的预测都是在同一张单GPU上进行的.</p><h3 id="supplement部分">Supplement部分</h3><h4 id="additional-comments-on-mobius-parametrization部分">Additional Comments on Mobius Parametrization部分</h4><p>The OBB heading angle <span class="math inline">\(\theta\)</span> is typically deﬁned as <u>an angle between x-axis and a vector towards a center of one of OBB faces</u>.</p><p>有向边界框的朝向角一般是<span class="math inline">\(x\)</span>轴和一个面中心点对应的向量的夹角.</p><p><u>If a frontal face exists, then <span class="math inline">\(\theta\)</span> is deﬁned unambiguously</u>; however, this is not the case for some indoor objects.</p><p>如果正面是确定的, 那么<span class="math inline">\(\theta\)</span>不会产生歧义, 但是对大多数室内物体来说, 正面不是确定的.</p><p>If a frontal face cannot be chosen unequivocally, there are <u>four possible representations</u> for a single OBB.</p><p>如果正面无法被确定, 那么对于一个有向边界框将会有四种不同的表达方式.</p><p>The <u>heading angle describes a rotation</u> within the <span class="math inline">\(xy\)</span> plane around <span class="math inline">\(z\)</span>-axis w.r.t. the OBB center.</p><p>朝向角只描述了边界框绕<span class="math inline">\(z\)</span>轴旋转的角度.</p><p>Therefore, the <u>OBB center <span class="math inline">\((x, y, z)\)</span>, height <span class="math inline">\(h\)</span>, and the OBB size <span class="math inline">\(s=w+l\)</span> are the same</u> for all representations.</p><p>因此, 边界框中心<span class="math inline">\((x, y, z)\)</span>, 高<span class="math inline">\(h\)</span>和大小<span class="math inline">\(s=w+l\)</span>对于这四种表达来说是相同的.</p><p>Meanwhile, <u>the ratio <span class="math inline">\(q = \frac{w}{l}\)</span> of the frontal and lateral OBB faces and the heading angle <span class="math inline">\(\theta\)</span> do vary</u>.</p><p>但是比例<span class="math inline">\(q = \frac{w}{l}\)</span>和角度<span class="math inline">\(\theta\)</span>对这四种表达来说是不同的.</p><p>Speciﬁcally, there are <u>four options for the heading angle</u>: <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\theta+\frac{\pi}{2}\)</span>, <span class="math inline">\(\theta+\pi\)</span>, <span class="math inline">\(\theta+\frac{3\pi}{2}\)</span>.</p><p>有四种不同的角度: <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\theta+\frac{\pi}{2}\)</span>, <span class="math inline">\(\theta+\pi\)</span>, <span class="math inline">\(\theta+\frac{3\pi}{2}\)</span>.</p><p>Swapping frontal and lateral faces gives <u>two ratio options</u>: <span class="math inline">\(q\)</span> and <span class="math inline">\(\frac{1}{q}\)</span>.</p><p>有两种不同的比例: <span class="math inline">\(q\)</span> and <span class="math inline">\(\frac{1}{q}\)</span>.</p><p>Overall, there are <u>four different tuples <span class="math inline">\((q, \theta)\)</span> for the same OBB</u>: <span class="math inline">\((q, \theta)\)</span>, <span class="math inline">\(\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right)\)</span>, <span class="math inline">\((q, \theta+\pi)\)</span>, <span class="math inline">\(\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right)\)</span>.</p><p>于是这四种表示分别是: <span class="math inline">\((q, \theta)\)</span>, <span class="math inline">\(\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right)\)</span>, <span class="math inline">\((q, \theta+\pi)\)</span>, <span class="math inline">\(\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right)\)</span>.</p><p>Here, we prove that <u>four different representations of the same OBB map to the same point on a Mobius strip</u>.</p><p>可以证明这四种表示在莫比乌斯环上对应同一个点. <span class="math display">\[\begin{aligned}(q, \theta) \mapsto &amp; (\ln (q) \sin (2 \theta), \ln (q) \cos (2 \theta), \sin (4 \theta), \cos (4 \theta)) \\\left(\frac{1}{q}, \theta+\frac{\pi}{2}\right) \mapsto &amp; \left(\ln \left(\frac{1}{q}\right) \sin (2 \theta+\pi), \ln \left(\frac{1}{q}\right) \cos (2 \theta+\pi), \sin (4 \theta+2 \pi), \cos (4 \theta+2 \pi)\right) \\&amp;=(\ln (q) \sin (2 \theta), \ln (q) \cos (2 \theta), \sin (4 \theta), \cos (4 \theta)) \\(q, \theta+\pi) \mapsto &amp; (\ln (q) \sin (2 \theta+2 \pi)), \ln (q) \cos (2 \theta+2 \pi), \sin (4 \theta+4 \pi), \cos (4 \theta+4 \pi)) \\&amp;=(\ln (q) \sin (2 \theta), \ln (q) \cos (2 \theta), \sin (4 \theta), \cos (4 \theta)) \\\left(\frac{1}{q}, \theta+\frac{3 \pi}{2}\right) \mapsto &amp; \left(\ln \left(\frac{1}{q}\right) \sin (2 \theta+3 \pi)\right), \ln \left(\frac{1}{q}\right) \cos (2 \theta+3 \pi), \sin (4 \theta+6 \pi), \cos (4 \theta+6 \pi) \\&amp;=(\ln (q) \sin (2 \theta), \ln (q) \cos (2 \theta), \sin (4 \theta), \cos (4 \theta))\end{aligned}\]</span></p><h4 id="per-category-results部分">Per-category results部分</h4><h4 id="visualization部分">Visualization部分</h4><h3 id="精读总结">精读总结</h3><blockquote><p>精读目标及效果自测：所读段落是否详细掌握，回答三个的终极问题(你是谁 - 论文提出/采用什么方法、细节是什么；从哪里来 - 论文要解决什么问题/任务、其启发点或借鉴之处在哪；到哪里去 - 论文方法达到什么效果)。</p></blockquote><p>论文采用稀疏卷积结合类似FCOS的设计, 构建了一个多特征层级的三维室内物体检测器.</p><p>论文使用旋转IoU损失函数实现anchor-free并提升模型的生成能力.</p><p>论文还提出了基于莫比乌斯表达的新型三维边界框参数化模型以解决三维框朝向角歧义的问题.</p><p>论文指出这种新型参数化方法应用到任何一种现有的三维物体检测方法上都有助于提升检测精度.</p><p>论文设计的方案在现有的室内检测数据集上达到了SOTA并且保持较快的预测速度.</p><h2 id="总结">总结</h2><blockquote><p>总结：总览全文，归纳总结，总结文中创新点，关键点，启发点（论文idea的来源或者说是论文的motivation）等重要信息。</p></blockquote><p>论文的创新点、关键点:</p><ul><li>使用多特征层级的方案进行三维目标检测;</li><li>设计新的边界框参数化方法优化预测效果;</li><li>使用旋转IoU损失函数实现anchor-free;</li><li>使用稀疏卷积和剪枝控制模型参数加快预测速度;</li><li>在目前主要的室内三维检测数据集上达到了SOTA.</li></ul><p>论文的启发点:</p><ul><li>FCOS的网络架构;</li><li>GSDN的稀疏卷积;</li><li>ImVoxelNet的旋转IoU损失函数.</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
